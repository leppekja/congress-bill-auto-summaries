{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Summarization of Congressional Bills\n",
    "\n",
    "Acknowledgments: This code builds on Seq2Seq modeling availabile here and HW4 of Advanced Machine Learning written by Zewei Chu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract:\n",
    "\n",
    "In this paper, we describe a Sequence to Sequence neural network with multilayered Long Short-Term Memory (LSTM) networks. We use a novel data set of Congressional bills and human-generated summaries to train the model. We find that WHAT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction:\n",
    "\n",
    "The effectiveness of abstract summarization techniques is very limited on complex and lengthy texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::nbclassic==0.2.6=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::smart_open==2.2.1=pyh9f0ad1d_0\n",
      "  - conda-forge/linux-64::blaze==0.11.3=py36_0\n",
      "  - defaults/linux-64::_anaconda_depends==5.1.0=py36_2\n",
      "  - conda-forge/noarch::requests==2.25.1=pyhd3deb0d_0\n",
      "  - conda-forge/noarch::jupyterlab==3.0.9=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::python-language-server==0.36.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::jupyterlab_server==2.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::google-api-core==1.25.1=pyh44b312d_0\n",
      "  - conda-forge/noarch::pyls-black==0.4.6=pyh9f0ad1d_0\n",
      "  - fastai/noarch::fastai==1.0.61=1\n",
      "  - conda-forge/noarch::pathy==0.4.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::black==20.8b1=py_1\n",
      "  - conda-forge/linux-64::spacy==3.0.3=py36h355b2fd_0\n",
      "  - conda-forge/linux-64::anyio==2.1.0=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::jupyter_server==1.4.1=py36h5fab9bb_0\n",
      "  - conda-forge/noarch::google-auth==1.24.0=pyhd3deb0d_0\n",
      "  - conda-forge/linux-64::thinc==8.0.1=py36h355b2fd_1\n",
      "  - conda-forge/noarch::numpydoc==1.1.0=py_1\n",
      "  - conda-forge/linux-64::spyder==4.2.0=py36h5fab9bb_0\n",
      "  - conda-forge/linux-64::pydantic==1.7.3=py36h8f6f2f9_0\n",
      "  - conda-forge/noarch::pyls-spyder==0.3.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::typer==0.3.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::google-cloud-storage==1.19.0=py_0\n",
      "  - conda-forge/noarch::sphinx==3.5.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py36h5fab9bb_6\n",
      "  - conda-forge/noarch::google-cloud-core==1.5.0=pyhd3deb0d_0\n",
      "  - conda-forge/noarch::s3fs==0.5.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-client==1.7.2=py_0\n",
      "  - conda-forge/noarch::anaconda-project==0.9.1=pyhd8ed1ab_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/pytorch_p36\n",
      "\n",
      "  added / updated specs:\n",
      "    - torchtext\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    astroid-2.5.6              |   py36h5fab9bb_0         300 KB  conda-forge\n",
      "    boto3-1.17.81              |     pyhd8ed1ab_0          70 KB  conda-forge\n",
      "    botocore-1.20.81           |     pyhd8ed1ab_0         4.7 MB  conda-forge\n",
      "    dataclasses-0.8            |     pyh787bdff_0          22 KB  conda-forge\n",
      "    docutils-0.17.1            |   py36h5fab9bb_0         762 KB  conda-forge\n",
      "    flask-cors-3.0.8           |             py_0          14 KB  conda-forge\n",
      "    jupyter_console-5.2.0      |           py36_1          34 KB  conda-forge\n",
      "    lxml-4.6.3                 |   py36h04a5ba7_0         1.5 MB  conda-forge\n",
      "    pylint-2.7.2               |   py36h5fab9bb_0         466 KB  conda-forge\n",
      "    torchtext-0.6.0            |             py_1          48 KB  pytorch\n",
      "    urllib3-1.26.5             |     pyhd8ed1ab_0          99 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         7.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  aiobotocore        pkgs/main/noarch::aiobotocore-1.2.2-pyhd3eb1b0_0\n",
      "  aiohttp            conda-forge/linux-64::aiohttp-3.7.4-py36h8f6f2f9_0\n",
      "  astroid            conda-forge/linux-64::astroid-2.5.6-py36h5fab9bb_0\n",
      "  boto3              conda-forge/noarch::boto3-1.17.81-pyhd8ed1ab_0\n",
      "  botocore           conda-forge/noarch::botocore-1.20.81-pyhd8ed1ab_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\n",
      "  dataclasses        conda-forge/noarch::dataclasses-0.8-pyh787bdff_0\n",
      "  docutils           conda-forge/linux-64::docutils-0.17.1-py36h5fab9bb_0\n",
      "  flask-cors         conda-forge/noarch::flask-cors-3.0.8-py_0\n",
      "  jupyter_console    conda-forge/linux-64::jupyter_console-5.2.0-py36_1\n",
      "  lxml               conda-forge/linux-64::lxml-4.6.3-py36h04a5ba7_0\n",
      "  pylint             conda-forge/linux-64::pylint-2.7.2-py36h5fab9bb_0\n",
      "  s3transfer         conda-forge/noarch::s3transfer-0.4.2-pyhd8ed1ab_0\n",
      "  torchtext          pytorch/noarch::torchtext-0.6.0-py_1\n",
      "  urllib3            conda-forge/noarch::urllib3-1.26.5-pyhd8ed1ab_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "torchtext-0.6.0      | 48 KB     | ##################################### | 100% \n",
      "pylint-2.7.2         | 466 KB    | ##################################### | 100% \n",
      "botocore-1.20.81     | 4.7 MB    | ##################################### | 100% \n",
      "dataclasses-0.8      | 22 KB     | ##################################### | 100% \n",
      "flask-cors-3.0.8     | 14 KB     | ##################################### | 100% \n",
      "astroid-2.5.6        | 300 KB    | ##################################### | 100% \n",
      "urllib3-1.26.5       | 99 KB     | ##################################### | 100% \n",
      "boto3-1.17.81        | 70 KB     | ##################################### | 100% \n",
      "lxml-4.6.3           | 1.5 MB    | ##################################### | 100% \n",
      "jupyter_console-5.2. | 34 KB     | ##################################### | 100% \n",
      "docutils-0.17.1      | 762 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install torchtext -y -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from numpy import floor\n",
    "from numpy.random import shuffle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from ast import literal_eval\n",
    "import boto3, os\n",
    "import sagemaker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Small development version\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 1\n",
    "\n",
    "print(DEVICE)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "COLAB = False\n",
    "AWS = True\n",
    "DEVELOPING = True\n",
    "SINGLE_RECORD = False\n",
    "\n",
    "if DEVELOPING:\n",
    "    if SINGLE_RECORD:\n",
    "        print('Training on single record')\n",
    "        BATCH_SIZE = 1\n",
    "        EMBEDDING_SIZE = 300\n",
    "        TRAINING_SIZE = 1\n",
    "        VALIDATION_SIZE = 0\n",
    "        TESTING_SIZE = 0\n",
    "        MAX_SUMMARY_LENGTH = 5\n",
    "        MAX_BILL_LENGTH = 8\n",
    "    else:\n",
    "        print('Small development version')\n",
    "        BATCH_SIZE = 20\n",
    "        EMBEDDING_SIZE = 300\n",
    "        DATA_FILE = 'Minimum_Length_Sample.csv'\n",
    "        TRAINING_SIZE = .7\n",
    "        VALIDATION_SIZE = .2\n",
    "        TESTING_SIZE = .1\n",
    "else:\n",
    "    print('Full version')\n",
    "    BATCH_SIZE = 4\n",
    "    EMBEDDING_SIZE = 300\n",
    "    DATA_FILE = 'Cleaned_Summaries_And_Bills.csv'\n",
    "    TRAINING_SIZE = .7\n",
    "    VALIDATION_SIZE = .2\n",
    "    TESTING_SIZE = .1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                            summary  \\\n",
      "0  id113hr3079    Provides for the relief of Jesus Garcia Flores.   \n",
      "1  id113hr3169  IRS Rulemaking Fairness Act of 2013 - Expands ...   \n",
      "\n",
      "                    name                                               link  \\\n",
      "0  BILLS-113hr3079ih.xml  https://www.govinfo.gov/bulkdata/BILLS/113/1/h...   \n",
      "1  BILLS-113hr3169ih.xml  https://www.govinfo.gov/bulkdata/BILLS/113/1/h...   \n",
      "\n",
      "                                                text  \\\n",
      "0  <?xml version=\"1.0\"?>\\n<?xml-stylesheet type=\"...   \n",
      "1  <?xml version=\"1.0\"?>\\n<?xml-stylesheet type=\"...   \n",
      "\n",
      "                                       summary_clean  \\\n",
      "0  [<sos>, provides, for, the, relief, of, jesus,...   \n",
      "1  [<sos>, irs, rulemaking, fairness, act, of, ##...   \n",
      "\n",
      "                                          bill_clean  \n",
      "0  [<sos>, :, for, the, relief, of, jesus, garcia...  \n",
      "1  [<sos>, :, irs, rulemaking, fairness, act, of,...  \n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    PATH = '/content/drive/MyDrive/'\n",
    "\n",
    "elif AWS:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "    bucket = 'capp-20235'\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=bucket, Key=DATA_FILE)\n",
    "else:\n",
    "    PATH = './'\n",
    "\n",
    "if SINGLE_RECORD:\n",
    "    DATA_SET = pd.DataFrame({'bill_clean':[['<sos>', 'make', 'all', 'drugs', 'legal','for','citizens', '<eos>']], 'summary_clean':[['<sos>', 'make', 'drugs', 'legal', '<eos>']]})\n",
    "elif AWS:\n",
    "    DATA_SET = pd.read_csv(response['Body'], converters={'summary_clean': literal_eval, 'bill_clean': literal_eval})\n",
    "else:\n",
    "    DATA_SET = pd.read_csv(PATH + DATA_FILE, converters={'summary_clean': literal_eval, 'bill_clean': literal_eval})\n",
    "\n",
    "print(DATA_SET.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(df, bottom_k_pct, top_k_pct):\n",
    "    '''\n",
    "    Remove the top and bottom n% records from the bills and summaries.\n",
    "    Expects tokenized and cleaned dataset.\n",
    "    Pass in pct as decimals.\n",
    "    '''\n",
    "    df['summary_length'] = df.summary_clean.apply(lambda x: len(x))\n",
    "    df['bill_length'] = df.bill_clean.apply(lambda x: len(x))\n",
    "    df['summary_rank'] = df.summary_length.rank(pct=True)\n",
    "    df['bill_rank'] = df.bill_length.rank(pct=True)\n",
    "    cut_df = df[(df.summary_rank >= bottom_k_pct) & (df.summary_rank <= top_k_pct) & (\n",
    "        df.bill_rank >= bottom_k_pct) & (df.bill_rank <= top_k_pct) & (df.summary_length <= df.bill_length)]\n",
    "\n",
    "    max_summary = cut_df.summary_length.max()\n",
    "    max_bill = cut_df.bill_length.max()\n",
    "\n",
    "    print('Cut ' + str(df.shape[0] - cut_df.shape[0]) + ' records.')\n",
    "    print('Count of records remaining: ', cut_df.shape[0])\n",
    "    print(f'New min summary length is {cut_df.summary_length.min()}')\n",
    "    print(f'New max summary length is {cut_df.summary_length.max()}')\n",
    "    print(f'New min bill length is {cut_df.bill_length.min()}')\n",
    "    print(f'New max bill length is {cut_df.bill_length.max()}')\n",
    "    print(f'Compression of summaries to bills is {compression(cut_df)}')\n",
    "    del cut_df['bill_length']\n",
    "    del cut_df['summary_length']\n",
    "    del cut_df['summary_rank']\n",
    "    del cut_df['bill_rank']\n",
    "    return (cut_df, max_summary, max_bill)\n",
    "    \n",
    "def compression(df):\n",
    "    return np.mean(df.summary_clean.apply(len) / df.bill_clean.apply(len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut 0 records.\n",
      "Count of records remaining:  455\n",
      "New min summary length is 2\n",
      "New max summary length is 28\n",
      "New min bill length is 54\n",
      "New max bill length is 128\n",
      "Compression of summaries to bills is 0.22755538080201299\n"
     ]
    }
   ],
   "source": [
    "if not SINGLE_RECORD:\n",
    "    SAMPLE, MAX_SUMMARY_LENGTH, MAX_BILL_LENGTH = trim_dataset(DATA_SET, 0, 1)\n",
    "else:\n",
    "    SAMPLE = DATA_SET.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, training_size, testing_size, valid_size, shuffle_data=True):\n",
    "    ''' Takes in a pandas dataframe as data. Returns three BillsDataset objects'''\n",
    "    assert training_size + testing_size + \\\n",
    "        valid_size == 1, 'Split sizes should sum to 1'\n",
    "\n",
    "    def split_index(size, index_length):\n",
    "        '''Converts decimal to # of samples to take'''\n",
    "        return int(floor(size * index_length))\n",
    "    # Split into training / testing / validation sets, assign as attributes.\n",
    "    indices = list(range(len(data)))\n",
    "\n",
    "    train_split = split_index(training_size, len(indices))\n",
    "    test_split = split_index(testing_size, len(indices))\n",
    "\n",
    "    if shuffle_data:\n",
    "        shuffle(indices)\n",
    "\n",
    "    training_data = data.iloc[indices[0:train_split]]\n",
    "    test_data = data.iloc[indices[train_split:train_split + test_split]]\n",
    "    validate_data = data.iloc[indices[train_split + test_split:]]\n",
    "\n",
    "    return (BillsDataset(training_data, 'summary_clean', 'bill_clean'),\n",
    "            BillsDataset(test_data, 'summary_clean', 'bill_clean'),\n",
    "            BillsDataset(validate_data, 'summary_clean', 'bill_clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BillsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Congressional Bills\n",
    "    Adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    and from https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, summaries_col, bills_col, transform=None):\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        self.labels = summaries_col\n",
    "        self.texts = bills_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        summary = self.data.loc[idx, self.labels]\n",
    "        bill = self.data.loc[idx, self.texts]\n",
    "\n",
    "        sample = {'summary': summary, 'bill': bill}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA, TEST_DATA, VALIDATE_DATA = split(SAMPLE, TRAINING_SIZE, TESTING_SIZE, VALIDATION_SIZE, shuffle_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(training_data, summary_col='summary', bill_col='bill', summaries=True, bills=True):\n",
    "    '''\n",
    "    Builds a Vocab object for a Dataset object.\n",
    "    If default BillsDataset object, summary and bill are dict keys.\n",
    "    '''\n",
    "    counter_words = Counter()\n",
    "\n",
    "    for index in range(len(training_data)):\n",
    "        example = training_data[index]\n",
    "        if summaries:\n",
    "          counter_words.update(example[summary_col])\n",
    "        if bills:\n",
    "          counter_words.update(example[bill_col])\n",
    "\n",
    "    return Vocab(counter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 2583\n"
     ]
    }
   ],
   "source": [
    "VOCAB = build_vocab(TRAIN_DATA, 'summary','bill')\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "PAD_TOKEN = VOCAB.stoi['<pad>']\n",
    "print(f'Vocab size is {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the VOCAB object, we'll leverage GLoVe pretrained embeddings. GLOVE_VECS is size (VOCAB, 300)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_glove(vocab):\n",
    "  '''\n",
    "  Return the pretrained embeddings for the vocab words.\n",
    "  '''\n",
    "  # https://nlp.stanford.edu/projects/glove/\n",
    "  VECTORS_CACHE_DIR = './.vector_cache'\n",
    "  glove = GloVe(name='6B', cache=VECTORS_CACHE_DIR)\n",
    "  glove_vectors = glove.get_vecs_by_tokens(vocab.itos)\n",
    "  return glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./.vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:58<00:00, 6873.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2583, 300])\n"
     ]
    }
   ],
   "source": [
    "GLOVE_VECS = build_glove(VOCAB)\n",
    "print(GLOVE_VECS.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, vocab, max_summary_length, max_bill_length, **kwargs):\n",
    "    '''\n",
    "    kwargs for training_data:, test_data:, and validation_data:.\n",
    "    Returns dict of dataloaders based on arg name input\n",
    "    '''\n",
    "    dataloaders = {}\n",
    "    # Set params for the collate function\n",
    "    collate_fn = partial(\n",
    "        \n",
    "        collate_bills_fn, vocab=vocab, max_summary_length=max_summary_length, max_bill_length=max_bill_length)\n",
    "\n",
    "    for dataset_name, data in kwargs.items():\n",
    "        if len(data) > 0:\n",
    "            dataloaders[dataset_name] = DataLoader(\n",
    "                data, batch_size=batch_size,\n",
    "                shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_bills_fn(batch, vocab, max_summary_length=512, max_bill_length=2048):\n",
    "    '''\n",
    "    Collates the batches into the dataloader. Pads unequal lengths with zeros\n",
    "    based on the max lengths given.\n",
    "    '''\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for idx, text_dict in enumerate(batch):\n",
    "        # Get the label and the text\n",
    "        label = text_dict['summary']\n",
    "        # Reversing it may improve performance re / research\n",
    "        text = text_dict['bill'] #[::-1]\n",
    "        # Output for the sample\n",
    "        label_vectors = []\n",
    "        text_vectors = []\n",
    "        # Check lengths; see how much to pad\n",
    "        label_length = len(label)\n",
    "        text_length = len(text)\n",
    "        labels_to_pad = max_summary_length - label_length\n",
    "        text_to_pad = max_bill_length - text_length\n",
    "\n",
    "        if label_length < max_summary_length:\n",
    "            label.extend(['<pad>'] * labels_to_pad)\n",
    "\n",
    "        if text_length < max_bill_length:\n",
    "            text.extend(['<pad>'] * text_to_pad)\n",
    "\n",
    "        for word in label:\n",
    "            label_vectors.append(vocab.stoi[word])\n",
    "        for word in text:\n",
    "            text_vectors.append(vocab.stoi[word])\n",
    "\n",
    "\n",
    "        labels.append(torch.LongTensor(label_vectors))\n",
    "        texts.append(torch.LongTensor(text_vectors))\n",
    "    # Returns shape of (batch size, max_summary (or bill)_length) for each\n",
    "    return (torch.stack(labels), torch.stack(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALOADERS_DICT = get_dataloaders(BATCH_SIZE, VOCAB, MAX_SUMMARY_LENGTH, MAX_BILL_LENGTH, train_data=TRAIN_DATA, test_data=TEST_DATA, validate_data=VALIDATE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in (number of samples in each set / batch size) tuples (or iteration steps) of size (batch size, max length).\n",
    "\n",
    "So if size of training is 15, and batch size is 5, enumerating through dataloader will have 3 steps of inputting label (batch size, summary_length) and text (batch size, bill length).\n",
    "\n",
    "We can view the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_data', 'test_data', 'validate_data'])\n",
      "Summary: tensor([[   9,  915,  201,    5, 1039,   15,  735, 1054,    5,  827,    2,  715,\n",
      "            4,  530,    4,   10,   11,    3,    8,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  638,  440,  119, 1567,   11,    4,   14,   64,    2,  145,   11,\n",
      "           10,   37,  259,    2,   27,   26,    3,  166,  127,   24,  111,  103,\n",
      "          141,    3,    8,    1],\n",
      "        [   9, 1019,  440,   63,   93,   11,  152,  274,  204,   15,  160,   15,\n",
      "          441,    6, 1741,  836,  194,   51,   14,    3,    8,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,   10,   37,  434,    2,  330,    4,  162, 2099,    6,  439,  447,\n",
      "          115,  377,  452,    6, 1057,  946, 1358,  198,   15,  162, 2100,    3,\n",
      "            8,    1,    1,    1],\n",
      "        [   9, 1308,    6, 1309, 1179,  184,   20,    6,  386,  822,   11,    4,\n",
      "           14,   10,   37,  507,    2,  183,    4, 2102, 1838,    8,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  750,  615,  440,  426,   11,   10,   37,   97,    2, 1598,    5,\n",
      "          615,  426,   11,    4,   14,    5,  188,    2,  216, 1045,   94,   71,\n",
      "          122,    3,    8,    1],\n",
      "        [   9,   10,   37,  834,    2,   86,    4,  106,   38,   95,   28,   14,\n",
      "            3,    8,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  282,  232,  303,   11,   10,   37,  208,    2,   63,  683,    6,\n",
      "          282,  171,   47,  783,   64,  784,  232,  759,    3,    8,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  175,    2,  165,    4,    2,   13,   17,  166,  127,  140,   99,\n",
      "           14, 1294, 1495,   12, 1318,  455,   52,    2, 1624, 2081,  145,  158,\n",
      "          700,  288,    8,    1],\n",
      "        [   9,  142,   15,    2,   72,    4,  317,  311,  316,    3,    8,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  530,  505,  253,   72,   11,   10,   37,  507,  628,  725,  463,\n",
      "            3,    8,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,   10,   37,   97,    2,   92,   91,   78,    5,   87,   85,   14,\n",
      "            2,   79,   69,   15,  168,  300,  293,  155,    3,    8,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  103,  263,  376,  177,   11,  152,   97,    2, 1074,  204,  220,\n",
      "           14,    5,   87,  103,  263,  376,    8,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,   10,   37,  633,   47,  170,   28,  136,  297,  438, 2345,  226,\n",
      "          590,    6,   70, 1265,  116, 1894,  451, 2567,  667,  260,    5, 1461,\n",
      "           64,  565,    3,    8],\n",
      "        [   9,   10,   37,  142,   15,  153,  973, 1175, 1409, 1519,   79,   69,\n",
      "           15,  331, 1128, 1174, 2005,    3,    8,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  447,    2,  751,  181,  756,   71,    3,  208,    2,  146,    4,\n",
      "          133, 1528,   15,  584,  779,    6,  782,  133,    3,    8,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9, 1324, 1554,  296,   15,  182, 1005,   11,   10,   37,  132,    2,\n",
      "          296,    6,  272,   93,  407,   24,  111,  103,    3,    8,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  315,  712,   11,    4,   14,   10,   37,  474,  315,  201,    5,\n",
      "            2, 1051,    6,  987, 1090,  127,  283,  647,    6, 1072,  410, 1042,\n",
      "            3,    8,    1,    1],\n",
      "        [   9,  104,  391, 1364,   15,  221,  785,    3,    8,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [   9,  366,   72,    6,   93,   11,    4,   14,   10,   37, 1281,    6,\n",
      "          474,  729,  608,  258,  263, 1144,  305,  686,  176,   84,  258,  263,\n",
      "          376,    3,    8,    1]]) ['<sos>', 'creating', 'access', 'to', 'rehabilitation', 'for', 'every', 'senior', 'to', 'study', 'the', 'cost', 'of', 'impact', 'of', 'this', 'act', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] \n",
      "\n",
      "Bill: tensor([[   9,   45,  915,  ...,    1,    1,    1],\n",
      "        [   9,  638,  440,  ...,    1,    1,    1],\n",
      "        [   9,   45, 1019,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   9,   45,  315,  ...,    1,    1,    1],\n",
      "        [   9,   45,  104,  ...,    1,    1,    1],\n",
      "        [   9,   45,  366,  ...,    1,    1,    1]]) ['<sos>', ':', 'creating', 'access', 'to', 'rehabilitation', 'for', 'every', 'senior', 'act', 'of', '####', 'u', '.s', '.', 'house', 'of', 'representatives', '####-##-##', 'text/xml', 'en', 'pursuant', 'to', 'title', '##', 'section', '###', 'of', 'the', 'united', 'states', 'code,', 'this', 'file', 'is', 'not', 'subject', 'to', 'copyright', 'protection', 'and', 'is', 'in', 'the', 'public', 'domain', '.', 'i###th', 'congress#st', 'sessionh', '.', 'r', '.', '####in', 'the', 'house', 'of', 'representativesnovember', '##,', '####mr', '.', 'renacci', ',', 'no', 'later', 'than', 'june', '#,', '####', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(DATALOADERS_DICT.keys())\n",
    "\n",
    "l, f = next(iter(DATALOADERS_DICT['train_data']))\n",
    "\n",
    "if BATCH_SIZE > 1:\n",
    "    print(\"Summary:\", l, [VOCAB.itos[x] for x in l[0].squeeze(0)], '\\n')\n",
    "    print(\"Bill:\", f, [VOCAB.itos[x] for x in f[0].squeeze(0)])\n",
    "else:\n",
    "    print(\"Summary:\", l, '\\n')\n",
    "    print(\"Bill:\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![Image](seq2seq.png)\n",
    "# Image Credit to Yoav Goldberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Our model is a Sequence To Sequence model with an LSTM-based Encoder and Decoder.\n",
    "\n",
    "Given an input sequence (the text of a Congressional bill) $W_{0:n}$ of length $n$, we produce a target sequence (bill summary), $U_{0:m}$ of length $m$, where $n$ > $m$. Let $<sos>$ and $<eos>$ be tokens indicating the start and end of text, respectively; meaning $W_{0}, U_{0} = <sos>$ and $W_{n}, U_{m} = <eos>$.\n",
    "\n",
    "Furthermore, we define embedding $E \\in n \\times e$, hidden dimension $H$, and vocab size $|V|$. Additionally, for LSTM $L$ in both the $ENCODER$ and $DECODER$, the hidden state $h$ and cell state $c$ components of state $s$ are initialized as $h_0, c_0 = \\vec{0}$. Lastly, $L_{j}$ is layer $j$ of the LSTM $L$. \n",
    "\n",
    "The Sequence To Sequence model is broadly defined as:\n",
    "\n",
    "$\\underset{n \\times H}{C} = ENCODER_{RNN}(W_{0:n})$\n",
    "\n",
    "$\\underset{1 \\times |V|}{\\hat{u}}= DECODER_{RNN}(C; \\hat{y}_{0:m-1}; s_{0:m-1})$\n",
    "\n",
    "where $ENCODER$ produces context vector $C$ from text $W$ as follows: \n",
    "\n",
    "At step $i$ of input sequence $W_{0:n}$, for $0 \\leq i \\leq n$:\n",
    "\n",
    "$\\underset{1 \\times e}{x_i} = E_{W_i}$\n",
    "\n",
    "and passing to $x_{i:n}$ PyTorch, we obtain:\n",
    "\n",
    "$\\underset{n \\times H}{\\hat{y}_{i:n}}, \\underset{1 \\times H}{h_1}, \\underset{1 \\times H}{c_1} = LSTM_{L_{0}}(x_{i} , (h_{0}, c_{0}))$\n",
    "\n",
    "And for layer $j$ of LSTM $L$,\n",
    "\n",
    "$\\underset{1 \\times H}{\\hat{y}^{j}_{i:n}}, \\underset{1 \\times H}{h_1}, \\underset{1 \\times H}{c_1}= LSTM_{L_{j}}(\\hat{y}^{j-1}_{i:n}, (h^{j-1}, c^{j-1}))$\n",
    "\n",
    "where the last layer produces ${C}$ from the final state $s^{enc}$, made up of the final $h$ and $c$ results.\n",
    "\n",
    "Next, $DECODER$ decodes context vector $C$ in combination with state $s_{i-1}$ and predicted value $\\hat{y}_{i-1}$, producing $\\hat{u}_{1:m}$ as follows:\n",
    "\n",
    "$\\underset{1 \\times |V|}{\\hat{y}_i} = O(LSTM_{L_{i}}(c; \\hat{y}_{i-1}; s_{i-1}))$\n",
    "\n",
    "in which $O$ is the softmax function and $h_0, c_0 = C$. $\\hat{y}_i$ is the predicted probability distribution for $w_{i+1}$.\n",
    "\n",
    "For each $\\hat{y}_i$ of the $DECODER$, we calculate the loss $\\ell$ by $\\log \\widehat{y}_{i_{[w_{i+1}]}}$. Across $\\hat{y}_{1:m}$, it is the average cross entropy loss, $\\ell = \\frac{1}{m}\\sum_{i=1}^{m} \\log \\widehat{y}_{i_{[w_{i+1}]}}$.\n",
    "\n",
    "With this step, we preform end to end backpropagation through the $DECODER$ to the $ENCODER$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 pretrained_embeddings,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 pad_token,\n",
    "                 freeze_glove=False,\n",
    "                 ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            pretrained_embeddings, padding_idx=pad_token, freeze=freeze_glove)\n",
    "\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers, batch_first=False, dropout=dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "      '''\n",
    "      Text size is batch size, bill length\n",
    "      '''\n",
    "      # Embedded size is sequence length, batch size, glove vecs size\n",
    "      embedded = self.embedding(text) #.view(-1, len(text), 300)\n",
    "      # This above is batch first, but the rnn takes seq length, batch, size\n",
    "      embedded = embedded.permute(1, 0, 2)\n",
    "      # outputs is size (sequence length, batch_size, hidden_size)\n",
    "      outputs, hidden = self.rnn(embedded)\n",
    "      # each element in hidden is size (num_layers * num_directions (which is 1 unless using a bidirectional LSTM), batch_size, hidden_size)\n",
    "      return outputs.float(), tuple([v.float() for v in hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_dim, hidden_dim, pretrained_embeddings, num_layers, dropout, pad_token, freeze_glove=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        # output dim should equal VOCAB size\n",
    "        self.output_dim = output_dim\n",
    "        # hidden_dim should equal the hidden_dim of the ENCODER\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(self.input_size, hidden_dim, num_layers=self.num_layers, batch_first=False, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim) # transfer to device?\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            pretrained_embeddings, padding_idx=pad_token, freeze=freeze_glove)\n",
    "\n",
    "    def forward(self, input_word, hidden):\n",
    " \n",
    "        embedded = self.embedding(input_word)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        output, hidden= self.rnn(embedded, hidden)\n",
    "        linear = self.fc_out(output.squeeze(0))\n",
    "        # prediction = self.softmax(linear)\n",
    "\n",
    "        return linear, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, label, text):\n",
    "        batch_size = label.shape[0]\n",
    "        label_length = label.shape[1]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        decode_outputs = torch.zeros(label_length, batch_size, vocab_size)\n",
    "        output, hidden0 = self.encoder(text)\n",
    "        input_word = label[:, 0]\n",
    "\n",
    "        for t in range(1, label_length):\n",
    "            input_word = input_word.unsqueeze(1)\n",
    "            output, hidden = self.decoder(input_word, hidden0)\n",
    "            decode_outputs[t] = output\n",
    "            # top_choice = torch.max(output, dim=1)[1]\n",
    "            input_word = label[:, t]\n",
    "            hidden0 = hidden\n",
    "\n",
    "        # return top_choice, decode_outputs\n",
    "        return decode_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = GLOVE_VECS.size()[1] #300\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 4\n",
    "GRAD_CLIP = 2\n",
    "DROPOUT=.5\n",
    "LOG_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, GLOVE_VECS, NUM_LAYERS, DROPOUT, PAD_TOKEN, False).to(DEVICE)\n",
    "decoder = Decoder(INPUT_SIZE, VOCAB_SIZE, HIDDEN_SIZE, GLOVE_VECS, NUM_LAYERS, DROPOUT, PAD_TOKEN, False).to(DEVICE)\n",
    "seq2seq = Seq2Seq(encoder, decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = optim.Adam(decoder.parameters(), lr=.0003)\n",
    "OPTIMIZER2 = optim.Adam(encoder.parameters(), lr=.0003)\n",
    "LOSS_FN = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_an_epoch(model, dataloader, optimizer, optimizer2, loss_function, log_interval, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        if USE_CUDA:\n",
    "          label = label.to(device)\n",
    "          text = text.to(device)\n",
    "        outputs = model(label, text)\n",
    "        output_dim = outputs.shape[-1]\n",
    "        \n",
    "        output = outputs.view(-1, output_dim)\n",
    "        target = label.view(-1)\n",
    "\n",
    "        if USE_CUDA:\n",
    "          output = output.to(device)\n",
    "          target = target.to(device)\n",
    "\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "              \n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'Iteration: {idx}; Loss: {loss:.3f}.')\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, dataloader, loss_function, device):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for i, (label, text) in enumerate(dataloader):\n",
    "      if USE_CUDA:\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "      outputs = model(label, text)\n",
    "      output_dim = outputs.shape[-1]\n",
    "      \n",
    "      output = outputs.view(-1, output_dim)\n",
    "      target = label.view(-1)\n",
    "\n",
    "      if USE_CUDA:\n",
    "        output = output.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "      loss = loss_function(output, target)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL WRITTEN BY PYTORCH TUTORIAL SEQ2SEQ\n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# Author: Sean Robertson\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10; Loss: 7.211.\n",
      "0m 6s (- 551m 53s) (1 0%)\n",
      "Iteration: 10; Loss: 5.861.\n",
      "0m 13s (- 569m 39s) (2 0%)\n",
      "Iteration: 10; Loss: 5.756.\n",
      "0m 20s (- 574m 13s) (3 0%)\n",
      "Iteration: 10; Loss: 5.686.\n",
      "0m 27s (- 576m 49s) (4 0%)\n",
      "Iteration: 10; Loss: 5.676.\n",
      "0m 34s (- 578m 40s) (5 0%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "0m 41s (- 579m 57s) (6 0%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "0m 48s (- 580m 42s) (7 0%)\n",
      "Iteration: 10; Loss: 5.710.\n",
      "0m 55s (- 580m 40s) (8 0%)\n",
      "Iteration: 10; Loss: 5.714.\n",
      "1m 2s (- 581m 12s) (9 0%)\n",
      "Iteration: 10; Loss: 5.620.\n",
      "1m 9s (- 581m 16s) (10 0%)\n",
      "Iteration: 10; Loss: 5.823.\n",
      "1m 16s (- 581m 25s) (11 0%)\n",
      "Iteration: 10; Loss: 5.675.\n",
      "1m 23s (- 581m 25s) (12 0%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "1m 30s (- 581m 23s) (13 0%)\n",
      "Iteration: 10; Loss: 5.863.\n",
      "1m 37s (- 581m 13s) (14 0%)\n",
      "Iteration: 10; Loss: 5.757.\n",
      "1m 44s (- 581m 18s) (15 0%)\n",
      "Iteration: 10; Loss: 5.854.\n",
      "1m 51s (- 581m 20s) (16 0%)\n",
      "Iteration: 10; Loss: 5.719.\n",
      "1m 59s (- 581m 32s) (17 0%)\n",
      "Iteration: 10; Loss: 5.579.\n",
      "2m 6s (- 581m 27s) (18 0%)\n",
      "Iteration: 10; Loss: 5.723.\n",
      "2m 13s (- 581m 25s) (19 0%)\n",
      "Iteration: 10; Loss: 5.698.\n",
      "2m 20s (- 581m 26s) (20 0%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "2m 27s (- 581m 11s) (21 0%)\n",
      "Iteration: 10; Loss: 5.714.\n",
      "2m 34s (- 581m 5s) (22 0%)\n",
      "Iteration: 10; Loss: 5.727.\n",
      "2m 41s (- 580m 57s) (23 0%)\n",
      "Iteration: 10; Loss: 5.878.\n",
      "2m 48s (- 580m 54s) (24 0%)\n",
      "Iteration: 10; Loss: 5.820.\n",
      "2m 55s (- 580m 55s) (25 0%)\n",
      "Iteration: 10; Loss: 5.668.\n",
      "3m 2s (- 580m 48s) (26 0%)\n",
      "Iteration: 10; Loss: 5.761.\n",
      "3m 9s (- 580m 46s) (27 0%)\n",
      "Iteration: 10; Loss: 5.795.\n",
      "3m 16s (- 580m 37s) (28 0%)\n",
      "Iteration: 10; Loss: 5.828.\n",
      "3m 23s (- 580m 27s) (29 0%)\n",
      "Iteration: 10; Loss: 5.636.\n",
      "3m 30s (- 580m 21s) (30 0%)\n",
      "Iteration: 10; Loss: 5.771.\n",
      "3m 37s (- 580m 20s) (31 0%)\n",
      "Iteration: 10; Loss: 5.735.\n",
      "3m 44s (- 580m 13s) (32 0%)\n",
      "Iteration: 10; Loss: 5.783.\n",
      "3m 51s (- 580m 13s) (33 0%)\n",
      "Iteration: 10; Loss: 5.718.\n",
      "3m 58s (- 580m 8s) (34 0%)\n",
      "Iteration: 10; Loss: 5.725.\n",
      "4m 5s (- 580m 4s) (35 0%)\n",
      "Iteration: 10; Loss: 5.698.\n",
      "4m 12s (- 579m 54s) (36 0%)\n",
      "Iteration: 10; Loss: 5.717.\n",
      "4m 19s (- 579m 53s) (37 0%)\n",
      "Iteration: 10; Loss: 5.682.\n",
      "4m 26s (- 579m 50s) (38 0%)\n",
      "Iteration: 10; Loss: 5.712.\n",
      "4m 33s (- 579m 48s) (39 0%)\n",
      "Iteration: 10; Loss: 5.690.\n",
      "4m 40s (- 579m 42s) (40 0%)\n",
      "Iteration: 10; Loss: 5.923.\n",
      "4m 47s (- 579m 35s) (41 0%)\n",
      "Iteration: 10; Loss: 5.694.\n",
      "4m 54s (- 579m 24s) (42 0%)\n",
      "Iteration: 10; Loss: 5.942.\n",
      "5m 1s (- 579m 23s) (43 0%)\n",
      "Iteration: 10; Loss: 5.752.\n",
      "5m 8s (- 579m 15s) (44 0%)\n",
      "Iteration: 10; Loss: 5.901.\n",
      "5m 15s (- 579m 12s) (45 0%)\n",
      "Iteration: 10; Loss: 5.717.\n",
      "5m 22s (- 579m 8s) (46 0%)\n",
      "Iteration: 10; Loss: 5.673.\n",
      "5m 29s (- 579m 3s) (47 0%)\n",
      "Iteration: 10; Loss: 5.771.\n",
      "5m 36s (- 578m 57s) (48 0%)\n",
      "Iteration: 10; Loss: 5.803.\n",
      "5m 43s (- 578m 49s) (49 0%)\n",
      "Iteration: 10; Loss: 5.695.\n",
      "5m 50s (- 578m 47s) (50 1%)\n",
      "Iteration: 10; Loss: 5.790.\n",
      "5m 57s (- 578m 44s) (51 1%)\n",
      "Iteration: 10; Loss: 5.723.\n",
      "6m 4s (- 578m 40s) (52 1%)\n",
      "Iteration: 10; Loss: 5.473.\n",
      "6m 11s (- 578m 33s) (53 1%)\n",
      "Iteration: 10; Loss: 5.537.\n",
      "6m 18s (- 578m 23s) (54 1%)\n",
      "Iteration: 10; Loss: 5.844.\n",
      "6m 25s (- 578m 16s) (55 1%)\n",
      "Iteration: 10; Loss: 5.755.\n",
      "6m 32s (- 578m 11s) (56 1%)\n",
      "Iteration: 10; Loss: 5.816.\n",
      "6m 39s (- 578m 1s) (57 1%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "6m 46s (- 577m 54s) (58 1%)\n",
      "Iteration: 10; Loss: 5.786.\n",
      "6m 53s (- 577m 44s) (59 1%)\n",
      "Iteration: 10; Loss: 5.796.\n",
      "7m 0s (- 577m 35s) (60 1%)\n",
      "Iteration: 10; Loss: 5.713.\n",
      "7m 7s (- 577m 30s) (61 1%)\n",
      "Iteration: 10; Loss: 5.641.\n",
      "7m 15s (- 577m 27s) (62 1%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "7m 22s (- 577m 19s) (63 1%)\n",
      "Iteration: 10; Loss: 5.865.\n",
      "7m 28s (- 577m 7s) (64 1%)\n",
      "Iteration: 10; Loss: 5.846.\n",
      "7m 35s (- 576m 57s) (65 1%)\n",
      "Iteration: 10; Loss: 5.721.\n",
      "7m 42s (- 576m 49s) (66 1%)\n",
      "Iteration: 10; Loss: 5.803.\n",
      "7m 49s (- 576m 38s) (67 1%)\n",
      "Iteration: 10; Loss: 5.519.\n",
      "7m 56s (- 576m 34s) (68 1%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "8m 4s (- 576m 32s) (69 1%)\n",
      "Iteration: 10; Loss: 5.646.\n",
      "8m 11s (- 576m 22s) (70 1%)\n",
      "Iteration: 10; Loss: 5.649.\n",
      "8m 18s (- 576m 18s) (71 1%)\n",
      "Iteration: 10; Loss: 5.693.\n",
      "8m 25s (- 576m 11s) (72 1%)\n",
      "Iteration: 10; Loss: 5.753.\n",
      "8m 32s (- 576m 6s) (73 1%)\n",
      "Iteration: 10; Loss: 5.889.\n",
      "8m 39s (- 576m 1s) (74 1%)\n",
      "Iteration: 10; Loss: 5.652.\n",
      "8m 46s (- 575m 51s) (75 1%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "8m 53s (- 575m 45s) (76 1%)\n",
      "Iteration: 10; Loss: 5.796.\n",
      "9m 0s (- 575m 41s) (77 1%)\n",
      "Iteration: 10; Loss: 5.712.\n",
      "9m 7s (- 575m 34s) (78 1%)\n",
      "Iteration: 10; Loss: 5.597.\n",
      "9m 14s (- 575m 27s) (79 1%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "9m 21s (- 575m 21s) (80 1%)\n",
      "Iteration: 10; Loss: 5.892.\n",
      "9m 28s (- 575m 16s) (81 1%)\n",
      "Iteration: 10; Loss: 5.773.\n",
      "9m 35s (- 575m 13s) (82 1%)\n",
      "Iteration: 10; Loss: 5.698.\n",
      "9m 42s (- 575m 6s) (83 1%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "9m 49s (- 574m 59s) (84 1%)\n",
      "Iteration: 10; Loss: 5.786.\n",
      "9m 56s (- 574m 52s) (85 1%)\n",
      "Iteration: 10; Loss: 5.664.\n",
      "10m 3s (- 574m 47s) (86 1%)\n",
      "Iteration: 10; Loss: 5.712.\n",
      "10m 10s (- 574m 40s) (87 1%)\n",
      "Iteration: 10; Loss: 6.007.\n",
      "10m 17s (- 574m 33s) (88 1%)\n",
      "Iteration: 10; Loss: 5.516.\n",
      "10m 24s (- 574m 26s) (89 1%)\n",
      "Iteration: 10; Loss: 5.876.\n",
      "10m 31s (- 574m 20s) (90 1%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "10m 38s (- 574m 14s) (91 1%)\n",
      "Iteration: 10; Loss: 5.875.\n",
      "10m 45s (- 574m 8s) (92 1%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "10m 52s (- 574m 1s) (93 1%)\n",
      "Iteration: 10; Loss: 5.677.\n",
      "10m 59s (- 573m 54s) (94 1%)\n",
      "Iteration: 10; Loss: 5.755.\n",
      "11m 6s (- 573m 48s) (95 1%)\n",
      "Iteration: 10; Loss: 5.817.\n",
      "11m 13s (- 573m 44s) (96 1%)\n",
      "Iteration: 10; Loss: 5.723.\n",
      "11m 20s (- 573m 39s) (97 1%)\n",
      "Iteration: 10; Loss: 5.736.\n",
      "11m 27s (- 573m 33s) (98 1%)\n",
      "Iteration: 10; Loss: 5.698.\n",
      "11m 34s (- 573m 23s) (99 1%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "11m 41s (- 573m 15s) (100 2%)\n",
      "Iteration: 10; Loss: 5.747.\n",
      "11m 49s (- 573m 10s) (101 2%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "11m 56s (- 573m 3s) (102 2%)\n",
      "Iteration: 10; Loss: 5.748.\n",
      "12m 3s (- 572m 58s) (103 2%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "12m 10s (- 572m 49s) (104 2%)\n",
      "Iteration: 10; Loss: 5.621.\n",
      "12m 17s (- 572m 40s) (105 2%)\n",
      "Iteration: 10; Loss: 5.694.\n",
      "12m 24s (- 572m 34s) (106 2%)\n",
      "Iteration: 10; Loss: 5.753.\n",
      "12m 31s (- 572m 29s) (107 2%)\n",
      "Iteration: 10; Loss: 5.804.\n",
      "12m 38s (- 572m 22s) (108 2%)\n",
      "Iteration: 10; Loss: 5.610.\n",
      "12m 45s (- 572m 18s) (109 2%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "12m 52s (- 572m 13s) (110 2%)\n",
      "Iteration: 10; Loss: 5.641.\n",
      "12m 59s (- 572m 7s) (111 2%)\n",
      "Iteration: 10; Loss: 5.594.\n",
      "13m 6s (- 572m 3s) (112 2%)\n",
      "Iteration: 10; Loss: 5.731.\n",
      "13m 13s (- 571m 57s) (113 2%)\n",
      "Iteration: 10; Loss: 5.807.\n",
      "13m 20s (- 571m 51s) (114 2%)\n",
      "Iteration: 10; Loss: 5.718.\n",
      "13m 27s (- 571m 43s) (115 2%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "13m 34s (- 571m 36s) (116 2%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "13m 41s (- 571m 31s) (117 2%)\n",
      "Iteration: 10; Loss: 5.763.\n",
      "13m 48s (- 571m 24s) (118 2%)\n",
      "Iteration: 10; Loss: 5.714.\n",
      "13m 55s (- 571m 23s) (119 2%)\n",
      "Iteration: 10; Loss: 5.749.\n",
      "14m 2s (- 571m 17s) (120 2%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "14m 9s (- 571m 11s) (121 2%)\n",
      "Iteration: 10; Loss: 5.688.\n",
      "14m 16s (- 571m 4s) (122 2%)\n",
      "Iteration: 10; Loss: 5.692.\n",
      "14m 23s (- 570m 56s) (123 2%)\n",
      "Iteration: 10; Loss: 5.769.\n",
      "14m 31s (- 570m 50s) (124 2%)\n",
      "Iteration: 10; Loss: 5.707.\n",
      "14m 38s (- 570m 43s) (125 2%)\n",
      "Iteration: 10; Loss: 5.646.\n",
      "14m 45s (- 570m 36s) (126 2%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "14m 52s (- 570m 29s) (127 2%)\n",
      "Iteration: 10; Loss: 5.696.\n",
      "14m 59s (- 570m 23s) (128 2%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "15m 6s (- 570m 18s) (129 2%)\n",
      "Iteration: 10; Loss: 5.748.\n",
      "15m 13s (- 570m 11s) (130 2%)\n",
      "Iteration: 10; Loss: 5.660.\n",
      "15m 20s (- 570m 4s) (131 2%)\n",
      "Iteration: 10; Loss: 5.577.\n",
      "15m 27s (- 570m 0s) (132 2%)\n",
      "Iteration: 10; Loss: 5.671.\n",
      "15m 34s (- 569m 54s) (133 2%)\n",
      "Iteration: 10; Loss: 5.638.\n",
      "15m 41s (- 569m 46s) (134 2%)\n",
      "Iteration: 10; Loss: 5.593.\n",
      "15m 48s (- 569m 41s) (135 2%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "15m 55s (- 569m 35s) (136 2%)\n",
      "Iteration: 10; Loss: 5.508.\n",
      "16m 2s (- 569m 29s) (137 2%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "16m 9s (- 569m 22s) (138 2%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "16m 16s (- 569m 14s) (139 2%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "16m 23s (- 569m 7s) (140 2%)\n",
      "Iteration: 10; Loss: 5.759.\n",
      "16m 30s (- 569m 0s) (141 2%)\n",
      "Iteration: 10; Loss: 5.640.\n",
      "16m 37s (- 568m 54s) (142 2%)\n",
      "Iteration: 10; Loss: 5.736.\n",
      "16m 44s (- 568m 49s) (143 2%)\n",
      "Iteration: 10; Loss: 5.768.\n",
      "16m 51s (- 568m 42s) (144 2%)\n",
      "Iteration: 10; Loss: 5.660.\n",
      "16m 58s (- 568m 37s) (145 2%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "17m 5s (- 568m 30s) (146 2%)\n",
      "Iteration: 10; Loss: 5.655.\n",
      "17m 13s (- 568m 23s) (147 2%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "17m 20s (- 568m 17s) (148 2%)\n",
      "Iteration: 10; Loss: 5.527.\n",
      "17m 27s (- 568m 10s) (149 2%)\n",
      "Iteration: 10; Loss: 5.736.\n",
      "17m 34s (- 568m 5s) (150 3%)\n",
      "Iteration: 10; Loss: 5.661.\n",
      "17m 41s (- 567m 59s) (151 3%)\n",
      "Iteration: 10; Loss: 5.674.\n",
      "17m 48s (- 567m 52s) (152 3%)\n",
      "Iteration: 10; Loss: 5.928.\n",
      "17m 55s (- 567m 43s) (153 3%)\n",
      "Iteration: 10; Loss: 5.729.\n",
      "18m 2s (- 567m 34s) (154 3%)\n",
      "Iteration: 10; Loss: 5.560.\n",
      "18m 9s (- 567m 28s) (155 3%)\n",
      "Iteration: 10; Loss: 5.707.\n",
      "18m 16s (- 567m 21s) (156 3%)\n",
      "Iteration: 10; Loss: 5.636.\n",
      "18m 23s (- 567m 15s) (157 3%)\n",
      "Iteration: 10; Loss: 5.619.\n",
      "18m 30s (- 567m 7s) (158 3%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "18m 37s (- 566m 58s) (159 3%)\n",
      "Iteration: 10; Loss: 5.689.\n",
      "18m 44s (- 566m 52s) (160 3%)\n",
      "Iteration: 10; Loss: 5.790.\n",
      "18m 51s (- 566m 44s) (161 3%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "18m 58s (- 566m 38s) (162 3%)\n",
      "Iteration: 10; Loss: 5.529.\n",
      "19m 5s (- 566m 32s) (163 3%)\n",
      "Iteration: 10; Loss: 5.562.\n",
      "19m 12s (- 566m 25s) (164 3%)\n",
      "Iteration: 10; Loss: 5.592.\n",
      "19m 19s (- 566m 17s) (165 3%)\n",
      "Iteration: 10; Loss: 5.633.\n",
      "19m 26s (- 566m 10s) (166 3%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "19m 33s (- 566m 3s) (167 3%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "19m 40s (- 565m 57s) (168 3%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "19m 47s (- 565m 50s) (169 3%)\n",
      "Iteration: 10; Loss: 5.766.\n",
      "19m 54s (- 565m 43s) (170 3%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "20m 1s (- 565m 38s) (171 3%)\n",
      "Iteration: 10; Loss: 5.519.\n",
      "20m 8s (- 565m 31s) (172 3%)\n",
      "Iteration: 10; Loss: 5.639.\n",
      "20m 15s (- 565m 25s) (173 3%)\n",
      "Iteration: 10; Loss: 5.675.\n",
      "20m 22s (- 565m 18s) (174 3%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "20m 29s (- 565m 11s) (175 3%)\n",
      "Iteration: 10; Loss: 5.469.\n",
      "20m 37s (- 565m 5s) (176 3%)\n",
      "Iteration: 10; Loss: 5.654.\n",
      "20m 44s (- 564m 58s) (177 3%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "20m 51s (- 564m 51s) (178 3%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "20m 58s (- 564m 45s) (179 3%)\n",
      "Iteration: 10; Loss: 5.606.\n",
      "21m 5s (- 564m 40s) (180 3%)\n",
      "Iteration: 10; Loss: 5.699.\n",
      "21m 12s (- 564m 32s) (181 3%)\n",
      "Iteration: 10; Loss: 5.684.\n",
      "21m 19s (- 564m 26s) (182 3%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "21m 26s (- 564m 19s) (183 3%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "21m 33s (- 564m 12s) (184 3%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "21m 40s (- 564m 5s) (185 3%)\n",
      "Iteration: 10; Loss: 5.708.\n",
      "21m 47s (- 563m 59s) (186 3%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "21m 54s (- 563m 53s) (187 3%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "22m 1s (- 563m 45s) (188 3%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "22m 8s (- 563m 39s) (189 3%)\n",
      "Iteration: 10; Loss: 5.563.\n",
      "22m 15s (- 563m 33s) (190 3%)\n",
      "Iteration: 10; Loss: 5.818.\n",
      "22m 22s (- 563m 29s) (191 3%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "22m 29s (- 563m 23s) (192 3%)\n",
      "Iteration: 10; Loss: 5.410.\n",
      "22m 36s (- 563m 16s) (193 3%)\n",
      "Iteration: 10; Loss: 5.656.\n",
      "22m 44s (- 563m 10s) (194 3%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "22m 51s (- 563m 4s) (195 3%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "22m 58s (- 562m 58s) (196 3%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "23m 5s (- 562m 52s) (197 3%)\n",
      "Iteration: 10; Loss: 5.510.\n",
      "23m 12s (- 562m 45s) (198 3%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "23m 19s (- 562m 38s) (199 3%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "23m 26s (- 562m 31s) (200 4%)\n",
      "Iteration: 10; Loss: 5.841.\n",
      "23m 33s (- 562m 24s) (201 4%)\n",
      "Iteration: 10; Loss: 5.695.\n",
      "23m 40s (- 562m 18s) (202 4%)\n",
      "Iteration: 10; Loss: 5.668.\n",
      "23m 47s (- 562m 11s) (203 4%)\n",
      "Iteration: 10; Loss: 5.713.\n",
      "23m 54s (- 562m 2s) (204 4%)\n",
      "Iteration: 10; Loss: 5.903.\n",
      "24m 1s (- 561m 55s) (205 4%)\n",
      "Iteration: 10; Loss: 5.516.\n",
      "24m 8s (- 561m 48s) (206 4%)\n",
      "Iteration: 10; Loss: 5.720.\n",
      "24m 15s (- 561m 40s) (207 4%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "24m 22s (- 561m 32s) (208 4%)\n",
      "Iteration: 10; Loss: 5.503.\n",
      "24m 29s (- 561m 24s) (209 4%)\n",
      "Iteration: 10; Loss: 5.588.\n",
      "24m 36s (- 561m 17s) (210 4%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "24m 43s (- 561m 11s) (211 4%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "24m 50s (- 561m 4s) (212 4%)\n",
      "Iteration: 10; Loss: 5.704.\n",
      "24m 57s (- 560m 57s) (213 4%)\n",
      "Iteration: 10; Loss: 5.526.\n",
      "25m 4s (- 560m 50s) (214 4%)\n",
      "Iteration: 10; Loss: 5.609.\n",
      "25m 11s (- 560m 42s) (215 4%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "25m 18s (- 560m 35s) (216 4%)\n",
      "Iteration: 10; Loss: 5.649.\n",
      "25m 25s (- 560m 28s) (217 4%)\n",
      "Iteration: 10; Loss: 5.774.\n",
      "25m 32s (- 560m 21s) (218 4%)\n",
      "Iteration: 10; Loss: 5.704.\n",
      "25m 39s (- 560m 13s) (219 4%)\n",
      "Iteration: 10; Loss: 5.659.\n",
      "25m 46s (- 560m 7s) (220 4%)\n",
      "Iteration: 10; Loss: 5.661.\n",
      "25m 53s (- 560m 0s) (221 4%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "26m 0s (- 559m 55s) (222 4%)\n",
      "Iteration: 10; Loss: 5.753.\n",
      "26m 7s (- 559m 48s) (223 4%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "26m 14s (- 559m 41s) (224 4%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "26m 22s (- 559m 34s) (225 4%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "26m 29s (- 559m 27s) (226 4%)\n",
      "Iteration: 10; Loss: 5.537.\n",
      "26m 36s (- 559m 20s) (227 4%)\n",
      "Iteration: 10; Loss: 5.615.\n",
      "26m 43s (- 559m 14s) (228 4%)\n",
      "Iteration: 10; Loss: 5.684.\n",
      "26m 50s (- 559m 7s) (229 4%)\n",
      "Iteration: 10; Loss: 5.662.\n",
      "26m 57s (- 559m 0s) (230 4%)\n",
      "Iteration: 10; Loss: 5.697.\n",
      "27m 4s (- 558m 53s) (231 4%)\n",
      "Iteration: 10; Loss: 5.501.\n",
      "27m 11s (- 558m 46s) (232 4%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "27m 18s (- 558m 40s) (233 4%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "27m 25s (- 558m 33s) (234 4%)\n",
      "Iteration: 10; Loss: 5.673.\n",
      "27m 32s (- 558m 27s) (235 4%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "27m 39s (- 558m 21s) (236 4%)\n",
      "Iteration: 10; Loss: 5.592.\n",
      "27m 46s (- 558m 13s) (237 4%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "27m 53s (- 558m 6s) (238 4%)\n",
      "Iteration: 10; Loss: 5.656.\n",
      "28m 0s (- 557m 59s) (239 4%)\n",
      "Iteration: 10; Loss: 5.315.\n",
      "28m 7s (- 557m 52s) (240 4%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "28m 14s (- 557m 45s) (241 4%)\n",
      "Iteration: 10; Loss: 5.773.\n",
      "28m 21s (- 557m 38s) (242 4%)\n",
      "Iteration: 10; Loss: 5.642.\n",
      "28m 28s (- 557m 31s) (243 4%)\n",
      "Iteration: 10; Loss: 5.713.\n",
      "28m 35s (- 557m 24s) (244 4%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "28m 42s (- 557m 17s) (245 4%)\n",
      "Iteration: 10; Loss: 5.733.\n",
      "28m 49s (- 557m 10s) (246 4%)\n",
      "Iteration: 10; Loss: 5.578.\n",
      "28m 57s (- 557m 5s) (247 4%)\n",
      "Iteration: 10; Loss: 5.405.\n",
      "29m 4s (- 556m 58s) (248 4%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "29m 11s (- 556m 51s) (249 4%)\n",
      "Iteration: 10; Loss: 5.472.\n",
      "29m 18s (- 556m 44s) (250 5%)\n",
      "Iteration: 10; Loss: 5.693.\n",
      "29m 25s (- 556m 36s) (251 5%)\n",
      "Iteration: 10; Loss: 5.709.\n",
      "29m 32s (- 556m 28s) (252 5%)\n",
      "Iteration: 10; Loss: 5.631.\n",
      "29m 39s (- 556m 21s) (253 5%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "29m 46s (- 556m 14s) (254 5%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "29m 53s (- 556m 6s) (255 5%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "30m 0s (- 556m 0s) (256 5%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "30m 7s (- 555m 52s) (257 5%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "30m 14s (- 555m 46s) (258 5%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "30m 21s (- 555m 39s) (259 5%)\n",
      "Iteration: 10; Loss: 5.586.\n",
      "30m 28s (- 555m 32s) (260 5%)\n",
      "Iteration: 10; Loss: 5.724.\n",
      "30m 35s (- 555m 25s) (261 5%)\n",
      "Iteration: 10; Loss: 5.582.\n",
      "30m 42s (- 555m 17s) (262 5%)\n",
      "Iteration: 10; Loss: 5.665.\n",
      "30m 49s (- 555m 10s) (263 5%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "30m 56s (- 555m 3s) (264 5%)\n",
      "Iteration: 10; Loss: 5.477.\n",
      "31m 3s (- 554m 56s) (265 5%)\n",
      "Iteration: 10; Loss: 5.347.\n",
      "31m 10s (- 554m 49s) (266 5%)\n",
      "Iteration: 10; Loss: 5.715.\n",
      "31m 17s (- 554m 43s) (267 5%)\n",
      "Iteration: 10; Loss: 5.756.\n",
      "31m 24s (- 554m 36s) (268 5%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "31m 31s (- 554m 30s) (269 5%)\n",
      "Iteration: 10; Loss: 5.547.\n",
      "31m 38s (- 554m 23s) (270 5%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "31m 45s (- 554m 16s) (271 5%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "31m 52s (- 554m 11s) (272 5%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "32m 0s (- 554m 12s) (273 5%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "32m 7s (- 554m 6s) (274 5%)\n",
      "Iteration: 10; Loss: 5.616.\n",
      "32m 14s (- 553m 59s) (275 5%)\n",
      "Iteration: 10; Loss: 5.578.\n",
      "32m 21s (- 553m 52s) (276 5%)\n",
      "Iteration: 10; Loss: 5.728.\n",
      "32m 28s (- 553m 46s) (277 5%)\n",
      "Iteration: 10; Loss: 5.538.\n",
      "32m 35s (- 553m 39s) (278 5%)\n",
      "Iteration: 10; Loss: 5.540.\n",
      "32m 42s (- 553m 32s) (279 5%)\n",
      "Iteration: 10; Loss: 5.483.\n",
      "32m 49s (- 553m 26s) (280 5%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "32m 56s (- 553m 19s) (281 5%)\n",
      "Iteration: 10; Loss: 5.638.\n",
      "33m 3s (- 553m 12s) (282 5%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "33m 10s (- 553m 5s) (283 5%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "33m 18s (- 552m 58s) (284 5%)\n",
      "Iteration: 10; Loss: 5.546.\n",
      "33m 25s (- 552m 51s) (285 5%)\n",
      "Iteration: 10; Loss: 5.716.\n",
      "33m 32s (- 552m 45s) (286 5%)\n",
      "Iteration: 10; Loss: 5.659.\n",
      "33m 39s (- 552m 38s) (287 5%)\n",
      "Iteration: 10; Loss: 5.691.\n",
      "33m 46s (- 552m 30s) (288 5%)\n",
      "Iteration: 10; Loss: 5.461.\n",
      "33m 53s (- 552m 24s) (289 5%)\n",
      "Iteration: 10; Loss: 5.686.\n",
      "34m 0s (- 552m 17s) (290 5%)\n",
      "Iteration: 10; Loss: 5.547.\n",
      "34m 7s (- 552m 10s) (291 5%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "34m 14s (- 552m 3s) (292 5%)\n",
      "Iteration: 10; Loss: 5.530.\n",
      "34m 21s (- 551m 56s) (293 5%)\n",
      "Iteration: 10; Loss: 5.242.\n",
      "34m 28s (- 551m 48s) (294 5%)\n",
      "Iteration: 10; Loss: 5.597.\n",
      "34m 35s (- 551m 41s) (295 5%)\n",
      "Iteration: 10; Loss: 5.608.\n",
      "34m 42s (- 551m 34s) (296 5%)\n",
      "Iteration: 10; Loss: 5.556.\n",
      "34m 49s (- 551m 27s) (297 5%)\n",
      "Iteration: 10; Loss: 5.727.\n",
      "34m 56s (- 551m 20s) (298 5%)\n",
      "Iteration: 10; Loss: 5.725.\n",
      "35m 3s (- 551m 14s) (299 5%)\n",
      "Iteration: 10; Loss: 5.476.\n",
      "35m 10s (- 551m 7s) (300 6%)\n",
      "Iteration: 10; Loss: 5.497.\n",
      "35m 17s (- 551m 0s) (301 6%)\n",
      "Iteration: 10; Loss: 5.383.\n",
      "35m 24s (- 550m 53s) (302 6%)\n",
      "Iteration: 10; Loss: 5.545.\n",
      "35m 31s (- 550m 46s) (303 6%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "35m 38s (- 550m 40s) (304 6%)\n",
      "Iteration: 10; Loss: 5.499.\n",
      "35m 45s (- 550m 33s) (305 6%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "35m 52s (- 550m 26s) (306 6%)\n",
      "Iteration: 10; Loss: 5.551.\n",
      "36m 0s (- 550m 19s) (307 6%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "36m 7s (- 550m 11s) (308 6%)\n",
      "Iteration: 10; Loss: 5.542.\n",
      "36m 14s (- 550m 4s) (309 6%)\n",
      "Iteration: 10; Loss: 5.752.\n",
      "36m 21s (- 549m 57s) (310 6%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "36m 28s (- 549m 51s) (311 6%)\n",
      "Iteration: 10; Loss: 5.597.\n",
      "36m 35s (- 549m 43s) (312 6%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "36m 42s (- 549m 37s) (313 6%)\n",
      "Iteration: 10; Loss: 5.722.\n",
      "36m 49s (- 549m 30s) (314 6%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "36m 56s (- 549m 23s) (315 6%)\n",
      "Iteration: 10; Loss: 5.366.\n",
      "37m 3s (- 549m 16s) (316 6%)\n",
      "Iteration: 10; Loss: 5.603.\n",
      "37m 10s (- 549m 9s) (317 6%)\n",
      "Iteration: 10; Loss: 5.675.\n",
      "37m 17s (- 549m 1s) (318 6%)\n",
      "Iteration: 10; Loss: 5.633.\n",
      "37m 24s (- 548m 53s) (319 6%)\n",
      "Iteration: 10; Loss: 5.698.\n",
      "37m 31s (- 548m 46s) (320 6%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "37m 38s (- 548m 39s) (321 6%)\n",
      "Iteration: 10; Loss: 5.696.\n",
      "37m 45s (- 548m 33s) (322 6%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "37m 52s (- 548m 26s) (323 6%)\n",
      "Iteration: 10; Loss: 5.606.\n",
      "37m 59s (- 548m 20s) (324 6%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "38m 6s (- 548m 13s) (325 6%)\n",
      "Iteration: 10; Loss: 5.517.\n",
      "38m 13s (- 548m 6s) (326 6%)\n",
      "Iteration: 10; Loss: 5.673.\n",
      "38m 20s (- 547m 59s) (327 6%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "38m 27s (- 547m 52s) (328 6%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "38m 34s (- 547m 45s) (329 6%)\n",
      "Iteration: 10; Loss: 5.487.\n",
      "38m 41s (- 547m 37s) (330 6%)\n",
      "Iteration: 10; Loss: 5.665.\n",
      "38m 48s (- 547m 31s) (331 6%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "38m 55s (- 547m 24s) (332 6%)\n",
      "Iteration: 10; Loss: 5.494.\n",
      "39m 3s (- 547m 17s) (333 6%)\n",
      "Iteration: 10; Loss: 5.493.\n",
      "39m 10s (- 547m 10s) (334 6%)\n",
      "Iteration: 10; Loss: 5.752.\n",
      "39m 17s (- 547m 3s) (335 6%)\n",
      "Iteration: 10; Loss: 5.816.\n",
      "39m 24s (- 546m 56s) (336 6%)\n",
      "Iteration: 10; Loss: 5.631.\n",
      "39m 31s (- 546m 49s) (337 6%)\n",
      "Iteration: 10; Loss: 5.747.\n",
      "39m 38s (- 546m 42s) (338 6%)\n",
      "Iteration: 10; Loss: 5.753.\n",
      "39m 45s (- 546m 36s) (339 6%)\n",
      "Iteration: 10; Loss: 5.478.\n",
      "39m 52s (- 546m 28s) (340 6%)\n",
      "Iteration: 10; Loss: 5.618.\n",
      "39m 59s (- 546m 21s) (341 6%)\n",
      "Iteration: 10; Loss: 5.499.\n",
      "40m 6s (- 546m 13s) (342 6%)\n",
      "Iteration: 10; Loss: 5.500.\n",
      "40m 13s (- 546m 6s) (343 6%)\n",
      "Iteration: 10; Loss: 5.348.\n",
      "40m 20s (- 546m 0s) (344 6%)\n",
      "Iteration: 10; Loss: 5.721.\n",
      "40m 27s (- 545m 54s) (345 6%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "40m 34s (- 545m 47s) (346 6%)\n",
      "Iteration: 10; Loss: 5.450.\n",
      "40m 41s (- 545m 39s) (347 6%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "40m 48s (- 545m 33s) (348 6%)\n",
      "Iteration: 10; Loss: 5.493.\n",
      "40m 55s (- 545m 27s) (349 6%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "41m 2s (- 545m 20s) (350 7%)\n",
      "Iteration: 10; Loss: 5.646.\n",
      "41m 9s (- 545m 13s) (351 7%)\n",
      "Iteration: 10; Loss: 5.558.\n",
      "41m 16s (- 545m 6s) (352 7%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "41m 23s (- 544m 59s) (353 7%)\n",
      "Iteration: 10; Loss: 5.605.\n",
      "41m 30s (- 544m 51s) (354 7%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "41m 37s (- 544m 44s) (355 7%)\n",
      "Iteration: 10; Loss: 5.633.\n",
      "41m 44s (- 544m 37s) (356 7%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "41m 51s (- 544m 30s) (357 7%)\n",
      "Iteration: 10; Loss: 5.473.\n",
      "41m 59s (- 544m 22s) (358 7%)\n",
      "Iteration: 10; Loss: 5.679.\n",
      "42m 6s (- 544m 15s) (359 7%)\n",
      "Iteration: 10; Loss: 5.578.\n",
      "42m 13s (- 544m 8s) (360 7%)\n",
      "Iteration: 10; Loss: 5.628.\n",
      "42m 20s (- 544m 1s) (361 7%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "42m 27s (- 543m 55s) (362 7%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "42m 34s (- 543m 48s) (363 7%)\n",
      "Iteration: 10; Loss: 5.543.\n",
      "42m 41s (- 543m 41s) (364 7%)\n",
      "Iteration: 10; Loss: 5.634.\n",
      "42m 48s (- 543m 34s) (365 7%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "42m 55s (- 543m 26s) (366 7%)\n",
      "Iteration: 10; Loss: 5.756.\n",
      "43m 2s (- 543m 20s) (367 7%)\n",
      "Iteration: 10; Loss: 5.476.\n",
      "43m 9s (- 543m 12s) (368 7%)\n",
      "Iteration: 10; Loss: 5.455.\n",
      "43m 16s (- 543m 5s) (369 7%)\n",
      "Iteration: 10; Loss: 5.563.\n",
      "43m 23s (- 542m 58s) (370 7%)\n",
      "Iteration: 10; Loss: 5.621.\n",
      "43m 30s (- 542m 51s) (371 7%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "43m 37s (- 542m 44s) (372 7%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "43m 44s (- 542m 37s) (373 7%)\n",
      "Iteration: 10; Loss: 5.529.\n",
      "43m 51s (- 542m 30s) (374 7%)\n",
      "Iteration: 10; Loss: 5.594.\n",
      "43m 58s (- 542m 24s) (375 7%)\n",
      "Iteration: 10; Loss: 5.360.\n",
      "44m 5s (- 542m 18s) (376 7%)\n",
      "Iteration: 10; Loss: 5.769.\n",
      "44m 12s (- 542m 10s) (377 7%)\n",
      "Iteration: 10; Loss: 5.497.\n",
      "44m 19s (- 542m 3s) (378 7%)\n",
      "Iteration: 10; Loss: 5.382.\n",
      "44m 26s (- 541m 55s) (379 7%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "44m 33s (- 541m 48s) (380 7%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "44m 40s (- 541m 42s) (381 7%)\n",
      "Iteration: 10; Loss: 5.646.\n",
      "44m 48s (- 541m 35s) (382 7%)\n",
      "Iteration: 10; Loss: 5.449.\n",
      "44m 55s (- 541m 28s) (383 7%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "45m 2s (- 541m 21s) (384 7%)\n",
      "Iteration: 10; Loss: 5.780.\n",
      "45m 9s (- 541m 15s) (385 7%)\n",
      "Iteration: 10; Loss: 5.498.\n",
      "45m 16s (- 541m 8s) (386 7%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "45m 23s (- 541m 0s) (387 7%)\n",
      "Iteration: 10; Loss: 5.649.\n",
      "45m 30s (- 540m 53s) (388 7%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "45m 37s (- 540m 46s) (389 7%)\n",
      "Iteration: 10; Loss: 5.527.\n",
      "45m 44s (- 540m 39s) (390 7%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "45m 51s (- 540m 32s) (391 7%)\n",
      "Iteration: 10; Loss: 5.478.\n",
      "45m 58s (- 540m 25s) (392 7%)\n",
      "Iteration: 10; Loss: 5.536.\n",
      "46m 5s (- 540m 18s) (393 7%)\n",
      "Iteration: 10; Loss: 5.734.\n",
      "46m 12s (- 540m 10s) (394 7%)\n",
      "Iteration: 10; Loss: 5.673.\n",
      "46m 19s (- 540m 3s) (395 7%)\n",
      "Iteration: 10; Loss: 5.458.\n",
      "46m 26s (- 539m 56s) (396 7%)\n",
      "Iteration: 10; Loss: 5.802.\n",
      "46m 33s (- 539m 49s) (397 7%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "46m 40s (- 539m 42s) (398 7%)\n",
      "Iteration: 10; Loss: 5.528.\n",
      "46m 47s (- 539m 35s) (399 7%)\n",
      "Iteration: 10; Loss: 5.764.\n",
      "46m 54s (- 539m 29s) (400 8%)\n",
      "Iteration: 10; Loss: 5.624.\n",
      "47m 1s (- 539m 22s) (401 8%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "47m 8s (- 539m 16s) (402 8%)\n",
      "Iteration: 10; Loss: 5.642.\n",
      "47m 15s (- 539m 9s) (403 8%)\n",
      "Iteration: 10; Loss: 5.534.\n",
      "47m 22s (- 539m 2s) (404 8%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "47m 30s (- 538m 55s) (405 8%)\n",
      "Iteration: 10; Loss: 5.536.\n",
      "47m 37s (- 538m 48s) (406 8%)\n",
      "Iteration: 10; Loss: 5.516.\n",
      "47m 44s (- 538m 41s) (407 8%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "47m 51s (- 538m 34s) (408 8%)\n",
      "Iteration: 10; Loss: 5.642.\n",
      "47m 58s (- 538m 27s) (409 8%)\n",
      "Iteration: 10; Loss: 5.566.\n",
      "48m 5s (- 538m 20s) (410 8%)\n",
      "Iteration: 10; Loss: 5.456.\n",
      "48m 12s (- 538m 13s) (411 8%)\n",
      "Iteration: 10; Loss: 5.709.\n",
      "48m 19s (- 538m 6s) (412 8%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "48m 26s (- 537m 59s) (413 8%)\n",
      "Iteration: 10; Loss: 5.407.\n",
      "48m 33s (- 537m 52s) (414 8%)\n",
      "Iteration: 10; Loss: 5.715.\n",
      "48m 40s (- 537m 45s) (415 8%)\n",
      "Iteration: 10; Loss: 5.518.\n",
      "48m 47s (- 537m 38s) (416 8%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "48m 54s (- 537m 31s) (417 8%)\n",
      "Iteration: 10; Loss: 5.567.\n",
      "49m 1s (- 537m 24s) (418 8%)\n",
      "Iteration: 10; Loss: 5.464.\n",
      "49m 8s (- 537m 17s) (419 8%)\n",
      "Iteration: 10; Loss: 5.749.\n",
      "49m 15s (- 537m 10s) (420 8%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "49m 22s (- 537m 3s) (421 8%)\n",
      "Iteration: 10; Loss: 5.765.\n",
      "49m 29s (- 536m 56s) (422 8%)\n",
      "Iteration: 10; Loss: 5.594.\n",
      "49m 36s (- 536m 48s) (423 8%)\n",
      "Iteration: 10; Loss: 5.419.\n",
      "49m 43s (- 536m 41s) (424 8%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "49m 50s (- 536m 34s) (425 8%)\n",
      "Iteration: 10; Loss: 5.656.\n",
      "49m 57s (- 536m 28s) (426 8%)\n",
      "Iteration: 10; Loss: 5.738.\n",
      "50m 4s (- 536m 21s) (427 8%)\n",
      "Iteration: 10; Loss: 5.566.\n",
      "50m 11s (- 536m 14s) (428 8%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "50m 18s (- 536m 7s) (429 8%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "50m 26s (- 536m 0s) (430 8%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "50m 33s (- 535m 52s) (431 8%)\n",
      "Iteration: 10; Loss: 5.573.\n",
      "50m 40s (- 535m 45s) (432 8%)\n",
      "Iteration: 10; Loss: 5.610.\n",
      "50m 47s (- 535m 38s) (433 8%)\n",
      "Iteration: 10; Loss: 5.442.\n",
      "50m 54s (- 535m 32s) (434 8%)\n",
      "Iteration: 10; Loss: 5.549.\n",
      "51m 1s (- 535m 24s) (435 8%)\n",
      "Iteration: 10; Loss: 5.613.\n",
      "51m 8s (- 535m 17s) (436 8%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "51m 15s (- 535m 10s) (437 8%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "51m 22s (- 535m 3s) (438 8%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "51m 29s (- 534m 56s) (439 8%)\n",
      "Iteration: 10; Loss: 5.717.\n",
      "51m 36s (- 534m 49s) (440 8%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "51m 43s (- 534m 42s) (441 8%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "51m 50s (- 534m 36s) (442 8%)\n",
      "Iteration: 10; Loss: 5.705.\n",
      "51m 58s (- 534m 35s) (443 8%)\n",
      "Iteration: 10; Loss: 5.705.\n",
      "52m 5s (- 534m 28s) (444 8%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "52m 12s (- 534m 21s) (445 8%)\n",
      "Iteration: 10; Loss: 5.556.\n",
      "52m 19s (- 534m 14s) (446 8%)\n",
      "Iteration: 10; Loss: 5.460.\n",
      "52m 26s (- 534m 6s) (447 8%)\n",
      "Iteration: 10; Loss: 5.886.\n",
      "52m 33s (- 533m 59s) (448 8%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "52m 40s (- 533m 53s) (449 8%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "52m 47s (- 533m 45s) (450 9%)\n",
      "Iteration: 10; Loss: 5.492.\n",
      "52m 54s (- 533m 39s) (451 9%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "53m 1s (- 533m 32s) (452 9%)\n",
      "Iteration: 10; Loss: 5.715.\n",
      "53m 8s (- 533m 26s) (453 9%)\n",
      "Iteration: 10; Loss: 5.533.\n",
      "53m 15s (- 533m 18s) (454 9%)\n",
      "Iteration: 10; Loss: 5.726.\n",
      "53m 22s (- 533m 10s) (455 9%)\n",
      "Iteration: 10; Loss: 5.700.\n",
      "53m 29s (- 533m 3s) (456 9%)\n",
      "Iteration: 10; Loss: 5.485.\n",
      "53m 36s (- 532m 55s) (457 9%)\n",
      "Iteration: 10; Loss: 5.680.\n",
      "53m 43s (- 532m 48s) (458 9%)\n",
      "Iteration: 10; Loss: 5.612.\n",
      "53m 50s (- 532m 41s) (459 9%)\n",
      "Iteration: 10; Loss: 5.631.\n",
      "53m 57s (- 532m 34s) (460 9%)\n",
      "Iteration: 10; Loss: 5.764.\n",
      "54m 4s (- 532m 27s) (461 9%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "54m 11s (- 532m 20s) (462 9%)\n",
      "Iteration: 10; Loss: 5.582.\n",
      "54m 18s (- 532m 13s) (463 9%)\n",
      "Iteration: 10; Loss: 5.734.\n",
      "54m 25s (- 532m 6s) (464 9%)\n",
      "Iteration: 10; Loss: 5.759.\n",
      "54m 32s (- 531m 59s) (465 9%)\n",
      "Iteration: 10; Loss: 5.521.\n",
      "54m 39s (- 531m 52s) (466 9%)\n",
      "Iteration: 10; Loss: 5.649.\n",
      "54m 46s (- 531m 45s) (467 9%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "54m 54s (- 531m 38s) (468 9%)\n",
      "Iteration: 10; Loss: 5.486.\n",
      "55m 1s (- 531m 31s) (469 9%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "55m 8s (- 531m 24s) (470 9%)\n",
      "Iteration: 10; Loss: 5.745.\n",
      "55m 15s (- 531m 17s) (471 9%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "55m 22s (- 531m 9s) (472 9%)\n",
      "Iteration: 10; Loss: 5.664.\n",
      "55m 29s (- 531m 2s) (473 9%)\n",
      "Iteration: 10; Loss: 5.742.\n",
      "55m 36s (- 530m 55s) (474 9%)\n",
      "Iteration: 10; Loss: 5.475.\n",
      "55m 43s (- 530m 48s) (475 9%)\n",
      "Iteration: 10; Loss: 5.470.\n",
      "55m 50s (- 530m 41s) (476 9%)\n",
      "Iteration: 10; Loss: 5.698.\n",
      "55m 57s (- 530m 35s) (477 9%)\n",
      "Iteration: 10; Loss: 5.790.\n",
      "56m 4s (- 530m 27s) (478 9%)\n",
      "Iteration: 10; Loss: 5.634.\n",
      "56m 11s (- 530m 20s) (479 9%)\n",
      "Iteration: 10; Loss: 5.651.\n",
      "56m 18s (- 530m 14s) (480 9%)\n",
      "Iteration: 10; Loss: 5.619.\n",
      "56m 25s (- 530m 7s) (481 9%)\n",
      "Iteration: 10; Loss: 5.610.\n",
      "56m 32s (- 530m 0s) (482 9%)\n",
      "Iteration: 10; Loss: 5.641.\n",
      "56m 39s (- 529m 53s) (483 9%)\n",
      "Iteration: 10; Loss: 5.763.\n",
      "56m 46s (- 529m 46s) (484 9%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "56m 53s (- 529m 39s) (485 9%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "57m 0s (- 529m 32s) (486 9%)\n",
      "Iteration: 10; Loss: 5.530.\n",
      "57m 7s (- 529m 24s) (487 9%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "57m 14s (- 529m 18s) (488 9%)\n",
      "Iteration: 10; Loss: 5.366.\n",
      "57m 21s (- 529m 11s) (489 9%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "57m 28s (- 529m 4s) (490 9%)\n",
      "Iteration: 10; Loss: 5.623.\n",
      "57m 35s (- 528m 56s) (491 9%)\n",
      "Iteration: 10; Loss: 5.524.\n",
      "57m 42s (- 528m 49s) (492 9%)\n",
      "Iteration: 10; Loss: 5.464.\n",
      "57m 50s (- 528m 43s) (493 9%)\n",
      "Iteration: 10; Loss: 5.686.\n",
      "57m 57s (- 528m 36s) (494 9%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "58m 4s (- 528m 29s) (495 9%)\n",
      "Iteration: 10; Loss: 5.624.\n",
      "58m 11s (- 528m 22s) (496 9%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "58m 18s (- 528m 15s) (497 9%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "58m 25s (- 528m 8s) (498 9%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "58m 32s (- 528m 0s) (499 9%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "58m 39s (- 527m 53s) (500 10%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "58m 46s (- 527m 46s) (501 10%)\n",
      "Iteration: 10; Loss: 5.554.\n",
      "58m 53s (- 527m 40s) (502 10%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "59m 0s (- 527m 33s) (503 10%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "59m 7s (- 527m 26s) (504 10%)\n",
      "Iteration: 10; Loss: 5.734.\n",
      "59m 14s (- 527m 19s) (505 10%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "59m 21s (- 527m 12s) (506 10%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "59m 28s (- 527m 5s) (507 10%)\n",
      "Iteration: 10; Loss: 5.671.\n",
      "59m 35s (- 526m 58s) (508 10%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "59m 42s (- 526m 51s) (509 10%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "59m 49s (- 526m 44s) (510 10%)\n",
      "Iteration: 10; Loss: 5.426.\n",
      "59m 56s (- 526m 37s) (511 10%)\n",
      "Iteration: 10; Loss: 5.648.\n",
      "60m 3s (- 526m 30s) (512 10%)\n",
      "Iteration: 10; Loss: 5.768.\n",
      "60m 11s (- 526m 24s) (513 10%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "60m 18s (- 526m 16s) (514 10%)\n",
      "Iteration: 10; Loss: 5.753.\n",
      "60m 25s (- 526m 9s) (515 10%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "60m 32s (- 526m 3s) (516 10%)\n",
      "Iteration: 10; Loss: 5.653.\n",
      "60m 39s (- 525m 56s) (517 10%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "60m 46s (- 525m 49s) (518 10%)\n",
      "Iteration: 10; Loss: 5.654.\n",
      "60m 53s (- 525m 42s) (519 10%)\n",
      "Iteration: 10; Loss: 5.530.\n",
      "61m 0s (- 525m 34s) (520 10%)\n",
      "Iteration: 10; Loss: 5.720.\n",
      "61m 7s (- 525m 27s) (521 10%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "61m 14s (- 525m 20s) (522 10%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "61m 21s (- 525m 13s) (523 10%)\n",
      "Iteration: 10; Loss: 5.539.\n",
      "61m 28s (- 525m 6s) (524 10%)\n",
      "Iteration: 10; Loss: 5.397.\n",
      "61m 35s (- 524m 59s) (525 10%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "61m 42s (- 524m 52s) (526 10%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "61m 49s (- 524m 45s) (527 10%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "61m 56s (- 524m 39s) (528 10%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "62m 3s (- 524m 32s) (529 10%)\n",
      "Iteration: 10; Loss: 5.697.\n",
      "62m 10s (- 524m 25s) (530 10%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "62m 17s (- 524m 18s) (531 10%)\n",
      "Iteration: 10; Loss: 5.634.\n",
      "62m 24s (- 524m 10s) (532 10%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "62m 31s (- 524m 3s) (533 10%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "62m 38s (- 523m 56s) (534 10%)\n",
      "Iteration: 10; Loss: 5.703.\n",
      "62m 45s (- 523m 49s) (535 10%)\n",
      "Iteration: 10; Loss: 5.390.\n",
      "62m 52s (- 523m 42s) (536 10%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "62m 59s (- 523m 35s) (537 10%)\n",
      "Iteration: 10; Loss: 5.504.\n",
      "63m 7s (- 523m 28s) (538 10%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "63m 14s (- 523m 20s) (539 10%)\n",
      "Iteration: 10; Loss: 5.447.\n",
      "63m 21s (- 523m 14s) (540 10%)\n",
      "Iteration: 10; Loss: 5.506.\n",
      "63m 28s (- 523m 7s) (541 10%)\n",
      "Iteration: 10; Loss: 5.530.\n",
      "63m 35s (- 523m 0s) (542 10%)\n",
      "Iteration: 10; Loss: 5.543.\n",
      "63m 42s (- 522m 53s) (543 10%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "63m 49s (- 522m 46s) (544 10%)\n",
      "Iteration: 10; Loss: 5.565.\n",
      "63m 56s (- 522m 39s) (545 10%)\n",
      "Iteration: 10; Loss: 5.684.\n",
      "64m 3s (- 522m 32s) (546 10%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "64m 10s (- 522m 25s) (547 10%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "64m 17s (- 522m 18s) (548 10%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "64m 24s (- 522m 11s) (549 10%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "64m 31s (- 522m 4s) (550 11%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "64m 38s (- 521m 57s) (551 11%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "64m 46s (- 521m 53s) (552 11%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "64m 53s (- 521m 46s) (553 11%)\n",
      "Iteration: 10; Loss: 5.423.\n",
      "65m 0s (- 521m 39s) (554 11%)\n",
      "Iteration: 10; Loss: 5.731.\n",
      "65m 7s (- 521m 33s) (555 11%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "65m 14s (- 521m 26s) (556 11%)\n",
      "Iteration: 10; Loss: 5.507.\n",
      "65m 21s (- 521m 19s) (557 11%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "65m 28s (- 521m 11s) (558 11%)\n",
      "Iteration: 10; Loss: 5.490.\n",
      "65m 35s (- 521m 4s) (559 11%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "65m 42s (- 520m 57s) (560 11%)\n",
      "Iteration: 10; Loss: 5.527.\n",
      "65m 49s (- 520m 50s) (561 11%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "65m 56s (- 520m 43s) (562 11%)\n",
      "Iteration: 10; Loss: 5.510.\n",
      "66m 3s (- 520m 36s) (563 11%)\n",
      "Iteration: 10; Loss: 5.394.\n",
      "66m 10s (- 520m 29s) (564 11%)\n",
      "Iteration: 10; Loss: 5.374.\n",
      "66m 17s (- 520m 21s) (565 11%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "66m 24s (- 520m 14s) (566 11%)\n",
      "Iteration: 10; Loss: 5.533.\n",
      "66m 31s (- 520m 8s) (567 11%)\n",
      "Iteration: 10; Loss: 5.638.\n",
      "66m 38s (- 520m 1s) (568 11%)\n",
      "Iteration: 10; Loss: 5.676.\n",
      "66m 45s (- 519m 54s) (569 11%)\n",
      "Iteration: 10; Loss: 5.578.\n",
      "66m 52s (- 519m 46s) (570 11%)\n",
      "Iteration: 10; Loss: 5.693.\n",
      "66m 59s (- 519m 39s) (571 11%)\n",
      "Iteration: 10; Loss: 5.716.\n",
      "67m 6s (- 519m 32s) (572 11%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "67m 13s (- 519m 25s) (573 11%)\n",
      "Iteration: 10; Loss: 5.484.\n",
      "67m 20s (- 519m 18s) (574 11%)\n",
      "Iteration: 10; Loss: 5.568.\n",
      "67m 27s (- 519m 11s) (575 11%)\n",
      "Iteration: 10; Loss: 5.703.\n",
      "67m 35s (- 519m 6s) (576 11%)\n",
      "Iteration: 10; Loss: 5.519.\n",
      "67m 42s (- 518m 59s) (577 11%)\n",
      "Iteration: 10; Loss: 5.586.\n",
      "67m 49s (- 518m 52s) (578 11%)\n",
      "Iteration: 10; Loss: 5.488.\n",
      "67m 56s (- 518m 46s) (579 11%)\n",
      "Iteration: 10; Loss: 5.621.\n",
      "68m 3s (- 518m 39s) (580 11%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "68m 10s (- 518m 32s) (581 11%)\n",
      "Iteration: 10; Loss: 5.556.\n",
      "68m 17s (- 518m 25s) (582 11%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "68m 24s (- 518m 17s) (583 11%)\n",
      "Iteration: 10; Loss: 5.507.\n",
      "68m 31s (- 518m 10s) (584 11%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "68m 38s (- 518m 3s) (585 11%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "68m 45s (- 517m 56s) (586 11%)\n",
      "Iteration: 10; Loss: 5.405.\n",
      "68m 52s (- 517m 49s) (587 11%)\n",
      "Iteration: 10; Loss: 5.514.\n",
      "68m 59s (- 517m 42s) (588 11%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "69m 6s (- 517m 35s) (589 11%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "69m 13s (- 517m 29s) (590 11%)\n",
      "Iteration: 10; Loss: 5.638.\n",
      "69m 21s (- 517m 22s) (591 11%)\n",
      "Iteration: 10; Loss: 5.505.\n",
      "69m 28s (- 517m 15s) (592 11%)\n",
      "Iteration: 10; Loss: 5.619.\n",
      "69m 35s (- 517m 9s) (593 11%)\n",
      "Iteration: 10; Loss: 5.515.\n",
      "69m 42s (- 517m 2s) (594 11%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "69m 49s (- 516m 54s) (595 11%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "69m 56s (- 516m 47s) (596 11%)\n",
      "Iteration: 10; Loss: 5.810.\n",
      "70m 3s (- 516m 41s) (597 11%)\n",
      "Iteration: 10; Loss: 5.727.\n",
      "70m 10s (- 516m 33s) (598 11%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "70m 17s (- 516m 26s) (599 11%)\n",
      "Iteration: 10; Loss: 5.441.\n",
      "70m 24s (- 516m 19s) (600 12%)\n",
      "Iteration: 10; Loss: 5.609.\n",
      "70m 31s (- 516m 12s) (601 12%)\n",
      "Iteration: 10; Loss: 5.538.\n",
      "70m 38s (- 516m 5s) (602 12%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "70m 45s (- 515m 58s) (603 12%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "70m 52s (- 515m 51s) (604 12%)\n",
      "Iteration: 10; Loss: 5.698.\n",
      "70m 59s (- 515m 44s) (605 12%)\n",
      "Iteration: 10; Loss: 5.582.\n",
      "71m 6s (- 515m 37s) (606 12%)\n",
      "Iteration: 10; Loss: 5.663.\n",
      "71m 13s (- 515m 30s) (607 12%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "71m 20s (- 515m 23s) (608 12%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "71m 27s (- 515m 16s) (609 12%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "71m 34s (- 515m 9s) (610 12%)\n",
      "Iteration: 10; Loss: 5.483.\n",
      "71m 41s (- 515m 2s) (611 12%)\n",
      "Iteration: 10; Loss: 5.605.\n",
      "71m 48s (- 514m 54s) (612 12%)\n",
      "Iteration: 10; Loss: 5.536.\n",
      "71m 55s (- 514m 47s) (613 12%)\n",
      "Iteration: 10; Loss: 5.482.\n",
      "72m 3s (- 514m 41s) (614 12%)\n",
      "Iteration: 10; Loss: 5.776.\n",
      "72m 10s (- 514m 34s) (615 12%)\n",
      "Iteration: 10; Loss: 5.727.\n",
      "72m 17s (- 514m 27s) (616 12%)\n",
      "Iteration: 10; Loss: 5.673.\n",
      "72m 24s (- 514m 20s) (617 12%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "72m 31s (- 514m 13s) (618 12%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "72m 38s (- 514m 6s) (619 12%)\n",
      "Iteration: 10; Loss: 5.513.\n",
      "72m 45s (- 513m 59s) (620 12%)\n",
      "Iteration: 10; Loss: 5.789.\n",
      "72m 52s (- 513m 52s) (621 12%)\n",
      "Iteration: 10; Loss: 5.510.\n",
      "73m 0s (- 513m 51s) (622 12%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "73m 7s (- 513m 44s) (623 12%)\n",
      "Iteration: 10; Loss: 5.640.\n",
      "73m 14s (- 513m 37s) (624 12%)\n",
      "Iteration: 10; Loss: 5.518.\n",
      "73m 21s (- 513m 30s) (625 12%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "73m 28s (- 513m 23s) (626 12%)\n",
      "Iteration: 10; Loss: 5.606.\n",
      "73m 35s (- 513m 15s) (627 12%)\n",
      "Iteration: 10; Loss: 5.689.\n",
      "73m 42s (- 513m 8s) (628 12%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "73m 49s (- 513m 1s) (629 12%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "73m 56s (- 512m 54s) (630 12%)\n",
      "Iteration: 10; Loss: 5.780.\n",
      "74m 3s (- 512m 47s) (631 12%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "74m 10s (- 512m 40s) (632 12%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "74m 17s (- 512m 33s) (633 12%)\n",
      "Iteration: 10; Loss: 5.509.\n",
      "74m 24s (- 512m 26s) (634 12%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "74m 31s (- 512m 19s) (635 12%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "74m 38s (- 512m 12s) (636 12%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "74m 45s (- 512m 5s) (637 12%)\n",
      "Iteration: 10; Loss: 5.661.\n",
      "74m 52s (- 511m 57s) (638 12%)\n",
      "Iteration: 10; Loss: 5.440.\n",
      "74m 59s (- 511m 50s) (639 12%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "75m 6s (- 511m 43s) (640 12%)\n",
      "Iteration: 10; Loss: 5.706.\n",
      "75m 13s (- 511m 36s) (641 12%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "75m 21s (- 511m 29s) (642 12%)\n",
      "Iteration: 10; Loss: 5.783.\n",
      "75m 28s (- 511m 22s) (643 12%)\n",
      "Iteration: 10; Loss: 5.565.\n",
      "75m 35s (- 511m 15s) (644 12%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "75m 42s (- 511m 8s) (645 12%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "75m 49s (- 511m 1s) (646 12%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "75m 56s (- 510m 54s) (647 12%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "76m 3s (- 510m 47s) (648 12%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "76m 10s (- 510m 40s) (649 12%)\n",
      "Iteration: 10; Loss: 5.593.\n",
      "76m 17s (- 510m 33s) (650 13%)\n",
      "Iteration: 10; Loss: 5.463.\n",
      "76m 24s (- 510m 26s) (651 13%)\n",
      "Iteration: 10; Loss: 5.439.\n",
      "76m 31s (- 510m 19s) (652 13%)\n",
      "Iteration: 10; Loss: 5.813.\n",
      "76m 38s (- 510m 12s) (653 13%)\n",
      "Iteration: 10; Loss: 5.526.\n",
      "76m 45s (- 510m 5s) (654 13%)\n",
      "Iteration: 10; Loss: 5.689.\n",
      "76m 52s (- 509m 58s) (655 13%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "76m 59s (- 509m 51s) (656 13%)\n",
      "Iteration: 10; Loss: 5.682.\n",
      "77m 6s (- 509m 44s) (657 13%)\n",
      "Iteration: 10; Loss: 5.618.\n",
      "77m 13s (- 509m 37s) (658 13%)\n",
      "Iteration: 10; Loss: 5.508.\n",
      "77m 20s (- 509m 30s) (659 13%)\n",
      "Iteration: 10; Loss: 5.697.\n",
      "77m 27s (- 509m 23s) (660 13%)\n",
      "Iteration: 10; Loss: 5.405.\n",
      "77m 34s (- 509m 16s) (661 13%)\n",
      "Iteration: 10; Loss: 5.451.\n",
      "77m 41s (- 509m 9s) (662 13%)\n",
      "Iteration: 10; Loss: 5.533.\n",
      "77m 48s (- 509m 1s) (663 13%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "77m 56s (- 508m 54s) (664 13%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "78m 3s (- 508m 47s) (665 13%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "78m 10s (- 508m 40s) (666 13%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "78m 17s (- 508m 33s) (667 13%)\n",
      "Iteration: 10; Loss: 5.417.\n",
      "78m 24s (- 508m 26s) (668 13%)\n",
      "Iteration: 10; Loss: 5.715.\n",
      "78m 31s (- 508m 19s) (669 13%)\n",
      "Iteration: 10; Loss: 5.694.\n",
      "78m 38s (- 508m 12s) (670 13%)\n",
      "Iteration: 10; Loss: 5.566.\n",
      "78m 45s (- 508m 5s) (671 13%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "78m 52s (- 507m 58s) (672 13%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "78m 59s (- 507m 51s) (673 13%)\n",
      "Iteration: 10; Loss: 5.466.\n",
      "79m 6s (- 507m 44s) (674 13%)\n",
      "Iteration: 10; Loss: 5.609.\n",
      "79m 13s (- 507m 37s) (675 13%)\n",
      "Iteration: 10; Loss: 5.458.\n",
      "79m 20s (- 507m 30s) (676 13%)\n",
      "Iteration: 10; Loss: 5.734.\n",
      "79m 27s (- 507m 23s) (677 13%)\n",
      "Iteration: 10; Loss: 5.567.\n",
      "79m 34s (- 507m 16s) (678 13%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "79m 41s (- 507m 9s) (679 13%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "79m 48s (- 507m 2s) (680 13%)\n",
      "Iteration: 10; Loss: 5.703.\n",
      "79m 55s (- 506m 55s) (681 13%)\n",
      "Iteration: 10; Loss: 5.591.\n",
      "80m 2s (- 506m 48s) (682 13%)\n",
      "Iteration: 10; Loss: 5.610.\n",
      "80m 9s (- 506m 41s) (683 13%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "80m 16s (- 506m 34s) (684 13%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "80m 24s (- 506m 27s) (685 13%)\n",
      "Iteration: 10; Loss: 5.746.\n",
      "80m 31s (- 506m 20s) (686 13%)\n",
      "Iteration: 10; Loss: 5.469.\n",
      "80m 38s (- 506m 13s) (687 13%)\n",
      "Iteration: 10; Loss: 5.471.\n",
      "80m 45s (- 506m 6s) (688 13%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "80m 52s (- 505m 59s) (689 13%)\n",
      "Iteration: 10; Loss: 5.636.\n",
      "80m 59s (- 505m 52s) (690 13%)\n",
      "Iteration: 10; Loss: 5.478.\n",
      "81m 6s (- 505m 45s) (691 13%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "81m 13s (- 505m 38s) (692 13%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "81m 20s (- 505m 31s) (693 13%)\n",
      "Iteration: 10; Loss: 5.429.\n",
      "81m 27s (- 505m 24s) (694 13%)\n",
      "Iteration: 10; Loss: 5.563.\n",
      "81m 34s (- 505m 17s) (695 13%)\n",
      "Iteration: 10; Loss: 5.753.\n",
      "81m 41s (- 505m 10s) (696 13%)\n",
      "Iteration: 10; Loss: 5.680.\n",
      "81m 48s (- 505m 3s) (697 13%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "81m 55s (- 504m 56s) (698 13%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "82m 2s (- 504m 49s) (699 13%)\n",
      "Iteration: 10; Loss: 5.458.\n",
      "82m 9s (- 504m 42s) (700 14%)\n",
      "Iteration: 10; Loss: 5.542.\n",
      "82m 16s (- 504m 35s) (701 14%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "82m 23s (- 504m 27s) (702 14%)\n",
      "Iteration: 10; Loss: 5.573.\n",
      "82m 30s (- 504m 21s) (703 14%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "82m 37s (- 504m 13s) (704 14%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "82m 45s (- 504m 11s) (705 14%)\n",
      "Iteration: 10; Loss: 5.583.\n",
      "82m 52s (- 504m 4s) (706 14%)\n",
      "Iteration: 10; Loss: 5.530.\n",
      "82m 59s (- 503m 57s) (707 14%)\n",
      "Iteration: 10; Loss: 5.458.\n",
      "83m 6s (- 503m 50s) (708 14%)\n",
      "Iteration: 10; Loss: 5.449.\n",
      "83m 13s (- 503m 43s) (709 14%)\n",
      "Iteration: 10; Loss: 5.739.\n",
      "83m 20s (- 503m 36s) (710 14%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "83m 27s (- 503m 29s) (711 14%)\n",
      "Iteration: 10; Loss: 5.429.\n",
      "83m 34s (- 503m 22s) (712 14%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "83m 41s (- 503m 15s) (713 14%)\n",
      "Iteration: 10; Loss: 5.681.\n",
      "83m 49s (- 503m 8s) (714 14%)\n",
      "Iteration: 10; Loss: 5.654.\n",
      "83m 56s (- 503m 1s) (715 14%)\n",
      "Iteration: 10; Loss: 5.663.\n",
      "84m 3s (- 502m 54s) (716 14%)\n",
      "Iteration: 10; Loss: 5.623.\n",
      "84m 10s (- 502m 46s) (717 14%)\n",
      "Iteration: 10; Loss: 5.498.\n",
      "84m 17s (- 502m 39s) (718 14%)\n",
      "Iteration: 10; Loss: 5.692.\n",
      "84m 24s (- 502m 32s) (719 14%)\n",
      "Iteration: 10; Loss: 5.605.\n",
      "84m 31s (- 502m 25s) (720 14%)\n",
      "Iteration: 10; Loss: 5.603.\n",
      "84m 38s (- 502m 18s) (721 14%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "84m 45s (- 502m 11s) (722 14%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "84m 52s (- 502m 4s) (723 14%)\n",
      "Iteration: 10; Loss: 5.408.\n",
      "84m 59s (- 501m 57s) (724 14%)\n",
      "Iteration: 10; Loss: 5.603.\n",
      "85m 6s (- 501m 50s) (725 14%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "85m 13s (- 501m 43s) (726 14%)\n",
      "Iteration: 10; Loss: 5.472.\n",
      "85m 20s (- 501m 36s) (727 14%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "85m 27s (- 501m 28s) (728 14%)\n",
      "Iteration: 10; Loss: 5.837.\n",
      "85m 34s (- 501m 22s) (729 14%)\n",
      "Iteration: 10; Loss: 5.503.\n",
      "85m 41s (- 501m 14s) (730 14%)\n",
      "Iteration: 10; Loss: 5.545.\n",
      "85m 48s (- 501m 8s) (731 14%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "85m 55s (- 501m 1s) (732 14%)\n",
      "Iteration: 10; Loss: 5.780.\n",
      "86m 2s (- 500m 54s) (733 14%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "86m 9s (- 500m 47s) (734 14%)\n",
      "Iteration: 10; Loss: 5.664.\n",
      "86m 16s (- 500m 40s) (735 14%)\n",
      "Iteration: 10; Loss: 5.538.\n",
      "86m 23s (- 500m 32s) (736 14%)\n",
      "Iteration: 10; Loss: 5.709.\n",
      "86m 30s (- 500m 25s) (737 14%)\n",
      "Iteration: 10; Loss: 5.469.\n",
      "86m 37s (- 500m 18s) (738 14%)\n",
      "Iteration: 10; Loss: 5.514.\n",
      "86m 45s (- 500m 11s) (739 14%)\n",
      "Iteration: 10; Loss: 5.509.\n",
      "86m 52s (- 500m 4s) (740 14%)\n",
      "Iteration: 10; Loss: 5.679.\n",
      "86m 59s (- 499m 57s) (741 14%)\n",
      "Iteration: 10; Loss: 5.505.\n",
      "87m 6s (- 499m 50s) (742 14%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "87m 13s (- 499m 43s) (743 14%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "87m 20s (- 499m 36s) (744 14%)\n",
      "Iteration: 10; Loss: 5.609.\n",
      "87m 27s (- 499m 29s) (745 14%)\n",
      "Iteration: 10; Loss: 5.624.\n",
      "87m 34s (- 499m 22s) (746 14%)\n",
      "Iteration: 10; Loss: 5.631.\n",
      "87m 41s (- 499m 15s) (747 14%)\n",
      "Iteration: 10; Loss: 5.614.\n",
      "87m 48s (- 499m 8s) (748 14%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "87m 55s (- 499m 1s) (749 14%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "88m 2s (- 498m 54s) (750 15%)\n",
      "Iteration: 10; Loss: 5.500.\n",
      "88m 9s (- 498m 47s) (751 15%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "88m 16s (- 498m 40s) (752 15%)\n",
      "Iteration: 10; Loss: 5.510.\n",
      "88m 23s (- 498m 33s) (753 15%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "88m 30s (- 498m 26s) (754 15%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "88m 37s (- 498m 19s) (755 15%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "88m 44s (- 498m 12s) (756 15%)\n",
      "Iteration: 10; Loss: 5.680.\n",
      "88m 51s (- 498m 5s) (757 15%)\n",
      "Iteration: 10; Loss: 5.489.\n",
      "88m 59s (- 497m 59s) (758 15%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "89m 6s (- 497m 51s) (759 15%)\n",
      "Iteration: 10; Loss: 5.622.\n",
      "89m 13s (- 497m 44s) (760 15%)\n",
      "Iteration: 10; Loss: 5.605.\n",
      "89m 20s (- 497m 37s) (761 15%)\n",
      "Iteration: 10; Loss: 5.754.\n",
      "89m 27s (- 497m 30s) (762 15%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "89m 34s (- 497m 23s) (763 15%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "89m 41s (- 497m 16s) (764 15%)\n",
      "Iteration: 10; Loss: 5.696.\n",
      "89m 48s (- 497m 9s) (765 15%)\n",
      "Iteration: 10; Loss: 5.665.\n",
      "89m 55s (- 497m 1s) (766 15%)\n",
      "Iteration: 10; Loss: 5.715.\n",
      "90m 2s (- 496m 55s) (767 15%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "90m 9s (- 496m 47s) (768 15%)\n",
      "Iteration: 10; Loss: 5.477.\n",
      "90m 16s (- 496m 40s) (769 15%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "90m 23s (- 496m 33s) (770 15%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "90m 30s (- 496m 26s) (771 15%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "90m 37s (- 496m 19s) (772 15%)\n",
      "Iteration: 10; Loss: 5.538.\n",
      "90m 44s (- 496m 12s) (773 15%)\n",
      "Iteration: 10; Loss: 5.618.\n",
      "90m 51s (- 496m 4s) (774 15%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "90m 58s (- 495m 58s) (775 15%)\n",
      "Iteration: 10; Loss: 5.418.\n",
      "91m 5s (- 495m 51s) (776 15%)\n",
      "Iteration: 10; Loss: 5.493.\n",
      "91m 12s (- 495m 44s) (777 15%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "91m 19s (- 495m 37s) (778 15%)\n",
      "Iteration: 10; Loss: 5.443.\n",
      "91m 26s (- 495m 30s) (779 15%)\n",
      "Iteration: 10; Loss: 5.425.\n",
      "91m 33s (- 495m 23s) (780 15%)\n",
      "Iteration: 10; Loss: 5.634.\n",
      "91m 40s (- 495m 16s) (781 15%)\n",
      "Iteration: 10; Loss: 5.746.\n",
      "91m 47s (- 495m 9s) (782 15%)\n",
      "Iteration: 10; Loss: 5.668.\n",
      "91m 54s (- 495m 2s) (783 15%)\n",
      "Iteration: 10; Loss: 5.530.\n",
      "92m 2s (- 494m 55s) (784 15%)\n",
      "Iteration: 10; Loss: 5.526.\n",
      "92m 9s (- 494m 47s) (785 15%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "92m 16s (- 494m 40s) (786 15%)\n",
      "Iteration: 10; Loss: 5.509.\n",
      "92m 23s (- 494m 33s) (787 15%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "92m 30s (- 494m 26s) (788 15%)\n",
      "Iteration: 10; Loss: 5.605.\n",
      "92m 37s (- 494m 19s) (789 15%)\n",
      "Iteration: 10; Loss: 5.608.\n",
      "92m 44s (- 494m 12s) (790 15%)\n",
      "Iteration: 10; Loss: 5.655.\n",
      "92m 51s (- 494m 5s) (791 15%)\n",
      "Iteration: 10; Loss: 5.839.\n",
      "92m 58s (- 493m 58s) (792 15%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "93m 5s (- 493m 51s) (793 15%)\n",
      "Iteration: 10; Loss: 5.631.\n",
      "93m 12s (- 493m 44s) (794 15%)\n",
      "Iteration: 10; Loss: 5.403.\n",
      "93m 19s (- 493m 37s) (795 15%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "93m 26s (- 493m 30s) (796 15%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "93m 33s (- 493m 22s) (797 15%)\n",
      "Iteration: 10; Loss: 5.720.\n",
      "93m 40s (- 493m 15s) (798 15%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "93m 47s (- 493m 9s) (799 15%)\n",
      "Iteration: 10; Loss: 5.554.\n",
      "93m 54s (- 493m 2s) (800 16%)\n",
      "Iteration: 10; Loss: 5.679.\n",
      "94m 1s (- 492m 55s) (801 16%)\n",
      "Iteration: 10; Loss: 5.594.\n",
      "94m 8s (- 492m 48s) (802 16%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "94m 15s (- 492m 41s) (803 16%)\n",
      "Iteration: 10; Loss: 5.649.\n",
      "94m 22s (- 492m 34s) (804 16%)\n",
      "Iteration: 10; Loss: 5.499.\n",
      "94m 29s (- 492m 27s) (805 16%)\n",
      "Iteration: 10; Loss: 5.691.\n",
      "94m 37s (- 492m 20s) (806 16%)\n",
      "Iteration: 10; Loss: 5.701.\n",
      "94m 44s (- 492m 13s) (807 16%)\n",
      "Iteration: 10; Loss: 5.586.\n",
      "94m 51s (- 492m 6s) (808 16%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "94m 58s (- 491m 59s) (809 16%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "95m 5s (- 491m 52s) (810 16%)\n",
      "Iteration: 10; Loss: 5.526.\n",
      "95m 12s (- 491m 45s) (811 16%)\n",
      "Iteration: 10; Loss: 5.440.\n",
      "95m 19s (- 491m 39s) (812 16%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "95m 27s (- 491m 34s) (813 16%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "95m 34s (- 491m 27s) (814 16%)\n",
      "Iteration: 10; Loss: 5.676.\n",
      "95m 41s (- 491m 20s) (815 16%)\n",
      "Iteration: 10; Loss: 5.676.\n",
      "95m 48s (- 491m 13s) (816 16%)\n",
      "Iteration: 10; Loss: 5.472.\n",
      "95m 55s (- 491m 6s) (817 16%)\n",
      "Iteration: 10; Loss: 5.691.\n",
      "96m 2s (- 490m 59s) (818 16%)\n",
      "Iteration: 10; Loss: 5.432.\n",
      "96m 9s (- 490m 52s) (819 16%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "96m 16s (- 490m 45s) (820 16%)\n",
      "Iteration: 10; Loss: 5.761.\n",
      "96m 23s (- 490m 38s) (821 16%)\n",
      "Iteration: 10; Loss: 5.608.\n",
      "96m 30s (- 490m 31s) (822 16%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "96m 37s (- 490m 24s) (823 16%)\n",
      "Iteration: 10; Loss: 5.493.\n",
      "96m 44s (- 490m 16s) (824 16%)\n",
      "Iteration: 10; Loss: 5.694.\n",
      "96m 51s (- 490m 9s) (825 16%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "96m 58s (- 490m 2s) (826 16%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "97m 5s (- 489m 55s) (827 16%)\n",
      "Iteration: 10; Loss: 5.497.\n",
      "97m 12s (- 489m 48s) (828 16%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "97m 19s (- 489m 41s) (829 16%)\n",
      "Iteration: 10; Loss: 5.773.\n",
      "97m 26s (- 489m 34s) (830 16%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "97m 33s (- 489m 27s) (831 16%)\n",
      "Iteration: 10; Loss: 5.462.\n",
      "97m 40s (- 489m 20s) (832 16%)\n",
      "Iteration: 10; Loss: 5.529.\n",
      "97m 47s (- 489m 13s) (833 16%)\n",
      "Iteration: 10; Loss: 5.619.\n",
      "97m 54s (- 489m 6s) (834 16%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "98m 1s (- 488m 59s) (835 16%)\n",
      "Iteration: 10; Loss: 5.738.\n",
      "98m 8s (- 488m 52s) (836 16%)\n",
      "Iteration: 10; Loss: 5.577.\n",
      "98m 15s (- 488m 44s) (837 16%)\n",
      "Iteration: 10; Loss: 5.503.\n",
      "98m 23s (- 488m 37s) (838 16%)\n",
      "Iteration: 10; Loss: 5.435.\n",
      "98m 30s (- 488m 30s) (839 16%)\n",
      "Iteration: 10; Loss: 5.421.\n",
      "98m 37s (- 488m 23s) (840 16%)\n",
      "Iteration: 10; Loss: 5.631.\n",
      "98m 44s (- 488m 16s) (841 16%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "98m 51s (- 488m 9s) (842 16%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "98m 58s (- 488m 2s) (843 16%)\n",
      "Iteration: 10; Loss: 5.733.\n",
      "99m 5s (- 487m 54s) (844 16%)\n",
      "Iteration: 10; Loss: 5.732.\n",
      "99m 12s (- 487m 47s) (845 16%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "99m 19s (- 487m 40s) (846 16%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "99m 26s (- 487m 33s) (847 16%)\n",
      "Iteration: 10; Loss: 5.792.\n",
      "99m 33s (- 487m 26s) (848 16%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "99m 40s (- 487m 19s) (849 16%)\n",
      "Iteration: 10; Loss: 5.697.\n",
      "99m 47s (- 487m 12s) (850 17%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "99m 54s (- 487m 5s) (851 17%)\n",
      "Iteration: 10; Loss: 5.727.\n",
      "100m 1s (- 486m 58s) (852 17%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "100m 8s (- 486m 51s) (853 17%)\n",
      "Iteration: 10; Loss: 5.433.\n",
      "100m 15s (- 486m 43s) (854 17%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "100m 22s (- 486m 36s) (855 17%)\n",
      "Iteration: 10; Loss: 5.518.\n",
      "100m 29s (- 486m 29s) (856 17%)\n",
      "Iteration: 10; Loss: 5.568.\n",
      "100m 36s (- 486m 22s) (857 17%)\n",
      "Iteration: 10; Loss: 5.615.\n",
      "100m 43s (- 486m 15s) (858 17%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "100m 50s (- 486m 8s) (859 17%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "100m 57s (- 486m 1s) (860 17%)\n",
      "Iteration: 10; Loss: 5.442.\n",
      "101m 4s (- 485m 54s) (861 17%)\n",
      "Iteration: 10; Loss: 5.504.\n",
      "101m 11s (- 485m 47s) (862 17%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "101m 18s (- 485m 40s) (863 17%)\n",
      "Iteration: 10; Loss: 5.648.\n",
      "101m 25s (- 485m 33s) (864 17%)\n",
      "Iteration: 10; Loss: 5.515.\n",
      "101m 32s (- 485m 26s) (865 17%)\n",
      "Iteration: 10; Loss: 5.674.\n",
      "101m 40s (- 485m 19s) (866 17%)\n",
      "Iteration: 10; Loss: 5.560.\n",
      "101m 47s (- 485m 12s) (867 17%)\n",
      "Iteration: 10; Loss: 5.594.\n",
      "101m 54s (- 485m 5s) (868 17%)\n",
      "Iteration: 10; Loss: 5.586.\n",
      "102m 1s (- 484m 58s) (869 17%)\n",
      "Iteration: 10; Loss: 5.389.\n",
      "102m 8s (- 484m 51s) (870 17%)\n",
      "Iteration: 10; Loss: 5.654.\n",
      "102m 15s (- 484m 44s) (871 17%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "102m 22s (- 484m 37s) (872 17%)\n",
      "Iteration: 10; Loss: 5.496.\n",
      "102m 29s (- 484m 29s) (873 17%)\n",
      "Iteration: 10; Loss: 5.577.\n",
      "102m 36s (- 484m 22s) (874 17%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "102m 43s (- 484m 15s) (875 17%)\n",
      "Iteration: 10; Loss: 5.712.\n",
      "102m 50s (- 484m 8s) (876 17%)\n",
      "Iteration: 10; Loss: 5.717.\n",
      "102m 57s (- 484m 1s) (877 17%)\n",
      "Iteration: 10; Loss: 5.743.\n",
      "103m 4s (- 483m 54s) (878 17%)\n",
      "Iteration: 10; Loss: 5.471.\n",
      "103m 11s (- 483m 47s) (879 17%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "103m 18s (- 483m 40s) (880 17%)\n",
      "Iteration: 10; Loss: 5.767.\n",
      "103m 25s (- 483m 33s) (881 17%)\n",
      "Iteration: 10; Loss: 5.605.\n",
      "103m 32s (- 483m 25s) (882 17%)\n",
      "Iteration: 10; Loss: 5.664.\n",
      "103m 39s (- 483m 18s) (883 17%)\n",
      "Iteration: 10; Loss: 5.593.\n",
      "103m 46s (- 483m 11s) (884 17%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "103m 53s (- 483m 4s) (885 17%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "104m 0s (- 482m 57s) (886 17%)\n",
      "Iteration: 10; Loss: 5.452.\n",
      "104m 7s (- 482m 50s) (887 17%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "104m 14s (- 482m 43s) (888 17%)\n",
      "Iteration: 10; Loss: 5.494.\n",
      "104m 21s (- 482m 36s) (889 17%)\n",
      "Iteration: 10; Loss: 5.679.\n",
      "104m 28s (- 482m 29s) (890 17%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "104m 35s (- 482m 22s) (891 17%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "104m 42s (- 482m 15s) (892 17%)\n",
      "Iteration: 10; Loss: 5.643.\n",
      "104m 49s (- 482m 8s) (893 17%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "104m 56s (- 482m 0s) (894 17%)\n",
      "Iteration: 10; Loss: 5.522.\n",
      "105m 4s (- 481m 53s) (895 17%)\n",
      "Iteration: 10; Loss: 5.546.\n",
      "105m 11s (- 481m 46s) (896 17%)\n",
      "Iteration: 10; Loss: 5.497.\n",
      "105m 18s (- 481m 39s) (897 17%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "105m 25s (- 481m 32s) (898 17%)\n",
      "Iteration: 10; Loss: 5.495.\n",
      "105m 32s (- 481m 25s) (899 17%)\n",
      "Iteration: 10; Loss: 5.472.\n",
      "105m 39s (- 481m 18s) (900 18%)\n",
      "Iteration: 10; Loss: 5.697.\n",
      "105m 46s (- 481m 11s) (901 18%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "105m 53s (- 481m 4s) (902 18%)\n",
      "Iteration: 10; Loss: 5.659.\n",
      "106m 0s (- 480m 57s) (903 18%)\n",
      "Iteration: 10; Loss: 5.527.\n",
      "106m 7s (- 480m 50s) (904 18%)\n",
      "Iteration: 10; Loss: 5.563.\n",
      "106m 14s (- 480m 43s) (905 18%)\n",
      "Iteration: 10; Loss: 5.700.\n",
      "106m 21s (- 480m 36s) (906 18%)\n",
      "Iteration: 10; Loss: 5.662.\n",
      "106m 28s (- 480m 29s) (907 18%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "106m 35s (- 480m 22s) (908 18%)\n",
      "Iteration: 10; Loss: 5.367.\n",
      "106m 42s (- 480m 15s) (909 18%)\n",
      "Iteration: 10; Loss: 5.781.\n",
      "106m 49s (- 480m 8s) (910 18%)\n",
      "Iteration: 10; Loss: 5.442.\n",
      "106m 56s (- 480m 1s) (911 18%)\n",
      "Iteration: 10; Loss: 5.709.\n",
      "107m 3s (- 479m 54s) (912 18%)\n",
      "Iteration: 10; Loss: 5.483.\n",
      "107m 10s (- 479m 46s) (913 18%)\n",
      "Iteration: 10; Loss: 5.429.\n",
      "107m 17s (- 479m 39s) (914 18%)\n",
      "Iteration: 10; Loss: 5.838.\n",
      "107m 24s (- 479m 32s) (915 18%)\n",
      "Iteration: 10; Loss: 5.711.\n",
      "107m 31s (- 479m 25s) (916 18%)\n",
      "Iteration: 10; Loss: 5.623.\n",
      "107m 38s (- 479m 18s) (917 18%)\n",
      "Iteration: 10; Loss: 5.524.\n",
      "107m 45s (- 479m 11s) (918 18%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "107m 52s (- 479m 4s) (919 18%)\n",
      "Iteration: 10; Loss: 5.488.\n",
      "108m 0s (- 478m 57s) (920 18%)\n",
      "Iteration: 10; Loss: 5.515.\n",
      "108m 7s (- 478m 50s) (921 18%)\n",
      "Iteration: 10; Loss: 5.515.\n",
      "108m 14s (- 478m 43s) (922 18%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "108m 21s (- 478m 36s) (923 18%)\n",
      "Iteration: 10; Loss: 5.430.\n",
      "108m 28s (- 478m 29s) (924 18%)\n",
      "Iteration: 10; Loss: 5.727.\n",
      "108m 35s (- 478m 21s) (925 18%)\n",
      "Iteration: 10; Loss: 5.695.\n",
      "108m 42s (- 478m 14s) (926 18%)\n",
      "Iteration: 10; Loss: 5.616.\n",
      "108m 49s (- 478m 7s) (927 18%)\n",
      "Iteration: 10; Loss: 5.519.\n",
      "108m 56s (- 478m 0s) (928 18%)\n",
      "Iteration: 10; Loss: 5.668.\n",
      "109m 3s (- 477m 53s) (929 18%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "109m 10s (- 477m 46s) (930 18%)\n",
      "Iteration: 10; Loss: 5.682.\n",
      "109m 17s (- 477m 39s) (931 18%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "109m 24s (- 477m 32s) (932 18%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "109m 31s (- 477m 25s) (933 18%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "109m 38s (- 477m 18s) (934 18%)\n",
      "Iteration: 10; Loss: 5.516.\n",
      "109m 45s (- 477m 11s) (935 18%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "109m 52s (- 477m 4s) (936 18%)\n",
      "Iteration: 10; Loss: 5.446.\n",
      "109m 59s (- 476m 57s) (937 18%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "110m 6s (- 476m 50s) (938 18%)\n",
      "Iteration: 10; Loss: 5.708.\n",
      "110m 13s (- 476m 43s) (939 18%)\n",
      "Iteration: 10; Loss: 5.662.\n",
      "110m 20s (- 476m 36s) (940 18%)\n",
      "Iteration: 10; Loss: 5.492.\n",
      "110m 27s (- 476m 29s) (941 18%)\n",
      "Iteration: 10; Loss: 5.331.\n",
      "110m 34s (- 476m 22s) (942 18%)\n",
      "Iteration: 10; Loss: 5.481.\n",
      "110m 42s (- 476m 15s) (943 18%)\n",
      "Iteration: 10; Loss: 5.639.\n",
      "110m 49s (- 476m 8s) (944 18%)\n",
      "Iteration: 10; Loss: 5.661.\n",
      "110m 56s (- 476m 1s) (945 18%)\n",
      "Iteration: 10; Loss: 5.542.\n",
      "111m 3s (- 475m 54s) (946 18%)\n",
      "Iteration: 10; Loss: 5.388.\n",
      "111m 10s (- 475m 47s) (947 18%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "111m 17s (- 475m 40s) (948 18%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "111m 24s (- 475m 33s) (949 18%)\n",
      "Iteration: 10; Loss: 5.470.\n",
      "111m 31s (- 475m 26s) (950 19%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "111m 38s (- 475m 19s) (951 19%)\n",
      "Iteration: 10; Loss: 5.607.\n",
      "111m 45s (- 475m 11s) (952 19%)\n",
      "Iteration: 10; Loss: 5.642.\n",
      "111m 52s (- 475m 4s) (953 19%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "111m 59s (- 474m 57s) (954 19%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "112m 6s (- 474m 50s) (955 19%)\n",
      "Iteration: 10; Loss: 5.506.\n",
      "112m 13s (- 474m 43s) (956 19%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "112m 20s (- 474m 36s) (957 19%)\n",
      "Iteration: 10; Loss: 5.528.\n",
      "112m 27s (- 474m 29s) (958 19%)\n",
      "Iteration: 10; Loss: 5.728.\n",
      "112m 34s (- 474m 22s) (959 19%)\n",
      "Iteration: 10; Loss: 5.546.\n",
      "112m 41s (- 474m 15s) (960 19%)\n",
      "Iteration: 10; Loss: 5.677.\n",
      "112m 48s (- 474m 7s) (961 19%)\n",
      "Iteration: 10; Loss: 5.709.\n",
      "112m 55s (- 474m 1s) (962 19%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "113m 2s (- 473m 54s) (963 19%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "113m 9s (- 473m 47s) (964 19%)\n",
      "Iteration: 10; Loss: 5.711.\n",
      "113m 16s (- 473m 40s) (965 19%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "113m 23s (- 473m 32s) (966 19%)\n",
      "Iteration: 10; Loss: 5.644.\n",
      "113m 30s (- 473m 25s) (967 19%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "113m 37s (- 473m 18s) (968 19%)\n",
      "Iteration: 10; Loss: 5.728.\n",
      "113m 44s (- 473m 11s) (969 19%)\n",
      "Iteration: 10; Loss: 5.440.\n",
      "113m 51s (- 473m 4s) (970 19%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "113m 59s (- 472m 57s) (971 19%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "114m 6s (- 472m 50s) (972 19%)\n",
      "Iteration: 10; Loss: 5.443.\n",
      "114m 13s (- 472m 43s) (973 19%)\n",
      "Iteration: 10; Loss: 5.508.\n",
      "114m 20s (- 472m 35s) (974 19%)\n",
      "Iteration: 10; Loss: 5.597.\n",
      "114m 27s (- 472m 28s) (975 19%)\n",
      "Iteration: 10; Loss: 5.622.\n",
      "114m 34s (- 472m 21s) (976 19%)\n",
      "Iteration: 10; Loss: 5.619.\n",
      "114m 41s (- 472m 14s) (977 19%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "114m 48s (- 472m 7s) (978 19%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "114m 55s (- 472m 0s) (979 19%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "115m 2s (- 471m 53s) (980 19%)\n",
      "Iteration: 10; Loss: 5.549.\n",
      "115m 9s (- 471m 46s) (981 19%)\n",
      "Iteration: 10; Loss: 5.511.\n",
      "115m 16s (- 471m 40s) (982 19%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "115m 23s (- 471m 32s) (983 19%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "115m 30s (- 471m 25s) (984 19%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "115m 37s (- 471m 18s) (985 19%)\n",
      "Iteration: 10; Loss: 5.621.\n",
      "115m 44s (- 471m 11s) (986 19%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "115m 51s (- 471m 4s) (987 19%)\n",
      "Iteration: 10; Loss: 5.644.\n",
      "115m 58s (- 470m 57s) (988 19%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "116m 5s (- 470m 50s) (989 19%)\n",
      "Iteration: 10; Loss: 5.607.\n",
      "116m 12s (- 470m 43s) (990 19%)\n",
      "Iteration: 10; Loss: 5.467.\n",
      "116m 19s (- 470m 36s) (991 19%)\n",
      "Iteration: 10; Loss: 5.537.\n",
      "116m 27s (- 470m 29s) (992 19%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "116m 34s (- 470m 22s) (993 19%)\n",
      "Iteration: 10; Loss: 5.614.\n",
      "116m 41s (- 470m 15s) (994 19%)\n",
      "Iteration: 10; Loss: 5.628.\n",
      "116m 48s (- 470m 8s) (995 19%)\n",
      "Iteration: 10; Loss: 5.692.\n",
      "116m 55s (- 470m 1s) (996 19%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "117m 2s (- 469m 54s) (997 19%)\n",
      "Iteration: 10; Loss: 5.711.\n",
      "117m 9s (- 469m 47s) (998 19%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "117m 16s (- 469m 40s) (999 19%)\n",
      "Iteration: 10; Loss: 5.550.\n",
      "117m 23s (- 469m 33s) (1000 20%)\n",
      "Iteration: 10; Loss: 5.458.\n",
      "117m 30s (- 469m 26s) (1001 20%)\n",
      "Iteration: 10; Loss: 5.583.\n",
      "117m 37s (- 469m 19s) (1002 20%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "117m 44s (- 469m 12s) (1003 20%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "117m 51s (- 469m 5s) (1004 20%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "117m 58s (- 468m 58s) (1005 20%)\n",
      "Iteration: 10; Loss: 5.659.\n",
      "118m 5s (- 468m 51s) (1006 20%)\n",
      "Iteration: 10; Loss: 5.503.\n",
      "118m 12s (- 468m 44s) (1007 20%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "118m 19s (- 468m 37s) (1008 20%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "118m 26s (- 468m 30s) (1009 20%)\n",
      "Iteration: 10; Loss: 5.420.\n",
      "118m 33s (- 468m 23s) (1010 20%)\n",
      "Iteration: 10; Loss: 5.722.\n",
      "118m 40s (- 468m 16s) (1011 20%)\n",
      "Iteration: 10; Loss: 5.664.\n",
      "118m 47s (- 468m 9s) (1012 20%)\n",
      "Iteration: 10; Loss: 5.594.\n",
      "118m 55s (- 468m 2s) (1013 20%)\n",
      "Iteration: 10; Loss: 5.641.\n",
      "119m 2s (- 467m 55s) (1014 20%)\n",
      "Iteration: 10; Loss: 5.674.\n",
      "119m 9s (- 467m 48s) (1015 20%)\n",
      "Iteration: 10; Loss: 5.496.\n",
      "119m 16s (- 467m 41s) (1016 20%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "119m 23s (- 467m 34s) (1017 20%)\n",
      "Iteration: 10; Loss: 5.620.\n",
      "119m 30s (- 467m 27s) (1018 20%)\n",
      "Iteration: 10; Loss: 5.538.\n",
      "119m 37s (- 467m 20s) (1019 20%)\n",
      "Iteration: 10; Loss: 5.695.\n",
      "119m 44s (- 467m 13s) (1020 20%)\n",
      "Iteration: 10; Loss: 5.695.\n",
      "119m 51s (- 467m 6s) (1021 20%)\n",
      "Iteration: 10; Loss: 5.697.\n",
      "119m 58s (- 466m 59s) (1022 20%)\n",
      "Iteration: 10; Loss: 5.496.\n",
      "120m 5s (- 466m 52s) (1023 20%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "120m 12s (- 466m 45s) (1024 20%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "120m 19s (- 466m 37s) (1025 20%)\n",
      "Iteration: 10; Loss: 5.636.\n",
      "120m 26s (- 466m 30s) (1026 20%)\n",
      "Iteration: 10; Loss: 5.538.\n",
      "120m 33s (- 466m 23s) (1027 20%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "120m 40s (- 466m 16s) (1028 20%)\n",
      "Iteration: 10; Loss: 5.487.\n",
      "120m 47s (- 466m 9s) (1029 20%)\n",
      "Iteration: 10; Loss: 5.651.\n",
      "120m 54s (- 466m 2s) (1030 20%)\n",
      "Iteration: 10; Loss: 5.663.\n",
      "121m 1s (- 465m 55s) (1031 20%)\n",
      "Iteration: 10; Loss: 5.712.\n",
      "121m 8s (- 465m 48s) (1032 20%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "121m 15s (- 465m 41s) (1033 20%)\n",
      "Iteration: 10; Loss: 5.786.\n",
      "121m 22s (- 465m 34s) (1034 20%)\n",
      "Iteration: 10; Loss: 5.472.\n",
      "121m 29s (- 465m 27s) (1035 20%)\n",
      "Iteration: 10; Loss: 5.573.\n",
      "121m 36s (- 465m 19s) (1036 20%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "121m 43s (- 465m 12s) (1037 20%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "121m 51s (- 465m 5s) (1038 20%)\n",
      "Iteration: 10; Loss: 5.547.\n",
      "121m 58s (- 464m 59s) (1039 20%)\n",
      "Iteration: 10; Loss: 5.489.\n",
      "122m 5s (- 464m 52s) (1040 20%)\n",
      "Iteration: 10; Loss: 5.475.\n",
      "122m 12s (- 464m 45s) (1041 20%)\n",
      "Iteration: 10; Loss: 5.692.\n",
      "122m 19s (- 464m 38s) (1042 20%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "122m 26s (- 464m 30s) (1043 20%)\n",
      "Iteration: 10; Loss: 5.509.\n",
      "122m 33s (- 464m 23s) (1044 20%)\n",
      "Iteration: 10; Loss: 5.499.\n",
      "122m 40s (- 464m 16s) (1045 20%)\n",
      "Iteration: 10; Loss: 5.521.\n",
      "122m 47s (- 464m 9s) (1046 20%)\n",
      "Iteration: 10; Loss: 5.803.\n",
      "122m 54s (- 464m 2s) (1047 20%)\n",
      "Iteration: 10; Loss: 5.536.\n",
      "123m 1s (- 463m 55s) (1048 20%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "123m 8s (- 463m 48s) (1049 20%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "123m 15s (- 463m 41s) (1050 21%)\n",
      "Iteration: 10; Loss: 5.489.\n",
      "123m 22s (- 463m 33s) (1051 21%)\n",
      "Iteration: 10; Loss: 5.656.\n",
      "123m 29s (- 463m 26s) (1052 21%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "123m 36s (- 463m 19s) (1053 21%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "123m 43s (- 463m 12s) (1054 21%)\n",
      "Iteration: 10; Loss: 5.434.\n",
      "123m 50s (- 463m 5s) (1055 21%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "123m 57s (- 462m 58s) (1056 21%)\n",
      "Iteration: 10; Loss: 5.551.\n",
      "124m 4s (- 462m 51s) (1057 21%)\n",
      "Iteration: 10; Loss: 5.607.\n",
      "124m 11s (- 462m 44s) (1058 21%)\n",
      "Iteration: 10; Loss: 5.593.\n",
      "124m 18s (- 462m 37s) (1059 21%)\n",
      "Iteration: 10; Loss: 5.521.\n",
      "124m 25s (- 462m 30s) (1060 21%)\n",
      "Iteration: 10; Loss: 5.684.\n",
      "124m 32s (- 462m 23s) (1061 21%)\n",
      "Iteration: 10; Loss: 5.790.\n",
      "124m 39s (- 462m 16s) (1062 21%)\n",
      "Iteration: 10; Loss: 5.616.\n",
      "124m 46s (- 462m 9s) (1063 21%)\n",
      "Iteration: 10; Loss: 5.786.\n",
      "124m 53s (- 462m 2s) (1064 21%)\n",
      "Iteration: 10; Loss: 5.686.\n",
      "125m 1s (- 461m 55s) (1065 21%)\n",
      "Iteration: 10; Loss: 5.438.\n",
      "125m 8s (- 461m 48s) (1066 21%)\n",
      "Iteration: 10; Loss: 5.641.\n",
      "125m 15s (- 461m 41s) (1067 21%)\n",
      "Iteration: 10; Loss: 5.389.\n",
      "125m 22s (- 461m 33s) (1068 21%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "125m 29s (- 461m 26s) (1069 21%)\n",
      "Iteration: 10; Loss: 5.378.\n",
      "125m 36s (- 461m 19s) (1070 21%)\n",
      "Iteration: 10; Loss: 5.476.\n",
      "125m 43s (- 461m 12s) (1071 21%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "125m 50s (- 461m 5s) (1072 21%)\n",
      "Iteration: 10; Loss: 5.493.\n",
      "125m 57s (- 460m 58s) (1073 21%)\n",
      "Iteration: 10; Loss: 5.505.\n",
      "126m 4s (- 460m 51s) (1074 21%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "126m 11s (- 460m 43s) (1075 21%)\n",
      "Iteration: 10; Loss: 5.586.\n",
      "126m 18s (- 460m 36s) (1076 21%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "126m 25s (- 460m 29s) (1077 21%)\n",
      "Iteration: 10; Loss: 5.743.\n",
      "126m 32s (- 460m 22s) (1078 21%)\n",
      "Iteration: 10; Loss: 5.504.\n",
      "126m 39s (- 460m 15s) (1079 21%)\n",
      "Iteration: 10; Loss: 5.625.\n",
      "126m 46s (- 460m 8s) (1080 21%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "126m 53s (- 460m 1s) (1081 21%)\n",
      "Iteration: 10; Loss: 5.568.\n",
      "127m 0s (- 459m 54s) (1082 21%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "127m 7s (- 459m 46s) (1083 21%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "127m 14s (- 459m 39s) (1084 21%)\n",
      "Iteration: 10; Loss: 5.370.\n",
      "127m 21s (- 459m 32s) (1085 21%)\n",
      "Iteration: 10; Loss: 5.591.\n",
      "127m 28s (- 459m 25s) (1086 21%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "127m 35s (- 459m 18s) (1087 21%)\n",
      "Iteration: 10; Loss: 5.684.\n",
      "127m 42s (- 459m 11s) (1088 21%)\n",
      "Iteration: 10; Loss: 5.431.\n",
      "127m 49s (- 459m 4s) (1089 21%)\n",
      "Iteration: 10; Loss: 5.486.\n",
      "127m 56s (- 458m 57s) (1090 21%)\n",
      "Iteration: 10; Loss: 5.441.\n",
      "128m 3s (- 458m 50s) (1091 21%)\n",
      "Iteration: 10; Loss: 5.446.\n",
      "128m 10s (- 458m 43s) (1092 21%)\n",
      "Iteration: 10; Loss: 5.511.\n",
      "128m 17s (- 458m 36s) (1093 21%)\n",
      "Iteration: 10; Loss: 5.633.\n",
      "128m 24s (- 458m 29s) (1094 21%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "128m 31s (- 458m 22s) (1095 21%)\n",
      "Iteration: 10; Loss: 5.579.\n",
      "128m 38s (- 458m 15s) (1096 21%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "128m 45s (- 458m 7s) (1097 21%)\n",
      "Iteration: 10; Loss: 5.613.\n",
      "128m 52s (- 458m 0s) (1098 21%)\n",
      "Iteration: 10; Loss: 5.655.\n",
      "128m 59s (- 457m 53s) (1099 21%)\n",
      "Iteration: 10; Loss: 5.508.\n",
      "129m 6s (- 457m 46s) (1100 22%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "129m 14s (- 457m 39s) (1101 22%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "129m 21s (- 457m 32s) (1102 22%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "129m 28s (- 457m 25s) (1103 22%)\n",
      "Iteration: 10; Loss: 5.717.\n",
      "129m 35s (- 457m 18s) (1104 22%)\n",
      "Iteration: 10; Loss: 5.641.\n",
      "129m 42s (- 457m 11s) (1105 22%)\n",
      "Iteration: 10; Loss: 5.496.\n",
      "129m 49s (- 457m 4s) (1106 22%)\n",
      "Iteration: 10; Loss: 5.712.\n",
      "129m 56s (- 456m 57s) (1107 22%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "130m 3s (- 456m 50s) (1108 22%)\n",
      "Iteration: 10; Loss: 5.528.\n",
      "130m 10s (- 456m 43s) (1109 22%)\n",
      "Iteration: 10; Loss: 5.505.\n",
      "130m 17s (- 456m 36s) (1110 22%)\n",
      "Iteration: 10; Loss: 5.699.\n",
      "130m 24s (- 456m 29s) (1111 22%)\n",
      "Iteration: 10; Loss: 5.618.\n",
      "130m 31s (- 456m 21s) (1112 22%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "130m 38s (- 456m 14s) (1113 22%)\n",
      "Iteration: 10; Loss: 5.608.\n",
      "130m 45s (- 456m 7s) (1114 22%)\n",
      "Iteration: 10; Loss: 5.456.\n",
      "130m 52s (- 456m 0s) (1115 22%)\n",
      "Iteration: 10; Loss: 5.706.\n",
      "130m 59s (- 455m 53s) (1116 22%)\n",
      "Iteration: 10; Loss: 5.565.\n",
      "131m 6s (- 455m 46s) (1117 22%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "131m 13s (- 455m 39s) (1118 22%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "131m 20s (- 455m 32s) (1119 22%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "131m 27s (- 455m 25s) (1120 22%)\n",
      "Iteration: 10; Loss: 5.488.\n",
      "131m 34s (- 455m 18s) (1121 22%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "131m 41s (- 455m 11s) (1122 22%)\n",
      "Iteration: 10; Loss: 5.481.\n",
      "131m 49s (- 455m 4s) (1123 22%)\n",
      "Iteration: 10; Loss: 5.542.\n",
      "131m 56s (- 454m 57s) (1124 22%)\n",
      "Iteration: 10; Loss: 5.376.\n",
      "132m 3s (- 454m 50s) (1125 22%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "132m 10s (- 454m 43s) (1126 22%)\n",
      "Iteration: 10; Loss: 5.435.\n",
      "132m 17s (- 454m 36s) (1127 22%)\n",
      "Iteration: 10; Loss: 5.674.\n",
      "132m 24s (- 454m 29s) (1128 22%)\n",
      "Iteration: 10; Loss: 5.482.\n",
      "132m 31s (- 454m 22s) (1129 22%)\n",
      "Iteration: 10; Loss: 5.456.\n",
      "132m 38s (- 454m 15s) (1130 22%)\n",
      "Iteration: 10; Loss: 5.623.\n",
      "132m 45s (- 454m 8s) (1131 22%)\n",
      "Iteration: 10; Loss: 5.582.\n",
      "132m 52s (- 454m 0s) (1132 22%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "132m 59s (- 453m 53s) (1133 22%)\n",
      "Iteration: 10; Loss: 5.471.\n",
      "133m 6s (- 453m 46s) (1134 22%)\n",
      "Iteration: 10; Loss: 5.427.\n",
      "133m 13s (- 453m 39s) (1135 22%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "133m 20s (- 453m 32s) (1136 22%)\n",
      "Iteration: 10; Loss: 5.560.\n",
      "133m 27s (- 453m 25s) (1137 22%)\n",
      "Iteration: 10; Loss: 5.464.\n",
      "133m 34s (- 453m 18s) (1138 22%)\n",
      "Iteration: 10; Loss: 5.653.\n",
      "133m 41s (- 453m 11s) (1139 22%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "133m 48s (- 453m 4s) (1140 22%)\n",
      "Iteration: 10; Loss: 5.528.\n",
      "133m 55s (- 452m 57s) (1141 22%)\n",
      "Iteration: 10; Loss: 5.469.\n",
      "134m 2s (- 452m 50s) (1142 22%)\n",
      "Iteration: 10; Loss: 5.519.\n",
      "134m 9s (- 452m 43s) (1143 22%)\n",
      "Iteration: 10; Loss: 5.721.\n",
      "134m 16s (- 452m 36s) (1144 22%)\n",
      "Iteration: 10; Loss: 5.513.\n",
      "134m 23s (- 452m 29s) (1145 22%)\n",
      "Iteration: 10; Loss: 5.612.\n",
      "134m 30s (- 452m 22s) (1146 22%)\n",
      "Iteration: 10; Loss: 5.543.\n",
      "134m 37s (- 452m 15s) (1147 22%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "134m 44s (- 452m 8s) (1148 22%)\n",
      "Iteration: 10; Loss: 5.498.\n",
      "134m 51s (- 452m 1s) (1149 22%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "134m 59s (- 451m 54s) (1150 23%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "135m 6s (- 451m 47s) (1151 23%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "135m 13s (- 451m 40s) (1152 23%)\n",
      "Iteration: 10; Loss: 5.545.\n",
      "135m 20s (- 451m 33s) (1153 23%)\n",
      "Iteration: 10; Loss: 5.681.\n",
      "135m 27s (- 451m 26s) (1154 23%)\n",
      "Iteration: 10; Loss: 5.577.\n",
      "135m 34s (- 451m 18s) (1155 23%)\n",
      "Iteration: 10; Loss: 5.511.\n",
      "135m 41s (- 451m 11s) (1156 23%)\n",
      "Iteration: 10; Loss: 5.695.\n",
      "135m 48s (- 451m 4s) (1157 23%)\n",
      "Iteration: 10; Loss: 5.554.\n",
      "135m 55s (- 450m 57s) (1158 23%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "136m 2s (- 450m 50s) (1159 23%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "136m 9s (- 450m 43s) (1160 23%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "136m 16s (- 450m 36s) (1161 23%)\n",
      "Iteration: 10; Loss: 5.714.\n",
      "136m 23s (- 450m 29s) (1162 23%)\n",
      "Iteration: 10; Loss: 5.619.\n",
      "136m 30s (- 450m 22s) (1163 23%)\n",
      "Iteration: 10; Loss: 5.621.\n",
      "136m 37s (- 450m 15s) (1164 23%)\n",
      "Iteration: 10; Loss: 5.829.\n",
      "136m 44s (- 450m 8s) (1165 23%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "136m 51s (- 450m 0s) (1166 23%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "136m 58s (- 449m 54s) (1167 23%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "137m 5s (- 449m 46s) (1168 23%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "137m 12s (- 449m 39s) (1169 23%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "137m 19s (- 449m 32s) (1170 23%)\n",
      "Iteration: 10; Loss: 5.675.\n",
      "137m 26s (- 449m 25s) (1171 23%)\n",
      "Iteration: 10; Loss: 5.501.\n",
      "137m 33s (- 449m 18s) (1172 23%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "137m 40s (- 449m 11s) (1173 23%)\n",
      "Iteration: 10; Loss: 5.506.\n",
      "137m 47s (- 449m 4s) (1174 23%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "137m 54s (- 448m 57s) (1175 23%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "138m 1s (- 448m 49s) (1176 23%)\n",
      "Iteration: 10; Loss: 5.813.\n",
      "138m 8s (- 448m 43s) (1177 23%)\n",
      "Iteration: 10; Loss: 5.646.\n",
      "138m 15s (- 448m 35s) (1178 23%)\n",
      "Iteration: 10; Loss: 5.486.\n",
      "138m 22s (- 448m 28s) (1179 23%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "138m 29s (- 448m 21s) (1180 23%)\n",
      "Iteration: 10; Loss: 5.424.\n",
      "138m 36s (- 448m 14s) (1181 23%)\n",
      "Iteration: 10; Loss: 5.605.\n",
      "138m 44s (- 448m 7s) (1182 23%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "138m 51s (- 448m 0s) (1183 23%)\n",
      "Iteration: 10; Loss: 5.391.\n",
      "138m 58s (- 447m 53s) (1184 23%)\n",
      "Iteration: 10; Loss: 5.480.\n",
      "139m 5s (- 447m 46s) (1185 23%)\n",
      "Iteration: 10; Loss: 5.488.\n",
      "139m 12s (- 447m 39s) (1186 23%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "139m 19s (- 447m 32s) (1187 23%)\n",
      "Iteration: 10; Loss: 5.644.\n",
      "139m 26s (- 447m 25s) (1188 23%)\n",
      "Iteration: 10; Loss: 5.539.\n",
      "139m 33s (- 447m 18s) (1189 23%)\n",
      "Iteration: 10; Loss: 5.480.\n",
      "139m 40s (- 447m 11s) (1190 23%)\n",
      "Iteration: 10; Loss: 5.673.\n",
      "139m 47s (- 447m 4s) (1191 23%)\n",
      "Iteration: 10; Loss: 5.508.\n",
      "139m 54s (- 446m 57s) (1192 23%)\n",
      "Iteration: 10; Loss: 5.495.\n",
      "140m 1s (- 446m 50s) (1193 23%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "140m 8s (- 446m 43s) (1194 23%)\n",
      "Iteration: 10; Loss: 5.501.\n",
      "140m 15s (- 446m 36s) (1195 23%)\n",
      "Iteration: 10; Loss: 5.710.\n",
      "140m 22s (- 446m 29s) (1196 23%)\n",
      "Iteration: 10; Loss: 5.469.\n",
      "140m 29s (- 446m 22s) (1197 23%)\n",
      "Iteration: 10; Loss: 5.715.\n",
      "140m 36s (- 446m 15s) (1198 23%)\n",
      "Iteration: 10; Loss: 5.483.\n",
      "140m 43s (- 446m 8s) (1199 23%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "140m 50s (- 446m 1s) (1200 24%)\n",
      "Iteration: 10; Loss: 5.514.\n",
      "140m 57s (- 445m 53s) (1201 24%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "141m 4s (- 445m 46s) (1202 24%)\n",
      "Iteration: 10; Loss: 5.696.\n",
      "141m 11s (- 445m 39s) (1203 24%)\n",
      "Iteration: 10; Loss: 5.511.\n",
      "141m 18s (- 445m 32s) (1204 24%)\n",
      "Iteration: 10; Loss: 5.599.\n",
      "141m 26s (- 445m 25s) (1205 24%)\n",
      "Iteration: 10; Loss: 5.588.\n",
      "141m 33s (- 445m 18s) (1206 24%)\n",
      "Iteration: 10; Loss: 5.642.\n",
      "141m 40s (- 445m 11s) (1207 24%)\n",
      "Iteration: 10; Loss: 5.520.\n",
      "141m 47s (- 445m 4s) (1208 24%)\n",
      "Iteration: 10; Loss: 5.640.\n",
      "141m 54s (- 444m 57s) (1209 24%)\n",
      "Iteration: 10; Loss: 5.682.\n",
      "142m 1s (- 444m 50s) (1210 24%)\n",
      "Iteration: 10; Loss: 5.762.\n",
      "142m 8s (- 444m 44s) (1211 24%)\n",
      "Iteration: 10; Loss: 5.682.\n",
      "142m 15s (- 444m 37s) (1212 24%)\n",
      "Iteration: 10; Loss: 5.655.\n",
      "142m 22s (- 444m 29s) (1213 24%)\n",
      "Iteration: 10; Loss: 5.613.\n",
      "142m 29s (- 444m 22s) (1214 24%)\n",
      "Iteration: 10; Loss: 5.751.\n",
      "142m 36s (- 444m 15s) (1215 24%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "142m 43s (- 444m 9s) (1216 24%)\n",
      "Iteration: 10; Loss: 5.425.\n",
      "142m 50s (- 444m 1s) (1217 24%)\n",
      "Iteration: 10; Loss: 5.491.\n",
      "142m 57s (- 443m 54s) (1218 24%)\n",
      "Iteration: 10; Loss: 5.545.\n",
      "143m 4s (- 443m 47s) (1219 24%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "143m 11s (- 443m 40s) (1220 24%)\n",
      "Iteration: 10; Loss: 5.496.\n",
      "143m 18s (- 443m 33s) (1221 24%)\n",
      "Iteration: 10; Loss: 5.383.\n",
      "143m 25s (- 443m 26s) (1222 24%)\n",
      "Iteration: 10; Loss: 5.607.\n",
      "143m 33s (- 443m 19s) (1223 24%)\n",
      "Iteration: 10; Loss: 5.475.\n",
      "143m 40s (- 443m 12s) (1224 24%)\n",
      "Iteration: 10; Loss: 5.444.\n",
      "143m 47s (- 443m 5s) (1225 24%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "143m 54s (- 442m 58s) (1226 24%)\n",
      "Iteration: 10; Loss: 5.500.\n",
      "144m 1s (- 442m 50s) (1227 24%)\n",
      "Iteration: 10; Loss: 5.463.\n",
      "144m 8s (- 442m 43s) (1228 24%)\n",
      "Iteration: 10; Loss: 5.510.\n",
      "144m 15s (- 442m 36s) (1229 24%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "144m 22s (- 442m 29s) (1230 24%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "144m 29s (- 442m 22s) (1231 24%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "144m 36s (- 442m 15s) (1232 24%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "144m 43s (- 442m 8s) (1233 24%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "144m 50s (- 442m 1s) (1234 24%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "144m 57s (- 441m 54s) (1235 24%)\n",
      "Iteration: 10; Loss: 5.499.\n",
      "145m 4s (- 441m 47s) (1236 24%)\n",
      "Iteration: 10; Loss: 5.668.\n",
      "145m 11s (- 441m 40s) (1237 24%)\n",
      "Iteration: 10; Loss: 5.482.\n",
      "145m 18s (- 441m 33s) (1238 24%)\n",
      "Iteration: 10; Loss: 5.694.\n",
      "145m 25s (- 441m 26s) (1239 24%)\n",
      "Iteration: 10; Loss: 5.518.\n",
      "145m 32s (- 441m 19s) (1240 24%)\n",
      "Iteration: 10; Loss: 5.633.\n",
      "145m 39s (- 441m 12s) (1241 24%)\n",
      "Iteration: 10; Loss: 5.539.\n",
      "145m 46s (- 441m 5s) (1242 24%)\n",
      "Iteration: 10; Loss: 5.573.\n",
      "145m 53s (- 440m 58s) (1243 24%)\n",
      "Iteration: 10; Loss: 5.394.\n",
      "146m 0s (- 440m 51s) (1244 24%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "146m 7s (- 440m 44s) (1245 24%)\n",
      "Iteration: 10; Loss: 5.775.\n",
      "146m 14s (- 440m 37s) (1246 24%)\n",
      "Iteration: 10; Loss: 5.675.\n",
      "146m 21s (- 440m 29s) (1247 24%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "146m 28s (- 440m 22s) (1248 24%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "146m 35s (- 440m 15s) (1249 24%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "146m 42s (- 440m 8s) (1250 25%)\n",
      "Iteration: 10; Loss: 5.533.\n",
      "146m 49s (- 440m 1s) (1251 25%)\n",
      "Iteration: 10; Loss: 5.427.\n",
      "146m 57s (- 439m 54s) (1252 25%)\n",
      "Iteration: 10; Loss: 5.636.\n",
      "147m 4s (- 439m 47s) (1253 25%)\n",
      "Iteration: 10; Loss: 5.498.\n",
      "147m 11s (- 439m 40s) (1254 25%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "147m 18s (- 439m 33s) (1255 25%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "147m 25s (- 439m 26s) (1256 25%)\n",
      "Iteration: 10; Loss: 5.555.\n",
      "147m 32s (- 439m 19s) (1257 25%)\n",
      "Iteration: 10; Loss: 5.220.\n",
      "147m 39s (- 439m 12s) (1258 25%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "147m 46s (- 439m 4s) (1259 25%)\n",
      "Iteration: 10; Loss: 5.609.\n",
      "147m 53s (- 438m 57s) (1260 25%)\n",
      "Iteration: 10; Loss: 5.529.\n",
      "148m 0s (- 438m 50s) (1261 25%)\n",
      "Iteration: 10; Loss: 5.660.\n",
      "148m 7s (- 438m 43s) (1262 25%)\n",
      "Iteration: 10; Loss: 5.622.\n",
      "148m 14s (- 438m 36s) (1263 25%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "148m 21s (- 438m 29s) (1264 25%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "148m 28s (- 438m 22s) (1265 25%)\n",
      "Iteration: 10; Loss: 5.612.\n",
      "148m 35s (- 438m 15s) (1266 25%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "148m 42s (- 438m 8s) (1267 25%)\n",
      "Iteration: 10; Loss: 5.776.\n",
      "148m 49s (- 438m 1s) (1268 25%)\n",
      "Iteration: 10; Loss: 5.501.\n",
      "148m 56s (- 437m 54s) (1269 25%)\n",
      "Iteration: 10; Loss: 5.478.\n",
      "149m 3s (- 437m 47s) (1270 25%)\n",
      "Iteration: 10; Loss: 5.526.\n",
      "149m 10s (- 437m 40s) (1271 25%)\n",
      "Iteration: 10; Loss: 5.316.\n",
      "149m 17s (- 437m 33s) (1272 25%)\n",
      "Iteration: 10; Loss: 5.700.\n",
      "149m 24s (- 437m 26s) (1273 25%)\n",
      "Iteration: 10; Loss: 5.588.\n",
      "149m 31s (- 437m 19s) (1274 25%)\n",
      "Iteration: 10; Loss: 5.426.\n",
      "149m 38s (- 437m 12s) (1275 25%)\n",
      "Iteration: 10; Loss: 5.418.\n",
      "149m 45s (- 437m 5s) (1276 25%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "149m 52s (- 436m 58s) (1277 25%)\n",
      "Iteration: 10; Loss: 5.430.\n",
      "149m 59s (- 436m 51s) (1278 25%)\n",
      "Iteration: 10; Loss: 5.701.\n",
      "150m 6s (- 436m 44s) (1279 25%)\n",
      "Iteration: 10; Loss: 5.456.\n",
      "150m 14s (- 436m 37s) (1280 25%)\n",
      "Iteration: 10; Loss: 5.460.\n",
      "150m 21s (- 436m 30s) (1281 25%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "150m 28s (- 436m 23s) (1282 25%)\n",
      "Iteration: 10; Loss: 5.610.\n",
      "150m 35s (- 436m 16s) (1283 25%)\n",
      "Iteration: 10; Loss: 5.693.\n",
      "150m 42s (- 436m 9s) (1284 25%)\n",
      "Iteration: 10; Loss: 5.689.\n",
      "150m 49s (- 436m 2s) (1285 25%)\n",
      "Iteration: 10; Loss: 5.514.\n",
      "150m 56s (- 435m 55s) (1286 25%)\n",
      "Iteration: 10; Loss: 5.530.\n",
      "151m 3s (- 435m 48s) (1287 25%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "151m 10s (- 435m 41s) (1288 25%)\n",
      "Iteration: 10; Loss: 5.752.\n",
      "151m 17s (- 435m 34s) (1289 25%)\n",
      "Iteration: 10; Loss: 5.495.\n",
      "151m 24s (- 435m 27s) (1290 25%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "151m 31s (- 435m 20s) (1291 25%)\n",
      "Iteration: 10; Loss: 5.705.\n",
      "151m 38s (- 435m 13s) (1292 25%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "151m 45s (- 435m 6s) (1293 25%)\n",
      "Iteration: 10; Loss: 5.421.\n",
      "151m 52s (- 434m 59s) (1294 25%)\n",
      "Iteration: 10; Loss: 5.618.\n",
      "151m 59s (- 434m 52s) (1295 25%)\n",
      "Iteration: 10; Loss: 5.613.\n",
      "152m 6s (- 434m 45s) (1296 25%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "152m 13s (- 434m 37s) (1297 25%)\n",
      "Iteration: 10; Loss: 5.516.\n",
      "152m 21s (- 434m 30s) (1298 25%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "152m 28s (- 434m 23s) (1299 25%)\n",
      "Iteration: 10; Loss: 5.463.\n",
      "152m 35s (- 434m 16s) (1300 26%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "152m 42s (- 434m 9s) (1301 26%)\n",
      "Iteration: 10; Loss: 5.610.\n",
      "152m 49s (- 434m 2s) (1302 26%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "152m 56s (- 433m 55s) (1303 26%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "153m 3s (- 433m 48s) (1304 26%)\n",
      "Iteration: 10; Loss: 5.639.\n",
      "153m 10s (- 433m 41s) (1305 26%)\n",
      "Iteration: 10; Loss: 5.543.\n",
      "153m 17s (- 433m 34s) (1306 26%)\n",
      "Iteration: 10; Loss: 5.654.\n",
      "153m 24s (- 433m 27s) (1307 26%)\n",
      "Iteration: 10; Loss: 5.697.\n",
      "153m 31s (- 433m 20s) (1308 26%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "153m 38s (- 433m 13s) (1309 26%)\n",
      "Iteration: 10; Loss: 5.390.\n",
      "153m 45s (- 433m 6s) (1310 26%)\n",
      "Iteration: 10; Loss: 5.616.\n",
      "153m 53s (- 433m 0s) (1311 26%)\n",
      "Iteration: 10; Loss: 5.624.\n",
      "154m 0s (- 432m 54s) (1312 26%)\n",
      "Iteration: 10; Loss: 5.651.\n",
      "154m 7s (- 432m 47s) (1313 26%)\n",
      "Iteration: 10; Loss: 5.650.\n",
      "154m 14s (- 432m 40s) (1314 26%)\n",
      "Iteration: 10; Loss: 5.565.\n",
      "154m 21s (- 432m 33s) (1315 26%)\n",
      "Iteration: 10; Loss: 5.354.\n",
      "154m 28s (- 432m 25s) (1316 26%)\n",
      "Iteration: 10; Loss: 5.495.\n",
      "154m 35s (- 432m 18s) (1317 26%)\n",
      "Iteration: 10; Loss: 5.597.\n",
      "154m 42s (- 432m 11s) (1318 26%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "154m 49s (- 432m 4s) (1319 26%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "154m 56s (- 431m 57s) (1320 26%)\n",
      "Iteration: 10; Loss: 5.664.\n",
      "155m 3s (- 431m 50s) (1321 26%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "155m 10s (- 431m 43s) (1322 26%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "155m 17s (- 431m 36s) (1323 26%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "155m 24s (- 431m 29s) (1324 26%)\n",
      "Iteration: 10; Loss: 5.412.\n",
      "155m 31s (- 431m 22s) (1325 26%)\n",
      "Iteration: 10; Loss: 5.576.\n",
      "155m 38s (- 431m 15s) (1326 26%)\n",
      "Iteration: 10; Loss: 5.547.\n",
      "155m 45s (- 431m 8s) (1327 26%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "155m 52s (- 431m 1s) (1328 26%)\n",
      "Iteration: 10; Loss: 5.628.\n",
      "155m 59s (- 430m 54s) (1329 26%)\n",
      "Iteration: 10; Loss: 5.623.\n",
      "156m 7s (- 430m 47s) (1330 26%)\n",
      "Iteration: 10; Loss: 5.789.\n",
      "156m 14s (- 430m 40s) (1331 26%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "156m 21s (- 430m 33s) (1332 26%)\n",
      "Iteration: 10; Loss: 5.634.\n",
      "156m 28s (- 430m 26s) (1333 26%)\n",
      "Iteration: 10; Loss: 5.509.\n",
      "156m 35s (- 430m 19s) (1334 26%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "156m 42s (- 430m 12s) (1335 26%)\n",
      "Iteration: 10; Loss: 5.665.\n",
      "156m 49s (- 430m 5s) (1336 26%)\n",
      "Iteration: 10; Loss: 5.485.\n",
      "156m 56s (- 429m 58s) (1337 26%)\n",
      "Iteration: 10; Loss: 5.639.\n",
      "157m 3s (- 429m 50s) (1338 26%)\n",
      "Iteration: 10; Loss: 5.642.\n",
      "157m 10s (- 429m 43s) (1339 26%)\n",
      "Iteration: 10; Loss: 5.725.\n",
      "157m 17s (- 429m 37s) (1340 26%)\n",
      "Iteration: 10; Loss: 5.550.\n",
      "157m 24s (- 429m 29s) (1341 26%)\n",
      "Iteration: 10; Loss: 5.470.\n",
      "157m 31s (- 429m 23s) (1342 26%)\n",
      "Iteration: 10; Loss: 5.808.\n",
      "157m 38s (- 429m 16s) (1343 26%)\n",
      "Iteration: 10; Loss: 5.507.\n",
      "157m 45s (- 429m 8s) (1344 26%)\n",
      "Iteration: 10; Loss: 5.624.\n",
      "157m 52s (- 429m 1s) (1345 26%)\n",
      "Iteration: 10; Loss: 5.689.\n",
      "157m 59s (- 428m 55s) (1346 26%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "158m 6s (- 428m 47s) (1347 26%)\n",
      "Iteration: 10; Loss: 5.679.\n",
      "158m 13s (- 428m 40s) (1348 26%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "158m 20s (- 428m 33s) (1349 26%)\n",
      "Iteration: 10; Loss: 5.511.\n",
      "158m 27s (- 428m 26s) (1350 27%)\n",
      "Iteration: 10; Loss: 5.799.\n",
      "158m 35s (- 428m 19s) (1351 27%)\n",
      "Iteration: 10; Loss: 5.534.\n",
      "158m 42s (- 428m 12s) (1352 27%)\n",
      "Iteration: 10; Loss: 5.606.\n",
      "158m 49s (- 428m 5s) (1353 27%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "158m 56s (- 427m 58s) (1354 27%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "159m 3s (- 427m 51s) (1355 27%)\n",
      "Iteration: 10; Loss: 5.371.\n",
      "159m 10s (- 427m 44s) (1356 27%)\n",
      "Iteration: 10; Loss: 5.693.\n",
      "159m 17s (- 427m 37s) (1357 27%)\n",
      "Iteration: 10; Loss: 5.646.\n",
      "159m 24s (- 427m 30s) (1358 27%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "159m 31s (- 427m 23s) (1359 27%)\n",
      "Iteration: 10; Loss: 5.500.\n",
      "159m 38s (- 427m 16s) (1360 27%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "159m 45s (- 427m 9s) (1361 27%)\n",
      "Iteration: 10; Loss: 5.501.\n",
      "159m 52s (- 427m 2s) (1362 27%)\n",
      "Iteration: 10; Loss: 5.413.\n",
      "159m 59s (- 426m 55s) (1363 27%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "160m 6s (- 426m 47s) (1364 27%)\n",
      "Iteration: 10; Loss: 5.455.\n",
      "160m 13s (- 426m 40s) (1365 27%)\n",
      "Iteration: 10; Loss: 5.575.\n",
      "160m 20s (- 426m 33s) (1366 27%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "160m 27s (- 426m 26s) (1367 27%)\n",
      "Iteration: 10; Loss: 5.562.\n",
      "160m 34s (- 426m 19s) (1368 27%)\n",
      "Iteration: 10; Loss: 5.578.\n",
      "160m 41s (- 426m 12s) (1369 27%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "160m 48s (- 426m 5s) (1370 27%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "160m 55s (- 425m 58s) (1371 27%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "161m 2s (- 425m 51s) (1372 27%)\n",
      "Iteration: 10; Loss: 5.550.\n",
      "161m 9s (- 425m 44s) (1373 27%)\n",
      "Iteration: 10; Loss: 5.621.\n",
      "161m 16s (- 425m 37s) (1374 27%)\n",
      "Iteration: 10; Loss: 5.453.\n",
      "161m 23s (- 425m 30s) (1375 27%)\n",
      "Iteration: 10; Loss: 5.623.\n",
      "161m 30s (- 425m 23s) (1376 27%)\n",
      "Iteration: 10; Loss: 5.478.\n",
      "161m 37s (- 425m 16s) (1377 27%)\n",
      "Iteration: 10; Loss: 5.606.\n",
      "161m 45s (- 425m 9s) (1378 27%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "161m 52s (- 425m 2s) (1379 27%)\n",
      "Iteration: 10; Loss: 5.591.\n",
      "161m 59s (- 424m 55s) (1380 27%)\n",
      "Iteration: 10; Loss: 5.490.\n",
      "162m 6s (- 424m 48s) (1381 27%)\n",
      "Iteration: 10; Loss: 5.686.\n",
      "162m 13s (- 424m 41s) (1382 27%)\n",
      "Iteration: 10; Loss: 5.515.\n",
      "162m 20s (- 424m 34s) (1383 27%)\n",
      "Iteration: 10; Loss: 5.696.\n",
      "162m 27s (- 424m 27s) (1384 27%)\n",
      "Iteration: 10; Loss: 5.514.\n",
      "162m 34s (- 424m 19s) (1385 27%)\n",
      "Iteration: 10; Loss: 5.676.\n",
      "162m 41s (- 424m 12s) (1386 27%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "162m 48s (- 424m 5s) (1387 27%)\n",
      "Iteration: 10; Loss: 5.544.\n",
      "162m 55s (- 423m 58s) (1388 27%)\n",
      "Iteration: 10; Loss: 5.533.\n",
      "163m 2s (- 423m 51s) (1389 27%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "163m 9s (- 423m 44s) (1390 27%)\n",
      "Iteration: 10; Loss: 5.678.\n",
      "163m 16s (- 423m 37s) (1391 27%)\n",
      "Iteration: 10; Loss: 5.463.\n",
      "163m 23s (- 423m 30s) (1392 27%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "163m 30s (- 423m 23s) (1393 27%)\n",
      "Iteration: 10; Loss: 5.614.\n",
      "163m 37s (- 423m 16s) (1394 27%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "163m 44s (- 423m 9s) (1395 27%)\n",
      "Iteration: 10; Loss: 5.613.\n",
      "163m 51s (- 423m 2s) (1396 27%)\n",
      "Iteration: 10; Loss: 5.517.\n",
      "163m 58s (- 422m 55s) (1397 27%)\n",
      "Iteration: 10; Loss: 5.549.\n",
      "164m 6s (- 422m 48s) (1398 27%)\n",
      "Iteration: 10; Loss: 5.537.\n",
      "164m 13s (- 422m 41s) (1399 27%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "164m 20s (- 422m 34s) (1400 28%)\n",
      "Iteration: 10; Loss: 5.751.\n",
      "164m 27s (- 422m 27s) (1401 28%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "164m 34s (- 422m 20s) (1402 28%)\n",
      "Iteration: 10; Loss: 5.633.\n",
      "164m 41s (- 422m 13s) (1403 28%)\n",
      "Iteration: 10; Loss: 5.593.\n",
      "164m 48s (- 422m 6s) (1404 28%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "164m 55s (- 422m 0s) (1405 28%)\n",
      "Iteration: 10; Loss: 5.637.\n",
      "165m 2s (- 421m 53s) (1406 28%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "165m 10s (- 421m 46s) (1407 28%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "165m 17s (- 421m 39s) (1408 28%)\n",
      "Iteration: 10; Loss: 5.599.\n",
      "165m 24s (- 421m 32s) (1409 28%)\n",
      "Iteration: 10; Loss: 5.837.\n",
      "165m 31s (- 421m 25s) (1410 28%)\n",
      "Iteration: 10; Loss: 5.723.\n",
      "165m 38s (- 421m 18s) (1411 28%)\n",
      "Iteration: 10; Loss: 5.517.\n",
      "165m 45s (- 421m 11s) (1412 28%)\n",
      "Iteration: 10; Loss: 5.660.\n",
      "165m 52s (- 421m 4s) (1413 28%)\n",
      "Iteration: 10; Loss: 5.476.\n",
      "165m 59s (- 420m 57s) (1414 28%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "166m 6s (- 420m 50s) (1415 28%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "166m 13s (- 420m 43s) (1416 28%)\n",
      "Iteration: 10; Loss: 5.689.\n",
      "166m 20s (- 420m 36s) (1417 28%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "166m 27s (- 420m 28s) (1418 28%)\n",
      "Iteration: 10; Loss: 5.666.\n",
      "166m 34s (- 420m 21s) (1419 28%)\n",
      "Iteration: 10; Loss: 5.718.\n",
      "166m 41s (- 420m 14s) (1420 28%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "166m 48s (- 420m 7s) (1421 28%)\n",
      "Iteration: 10; Loss: 5.723.\n",
      "166m 55s (- 420m 0s) (1422 28%)\n",
      "Iteration: 10; Loss: 5.784.\n",
      "167m 2s (- 419m 53s) (1423 28%)\n",
      "Iteration: 10; Loss: 5.404.\n",
      "167m 9s (- 419m 46s) (1424 28%)\n",
      "Iteration: 10; Loss: 5.476.\n",
      "167m 16s (- 419m 39s) (1425 28%)\n",
      "Iteration: 10; Loss: 5.442.\n",
      "167m 23s (- 419m 32s) (1426 28%)\n",
      "Iteration: 10; Loss: 5.728.\n",
      "167m 30s (- 419m 25s) (1427 28%)\n",
      "Iteration: 10; Loss: 5.573.\n",
      "167m 37s (- 419m 18s) (1428 28%)\n",
      "Iteration: 10; Loss: 5.696.\n",
      "167m 44s (- 419m 11s) (1429 28%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "167m 51s (- 419m 4s) (1430 28%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "167m 58s (- 418m 57s) (1431 28%)\n",
      "Iteration: 10; Loss: 5.634.\n",
      "168m 5s (- 418m 50s) (1432 28%)\n",
      "Iteration: 10; Loss: 5.636.\n",
      "168m 12s (- 418m 42s) (1433 28%)\n",
      "Iteration: 10; Loss: 5.486.\n",
      "168m 19s (- 418m 36s) (1434 28%)\n",
      "Iteration: 10; Loss: 5.737.\n",
      "168m 26s (- 418m 28s) (1435 28%)\n",
      "Iteration: 10; Loss: 5.534.\n",
      "168m 34s (- 418m 21s) (1436 28%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "168m 41s (- 418m 14s) (1437 28%)\n",
      "Iteration: 10; Loss: 5.427.\n",
      "168m 48s (- 418m 7s) (1438 28%)\n",
      "Iteration: 10; Loss: 5.684.\n",
      "168m 55s (- 418m 0s) (1439 28%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "169m 2s (- 417m 53s) (1440 28%)\n",
      "Iteration: 10; Loss: 5.694.\n",
      "169m 9s (- 417m 46s) (1441 28%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "169m 16s (- 417m 39s) (1442 28%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "169m 23s (- 417m 32s) (1443 28%)\n",
      "Iteration: 10; Loss: 5.651.\n",
      "169m 30s (- 417m 25s) (1444 28%)\n",
      "Iteration: 10; Loss: 5.498.\n",
      "169m 37s (- 417m 18s) (1445 28%)\n",
      "Iteration: 10; Loss: 5.456.\n",
      "169m 44s (- 417m 11s) (1446 28%)\n",
      "Iteration: 10; Loss: 5.450.\n",
      "169m 51s (- 417m 4s) (1447 28%)\n",
      "Iteration: 10; Loss: 5.384.\n",
      "169m 58s (- 416m 57s) (1448 28%)\n",
      "Iteration: 10; Loss: 5.589.\n",
      "170m 5s (- 416m 50s) (1449 28%)\n",
      "Iteration: 10; Loss: 5.468.\n",
      "170m 12s (- 416m 43s) (1450 28%)\n",
      "Iteration: 10; Loss: 5.546.\n",
      "170m 19s (- 416m 36s) (1451 29%)\n",
      "Iteration: 10; Loss: 5.681.\n",
      "170m 26s (- 416m 29s) (1452 29%)\n",
      "Iteration: 10; Loss: 5.460.\n",
      "170m 33s (- 416m 22s) (1453 29%)\n",
      "Iteration: 10; Loss: 5.420.\n",
      "170m 40s (- 416m 15s) (1454 29%)\n",
      "Iteration: 10; Loss: 5.696.\n",
      "170m 47s (- 416m 8s) (1455 29%)\n",
      "Iteration: 10; Loss: 5.462.\n",
      "170m 55s (- 416m 1s) (1456 29%)\n",
      "Iteration: 10; Loss: 5.644.\n",
      "171m 2s (- 415m 54s) (1457 29%)\n",
      "Iteration: 10; Loss: 5.440.\n",
      "171m 9s (- 415m 47s) (1458 29%)\n",
      "Iteration: 10; Loss: 5.607.\n",
      "171m 16s (- 415m 40s) (1459 29%)\n",
      "Iteration: 10; Loss: 5.603.\n",
      "171m 23s (- 415m 33s) (1460 29%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "171m 30s (- 415m 26s) (1461 29%)\n",
      "Iteration: 10; Loss: 5.717.\n",
      "171m 37s (- 415m 19s) (1462 29%)\n",
      "Iteration: 10; Loss: 5.639.\n",
      "171m 44s (- 415m 12s) (1463 29%)\n",
      "Iteration: 10; Loss: 5.693.\n",
      "171m 51s (- 415m 5s) (1464 29%)\n",
      "Iteration: 10; Loss: 5.560.\n",
      "171m 58s (- 414m 58s) (1465 29%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "172m 5s (- 414m 50s) (1466 29%)\n",
      "Iteration: 10; Loss: 5.428.\n",
      "172m 12s (- 414m 43s) (1467 29%)\n",
      "Iteration: 10; Loss: 5.671.\n",
      "172m 19s (- 414m 36s) (1468 29%)\n",
      "Iteration: 10; Loss: 5.539.\n",
      "172m 26s (- 414m 29s) (1469 29%)\n",
      "Iteration: 10; Loss: 5.609.\n",
      "172m 33s (- 414m 22s) (1470 29%)\n",
      "Iteration: 10; Loss: 5.543.\n",
      "172m 40s (- 414m 15s) (1471 29%)\n",
      "Iteration: 10; Loss: 5.522.\n",
      "172m 47s (- 414m 8s) (1472 29%)\n",
      "Iteration: 10; Loss: 5.505.\n",
      "172m 54s (- 414m 1s) (1473 29%)\n",
      "Iteration: 10; Loss: 5.726.\n",
      "173m 1s (- 413m 54s) (1474 29%)\n",
      "Iteration: 10; Loss: 5.526.\n",
      "173m 8s (- 413m 47s) (1475 29%)\n",
      "Iteration: 10; Loss: 5.441.\n",
      "173m 15s (- 413m 40s) (1476 29%)\n",
      "Iteration: 10; Loss: 5.736.\n",
      "173m 22s (- 413m 33s) (1477 29%)\n",
      "Iteration: 10; Loss: 5.674.\n",
      "173m 29s (- 413m 26s) (1478 29%)\n",
      "Iteration: 10; Loss: 5.662.\n",
      "173m 36s (- 413m 19s) (1479 29%)\n",
      "Iteration: 10; Loss: 5.483.\n",
      "173m 43s (- 413m 12s) (1480 29%)\n",
      "Iteration: 10; Loss: 5.295.\n",
      "173m 51s (- 413m 5s) (1481 29%)\n",
      "Iteration: 10; Loss: 5.524.\n",
      "173m 58s (- 412m 59s) (1482 29%)\n",
      "Iteration: 10; Loss: 5.546.\n",
      "174m 5s (- 412m 52s) (1483 29%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "174m 12s (- 412m 45s) (1484 29%)\n",
      "Iteration: 10; Loss: 5.537.\n",
      "174m 19s (- 412m 38s) (1485 29%)\n",
      "Iteration: 10; Loss: 5.701.\n",
      "174m 27s (- 412m 31s) (1486 29%)\n",
      "Iteration: 10; Loss: 5.362.\n",
      "174m 34s (- 412m 24s) (1487 29%)\n",
      "Iteration: 10; Loss: 5.642.\n",
      "174m 41s (- 412m 17s) (1488 29%)\n",
      "Iteration: 10; Loss: 5.536.\n",
      "174m 48s (- 412m 10s) (1489 29%)\n",
      "Iteration: 10; Loss: 5.522.\n",
      "174m 55s (- 412m 3s) (1490 29%)\n",
      "Iteration: 10; Loss: 5.495.\n",
      "175m 2s (- 411m 56s) (1491 29%)\n",
      "Iteration: 10; Loss: 5.520.\n",
      "175m 9s (- 411m 49s) (1492 29%)\n",
      "Iteration: 10; Loss: 5.638.\n",
      "175m 16s (- 411m 42s) (1493 29%)\n",
      "Iteration: 10; Loss: 5.562.\n",
      "175m 23s (- 411m 35s) (1494 29%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "175m 30s (- 411m 28s) (1495 29%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "175m 37s (- 411m 21s) (1496 29%)\n",
      "Iteration: 10; Loss: 5.726.\n",
      "175m 44s (- 411m 14s) (1497 29%)\n",
      "Iteration: 10; Loss: 5.624.\n",
      "175m 51s (- 411m 7s) (1498 29%)\n",
      "Iteration: 10; Loss: 5.505.\n",
      "175m 58s (- 411m 0s) (1499 29%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "176m 5s (- 410m 53s) (1500 30%)\n",
      "Iteration: 10; Loss: 5.513.\n",
      "176m 12s (- 410m 46s) (1501 30%)\n",
      "Iteration: 10; Loss: 5.585.\n",
      "176m 19s (- 410m 39s) (1502 30%)\n",
      "Iteration: 10; Loss: 5.427.\n",
      "176m 26s (- 410m 32s) (1503 30%)\n",
      "Iteration: 10; Loss: 5.673.\n",
      "176m 34s (- 410m 25s) (1504 30%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "176m 41s (- 410m 18s) (1505 30%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "176m 48s (- 410m 11s) (1506 30%)\n",
      "Iteration: 10; Loss: 5.545.\n",
      "176m 55s (- 410m 4s) (1507 30%)\n",
      "Iteration: 10; Loss: 5.578.\n",
      "177m 2s (- 409m 57s) (1508 30%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "177m 9s (- 409m 50s) (1509 30%)\n",
      "Iteration: 10; Loss: 5.700.\n",
      "177m 16s (- 409m 43s) (1510 30%)\n",
      "Iteration: 10; Loss: 5.545.\n",
      "177m 23s (- 409m 36s) (1511 30%)\n",
      "Iteration: 10; Loss: 5.623.\n",
      "177m 30s (- 409m 29s) (1512 30%)\n",
      "Iteration: 10; Loss: 5.358.\n",
      "177m 37s (- 409m 22s) (1513 30%)\n",
      "Iteration: 10; Loss: 5.553.\n",
      "177m 44s (- 409m 15s) (1514 30%)\n",
      "Iteration: 10; Loss: 5.563.\n",
      "177m 51s (- 409m 8s) (1515 30%)\n",
      "Iteration: 10; Loss: 5.614.\n",
      "177m 58s (- 409m 1s) (1516 30%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "178m 5s (- 408m 54s) (1517 30%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "178m 12s (- 408m 47s) (1518 30%)\n",
      "Iteration: 10; Loss: 5.449.\n",
      "178m 19s (- 408m 40s) (1519 30%)\n",
      "Iteration: 10; Loss: 5.628.\n",
      "178m 26s (- 408m 33s) (1520 30%)\n",
      "Iteration: 10; Loss: 5.527.\n",
      "178m 33s (- 408m 25s) (1521 30%)\n",
      "Iteration: 10; Loss: 5.661.\n",
      "178m 40s (- 408m 18s) (1522 30%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "178m 47s (- 408m 11s) (1523 30%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "178m 55s (- 408m 4s) (1524 30%)\n",
      "Iteration: 10; Loss: 5.569.\n",
      "179m 2s (- 407m 57s) (1525 30%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "179m 9s (- 407m 50s) (1526 30%)\n",
      "Iteration: 10; Loss: 5.416.\n",
      "179m 16s (- 407m 43s) (1527 30%)\n",
      "Iteration: 10; Loss: 5.542.\n",
      "179m 23s (- 407m 36s) (1528 30%)\n",
      "Iteration: 10; Loss: 5.622.\n",
      "179m 30s (- 407m 29s) (1529 30%)\n",
      "Iteration: 10; Loss: 5.506.\n",
      "179m 37s (- 407m 22s) (1530 30%)\n",
      "Iteration: 10; Loss: 5.489.\n",
      "179m 44s (- 407m 15s) (1531 30%)\n",
      "Iteration: 10; Loss: 5.661.\n",
      "179m 51s (- 407m 8s) (1532 30%)\n",
      "Iteration: 10; Loss: 5.333.\n",
      "179m 58s (- 407m 1s) (1533 30%)\n",
      "Iteration: 10; Loss: 5.662.\n",
      "180m 5s (- 406m 54s) (1534 30%)\n",
      "Iteration: 10; Loss: 5.503.\n",
      "180m 12s (- 406m 47s) (1535 30%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "180m 19s (- 406m 40s) (1536 30%)\n",
      "Iteration: 10; Loss: 5.409.\n",
      "180m 26s (- 406m 33s) (1537 30%)\n",
      "Iteration: 10; Loss: 5.446.\n",
      "180m 33s (- 406m 26s) (1538 30%)\n",
      "Iteration: 10; Loss: 5.722.\n",
      "180m 40s (- 406m 19s) (1539 30%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "180m 48s (- 406m 12s) (1540 30%)\n",
      "Iteration: 10; Loss: 5.472.\n",
      "180m 55s (- 406m 5s) (1541 30%)\n",
      "Iteration: 10; Loss: 5.479.\n",
      "181m 2s (- 405m 58s) (1542 30%)\n",
      "Iteration: 10; Loss: 5.516.\n",
      "181m 9s (- 405m 51s) (1543 30%)\n",
      "Iteration: 10; Loss: 5.415.\n",
      "181m 16s (- 405m 44s) (1544 30%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "181m 23s (- 405m 37s) (1545 30%)\n",
      "Iteration: 10; Loss: 5.721.\n",
      "181m 30s (- 405m 30s) (1546 30%)\n",
      "Iteration: 10; Loss: 5.630.\n",
      "181m 37s (- 405m 23s) (1547 30%)\n",
      "Iteration: 10; Loss: 5.658.\n",
      "181m 44s (- 405m 16s) (1548 30%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "181m 51s (- 405m 9s) (1549 30%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "181m 58s (- 405m 2s) (1550 31%)\n",
      "Iteration: 10; Loss: 5.478.\n",
      "182m 5s (- 404m 55s) (1551 31%)\n",
      "Iteration: 10; Loss: 5.655.\n",
      "182m 12s (- 404m 48s) (1552 31%)\n",
      "Iteration: 10; Loss: 5.675.\n",
      "182m 19s (- 404m 41s) (1553 31%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "182m 26s (- 404m 34s) (1554 31%)\n",
      "Iteration: 10; Loss: 5.615.\n",
      "182m 33s (- 404m 26s) (1555 31%)\n",
      "Iteration: 10; Loss: 5.593.\n",
      "182m 40s (- 404m 19s) (1556 31%)\n",
      "Iteration: 10; Loss: 5.560.\n",
      "182m 47s (- 404m 12s) (1557 31%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "182m 54s (- 404m 5s) (1558 31%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "183m 1s (- 403m 58s) (1559 31%)\n",
      "Iteration: 10; Loss: 5.634.\n",
      "183m 8s (- 403m 51s) (1560 31%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "183m 15s (- 403m 44s) (1561 31%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "183m 22s (- 403m 37s) (1562 31%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "183m 29s (- 403m 30s) (1563 31%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "183m 36s (- 403m 23s) (1564 31%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "183m 43s (- 403m 16s) (1565 31%)\n",
      "Iteration: 10; Loss: 5.548.\n",
      "183m 50s (- 403m 9s) (1566 31%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "183m 57s (- 403m 2s) (1567 31%)\n",
      "Iteration: 10; Loss: 5.608.\n",
      "184m 5s (- 402m 55s) (1568 31%)\n",
      "Iteration: 10; Loss: 5.509.\n",
      "184m 12s (- 402m 48s) (1569 31%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "184m 19s (- 402m 41s) (1570 31%)\n",
      "Iteration: 10; Loss: 5.489.\n",
      "184m 26s (- 402m 33s) (1571 31%)\n",
      "Iteration: 10; Loss: 5.457.\n",
      "184m 33s (- 402m 26s) (1572 31%)\n",
      "Iteration: 10; Loss: 5.744.\n",
      "184m 40s (- 402m 19s) (1573 31%)\n",
      "Iteration: 10; Loss: 5.574.\n",
      "184m 47s (- 402m 12s) (1574 31%)\n",
      "Iteration: 10; Loss: 5.640.\n",
      "184m 54s (- 402m 5s) (1575 31%)\n",
      "Iteration: 10; Loss: 5.616.\n",
      "185m 1s (- 401m 58s) (1576 31%)\n",
      "Iteration: 10; Loss: 5.493.\n",
      "185m 8s (- 401m 51s) (1577 31%)\n",
      "Iteration: 10; Loss: 5.498.\n",
      "185m 15s (- 401m 44s) (1578 31%)\n",
      "Iteration: 10; Loss: 5.888.\n",
      "185m 22s (- 401m 37s) (1579 31%)\n",
      "Iteration: 10; Loss: 5.657.\n",
      "185m 29s (- 401m 30s) (1580 31%)\n",
      "Iteration: 10; Loss: 5.469.\n",
      "185m 36s (- 401m 23s) (1581 31%)\n",
      "Iteration: 10; Loss: 5.506.\n",
      "185m 43s (- 401m 16s) (1582 31%)\n",
      "Iteration: 10; Loss: 5.578.\n",
      "185m 50s (- 401m 9s) (1583 31%)\n",
      "Iteration: 10; Loss: 5.706.\n",
      "185m 57s (- 401m 2s) (1584 31%)\n",
      "Iteration: 10; Loss: 5.518.\n",
      "186m 4s (- 400m 55s) (1585 31%)\n",
      "Iteration: 10; Loss: 5.667.\n",
      "186m 11s (- 400m 48s) (1586 31%)\n",
      "Iteration: 10; Loss: 5.404.\n",
      "186m 18s (- 400m 40s) (1587 31%)\n",
      "Iteration: 10; Loss: 5.794.\n",
      "186m 25s (- 400m 33s) (1588 31%)\n",
      "Iteration: 10; Loss: 5.591.\n",
      "186m 32s (- 400m 26s) (1589 31%)\n",
      "Iteration: 10; Loss: 5.494.\n",
      "186m 39s (- 400m 19s) (1590 31%)\n",
      "Iteration: 10; Loss: 5.390.\n",
      "186m 46s (- 400m 12s) (1591 31%)\n",
      "Iteration: 10; Loss: 5.508.\n",
      "186m 53s (- 400m 5s) (1592 31%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "187m 0s (- 399m 58s) (1593 31%)\n",
      "Iteration: 10; Loss: 5.551.\n",
      "187m 7s (- 399m 51s) (1594 31%)\n",
      "Iteration: 10; Loss: 5.639.\n",
      "187m 15s (- 399m 44s) (1595 31%)\n",
      "Iteration: 10; Loss: 5.537.\n",
      "187m 22s (- 399m 37s) (1596 31%)\n",
      "Iteration: 10; Loss: 5.503.\n",
      "187m 29s (- 399m 30s) (1597 31%)\n",
      "Iteration: 10; Loss: 5.659.\n",
      "187m 36s (- 399m 23s) (1598 31%)\n",
      "Iteration: 10; Loss: 5.636.\n",
      "187m 43s (- 399m 16s) (1599 31%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "187m 50s (- 399m 9s) (1600 32%)\n",
      "Iteration: 10; Loss: 5.494.\n",
      "187m 57s (- 399m 2s) (1601 32%)\n",
      "Iteration: 10; Loss: 5.573.\n",
      "188m 4s (- 398m 55s) (1602 32%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "188m 11s (- 398m 48s) (1603 32%)\n",
      "Iteration: 10; Loss: 5.497.\n",
      "188m 18s (- 398m 41s) (1604 32%)\n",
      "Iteration: 10; Loss: 5.670.\n",
      "188m 25s (- 398m 34s) (1605 32%)\n",
      "Iteration: 10; Loss: 5.633.\n",
      "188m 32s (- 398m 27s) (1606 32%)\n",
      "Iteration: 10; Loss: 5.603.\n",
      "188m 39s (- 398m 20s) (1607 32%)\n",
      "Iteration: 10; Loss: 5.560.\n",
      "188m 46s (- 398m 12s) (1608 32%)\n",
      "Iteration: 10; Loss: 5.529.\n",
      "188m 53s (- 398m 5s) (1609 32%)\n",
      "Iteration: 10; Loss: 5.590.\n",
      "189m 0s (- 397m 58s) (1610 32%)\n",
      "Iteration: 10; Loss: 5.702.\n",
      "189m 7s (- 397m 51s) (1611 32%)\n",
      "Iteration: 10; Loss: 5.614.\n",
      "189m 14s (- 397m 44s) (1612 32%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "189m 21s (- 397m 37s) (1613 32%)\n",
      "Iteration: 10; Loss: 5.815.\n",
      "189m 28s (- 397m 30s) (1614 32%)\n",
      "Iteration: 10; Loss: 5.503.\n",
      "189m 35s (- 397m 23s) (1615 32%)\n",
      "Iteration: 10; Loss: 5.500.\n",
      "189m 42s (- 397m 16s) (1616 32%)\n",
      "Iteration: 10; Loss: 5.496.\n",
      "189m 49s (- 397m 9s) (1617 32%)\n",
      "Iteration: 10; Loss: 5.635.\n",
      "189m 57s (- 397m 2s) (1618 32%)\n",
      "Iteration: 10; Loss: 5.757.\n",
      "190m 4s (- 396m 55s) (1619 32%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "190m 11s (- 396m 48s) (1620 32%)\n",
      "Iteration: 10; Loss: 5.507.\n",
      "190m 18s (- 396m 41s) (1621 32%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "190m 25s (- 396m 34s) (1622 32%)\n",
      "Iteration: 10; Loss: 5.517.\n",
      "190m 32s (- 396m 27s) (1623 32%)\n",
      "Iteration: 10; Loss: 5.480.\n",
      "190m 39s (- 396m 20s) (1624 32%)\n",
      "Iteration: 10; Loss: 5.659.\n",
      "190m 46s (- 396m 13s) (1625 32%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "190m 53s (- 396m 6s) (1626 32%)\n",
      "Iteration: 10; Loss: 5.556.\n",
      "191m 0s (- 395m 59s) (1627 32%)\n",
      "Iteration: 10; Loss: 5.668.\n",
      "191m 7s (- 395m 51s) (1628 32%)\n",
      "Iteration: 10; Loss: 5.536.\n",
      "191m 14s (- 395m 44s) (1629 32%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "191m 21s (- 395m 37s) (1630 32%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "191m 28s (- 395m 30s) (1631 32%)\n",
      "Iteration: 10; Loss: 5.654.\n",
      "191m 35s (- 395m 23s) (1632 32%)\n",
      "Iteration: 10; Loss: 5.756.\n",
      "191m 42s (- 395m 16s) (1633 32%)\n",
      "Iteration: 10; Loss: 5.324.\n",
      "191m 49s (- 395m 9s) (1634 32%)\n",
      "Iteration: 10; Loss: 5.410.\n",
      "191m 56s (- 395m 2s) (1635 32%)\n",
      "Iteration: 10; Loss: 5.577.\n",
      "192m 3s (- 394m 55s) (1636 32%)\n",
      "Iteration: 10; Loss: 5.676.\n",
      "192m 10s (- 394m 48s) (1637 32%)\n",
      "Iteration: 10; Loss: 5.551.\n",
      "192m 17s (- 394m 41s) (1638 32%)\n",
      "Iteration: 10; Loss: 5.540.\n",
      "192m 24s (- 394m 34s) (1639 32%)\n",
      "Iteration: 10; Loss: 5.474.\n",
      "192m 31s (- 394m 27s) (1640 32%)\n",
      "Iteration: 10; Loss: 5.607.\n",
      "192m 38s (- 394m 19s) (1641 32%)\n",
      "Iteration: 10; Loss: 5.581.\n",
      "192m 45s (- 394m 12s) (1642 32%)\n",
      "Iteration: 10; Loss: 5.579.\n",
      "192m 52s (- 394m 5s) (1643 32%)\n",
      "Iteration: 10; Loss: 5.601.\n",
      "192m 59s (- 393m 58s) (1644 32%)\n",
      "Iteration: 10; Loss: 5.466.\n",
      "193m 6s (- 393m 51s) (1645 32%)\n",
      "Iteration: 10; Loss: 5.648.\n",
      "193m 14s (- 393m 44s) (1646 32%)\n",
      "Iteration: 10; Loss: 5.521.\n",
      "193m 21s (- 393m 37s) (1647 32%)\n",
      "Iteration: 10; Loss: 5.504.\n",
      "193m 28s (- 393m 30s) (1648 32%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "193m 35s (- 393m 23s) (1649 32%)\n",
      "Iteration: 10; Loss: 5.641.\n",
      "193m 42s (- 393m 16s) (1650 33%)\n",
      "Iteration: 10; Loss: 5.707.\n",
      "193m 49s (- 393m 9s) (1651 33%)\n",
      "Iteration: 10; Loss: 5.374.\n",
      "193m 56s (- 393m 2s) (1652 33%)\n",
      "Iteration: 10; Loss: 5.652.\n",
      "194m 3s (- 392m 55s) (1653 33%)\n",
      "Iteration: 10; Loss: 5.549.\n",
      "194m 10s (- 392m 48s) (1654 33%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "194m 17s (- 392m 41s) (1655 33%)\n",
      "Iteration: 10; Loss: 5.654.\n",
      "194m 24s (- 392m 34s) (1656 33%)\n",
      "Iteration: 10; Loss: 5.592.\n",
      "194m 31s (- 392m 27s) (1657 33%)\n",
      "Iteration: 10; Loss: 5.656.\n",
      "194m 38s (- 392m 20s) (1658 33%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "194m 45s (- 392m 13s) (1659 33%)\n",
      "Iteration: 10; Loss: 5.615.\n",
      "194m 52s (- 392m 6s) (1660 33%)\n",
      "Iteration: 10; Loss: 5.491.\n",
      "194m 59s (- 391m 59s) (1661 33%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "195m 7s (- 391m 52s) (1662 33%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "195m 14s (- 391m 45s) (1663 33%)\n",
      "Iteration: 10; Loss: 5.517.\n",
      "195m 21s (- 391m 38s) (1664 33%)\n",
      "Iteration: 10; Loss: 5.515.\n",
      "195m 28s (- 391m 31s) (1665 33%)\n",
      "Iteration: 10; Loss: 5.600.\n",
      "195m 35s (- 391m 24s) (1666 33%)\n",
      "Iteration: 10; Loss: 5.679.\n",
      "195m 42s (- 391m 17s) (1667 33%)\n",
      "Iteration: 10; Loss: 5.687.\n",
      "195m 49s (- 391m 10s) (1668 33%)\n",
      "Iteration: 10; Loss: 5.562.\n",
      "195m 56s (- 391m 3s) (1669 33%)\n",
      "Iteration: 10; Loss: 5.477.\n",
      "196m 3s (- 390m 56s) (1670 33%)\n",
      "Iteration: 10; Loss: 5.512.\n",
      "196m 10s (- 390m 49s) (1671 33%)\n",
      "Iteration: 10; Loss: 5.405.\n",
      "196m 17s (- 390m 41s) (1672 33%)\n",
      "Iteration: 10; Loss: 5.591.\n",
      "196m 24s (- 390m 34s) (1673 33%)\n",
      "Iteration: 10; Loss: 5.404.\n",
      "196m 31s (- 390m 27s) (1674 33%)\n",
      "Iteration: 10; Loss: 5.651.\n",
      "196m 38s (- 390m 20s) (1675 33%)\n",
      "Iteration: 10; Loss: 5.622.\n",
      "196m 45s (- 390m 13s) (1676 33%)\n",
      "Iteration: 10; Loss: 5.502.\n",
      "196m 52s (- 390m 6s) (1677 33%)\n",
      "Iteration: 10; Loss: 5.664.\n",
      "196m 59s (- 389m 59s) (1678 33%)\n",
      "Iteration: 10; Loss: 5.476.\n",
      "197m 6s (- 389m 52s) (1679 33%)\n",
      "Iteration: 10; Loss: 5.554.\n",
      "197m 13s (- 389m 45s) (1680 33%)\n",
      "Iteration: 10; Loss: 5.531.\n",
      "197m 20s (- 389m 38s) (1681 33%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "197m 27s (- 389m 31s) (1682 33%)\n",
      "Iteration: 10; Loss: 5.632.\n",
      "197m 34s (- 389m 24s) (1683 33%)\n",
      "Iteration: 10; Loss: 5.489.\n",
      "197m 41s (- 389m 17s) (1684 33%)\n",
      "Iteration: 10; Loss: 5.717.\n",
      "197m 48s (- 389m 10s) (1685 33%)\n",
      "Iteration: 10; Loss: 5.676.\n",
      "197m 55s (- 389m 2s) (1686 33%)\n",
      "Iteration: 10; Loss: 5.616.\n",
      "198m 2s (- 388m 55s) (1687 33%)\n",
      "Iteration: 10; Loss: 5.500.\n",
      "198m 9s (- 388m 48s) (1688 33%)\n",
      "Iteration: 10; Loss: 5.516.\n",
      "198m 16s (- 388m 41s) (1689 33%)\n",
      "Iteration: 10; Loss: 5.830.\n",
      "198m 23s (- 388m 34s) (1690 33%)\n",
      "Iteration: 10; Loss: 5.559.\n",
      "198m 30s (- 388m 27s) (1691 33%)\n",
      "Iteration: 10; Loss: 5.591.\n",
      "198m 37s (- 388m 20s) (1692 33%)\n",
      "Iteration: 10; Loss: 5.525.\n",
      "198m 45s (- 388m 13s) (1693 33%)\n",
      "Iteration: 10; Loss: 5.526.\n",
      "198m 52s (- 388m 6s) (1694 33%)\n",
      "Iteration: 10; Loss: 5.562.\n",
      "198m 59s (- 387m 59s) (1695 33%)\n",
      "Iteration: 10; Loss: 5.573.\n",
      "199m 6s (- 387m 52s) (1696 33%)\n",
      "Iteration: 10; Loss: 5.381.\n",
      "199m 13s (- 387m 45s) (1697 33%)\n",
      "Iteration: 10; Loss: 5.611.\n",
      "199m 20s (- 387m 38s) (1698 33%)\n",
      "Iteration: 10; Loss: 5.681.\n",
      "199m 27s (- 387m 31s) (1699 33%)\n",
      "Iteration: 10; Loss: 5.527.\n",
      "199m 34s (- 387m 24s) (1700 34%)\n",
      "Iteration: 10; Loss: 5.631.\n",
      "199m 41s (- 387m 17s) (1701 34%)\n",
      "Iteration: 10; Loss: 5.517.\n",
      "199m 48s (- 387m 10s) (1702 34%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "199m 55s (- 387m 3s) (1703 34%)\n",
      "Iteration: 10; Loss: 5.479.\n",
      "200m 2s (- 386m 56s) (1704 34%)\n",
      "Iteration: 10; Loss: 5.617.\n",
      "200m 9s (- 386m 49s) (1705 34%)\n",
      "Iteration: 10; Loss: 5.372.\n",
      "200m 16s (- 386m 42s) (1706 34%)\n",
      "Iteration: 10; Loss: 5.536.\n",
      "200m 23s (- 386m 35s) (1707 34%)\n",
      "Iteration: 10; Loss: 5.522.\n",
      "200m 30s (- 386m 28s) (1708 34%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "200m 37s (- 386m 21s) (1709 34%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "200m 44s (- 386m 14s) (1710 34%)\n",
      "Iteration: 10; Loss: 5.628.\n",
      "200m 52s (- 386m 7s) (1711 34%)\n",
      "Iteration: 10; Loss: 5.607.\n",
      "200m 59s (- 386m 0s) (1712 34%)\n",
      "Iteration: 10; Loss: 5.649.\n",
      "201m 6s (- 385m 53s) (1713 34%)\n",
      "Iteration: 10; Loss: 5.434.\n",
      "201m 13s (- 385m 46s) (1714 34%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "201m 20s (- 385m 39s) (1715 34%)\n",
      "Iteration: 10; Loss: 5.669.\n",
      "201m 27s (- 385m 31s) (1716 34%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "201m 34s (- 385m 24s) (1717 34%)\n",
      "Iteration: 10; Loss: 5.579.\n",
      "201m 41s (- 385m 17s) (1718 34%)\n",
      "Iteration: 10; Loss: 5.551.\n",
      "201m 48s (- 385m 10s) (1719 34%)\n",
      "Iteration: 10; Loss: 5.484.\n",
      "201m 55s (- 385m 3s) (1720 34%)\n",
      "Iteration: 10; Loss: 5.727.\n",
      "202m 2s (- 384m 56s) (1721 34%)\n",
      "Iteration: 10; Loss: 5.615.\n",
      "202m 9s (- 384m 49s) (1722 34%)\n",
      "Iteration: 10; Loss: 5.637.\n",
      "202m 16s (- 384m 42s) (1723 34%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "202m 23s (- 384m 35s) (1724 34%)\n",
      "Iteration: 10; Loss: 5.460.\n",
      "202m 30s (- 384m 28s) (1725 34%)\n",
      "Iteration: 10; Loss: 5.476.\n",
      "202m 37s (- 384m 21s) (1726 34%)\n",
      "Iteration: 10; Loss: 5.647.\n",
      "202m 44s (- 384m 14s) (1727 34%)\n",
      "Iteration: 10; Loss: 5.564.\n",
      "202m 51s (- 384m 7s) (1728 34%)\n",
      "Iteration: 10; Loss: 5.312.\n",
      "202m 58s (- 384m 0s) (1729 34%)\n",
      "Iteration: 10; Loss: 5.364.\n",
      "203m 5s (- 383m 53s) (1730 34%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "203m 12s (- 383m 46s) (1731 34%)\n",
      "Iteration: 10; Loss: 5.730.\n",
      "203m 19s (- 383m 39s) (1732 34%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "203m 26s (- 383m 32s) (1733 34%)\n",
      "Iteration: 10; Loss: 5.551.\n",
      "203m 34s (- 383m 25s) (1734 34%)\n",
      "Iteration: 10; Loss: 5.691.\n",
      "203m 41s (- 383m 18s) (1735 34%)\n",
      "Iteration: 10; Loss: 5.547.\n",
      "203m 48s (- 383m 11s) (1736 34%)\n",
      "Iteration: 10; Loss: 5.808.\n",
      "203m 55s (- 383m 4s) (1737 34%)\n",
      "Iteration: 10; Loss: 5.350.\n",
      "204m 2s (- 382m 57s) (1738 34%)\n",
      "Iteration: 10; Loss: 5.478.\n",
      "204m 9s (- 382m 50s) (1739 34%)\n",
      "Iteration: 10; Loss: 5.462.\n",
      "204m 16s (- 382m 42s) (1740 34%)\n",
      "Iteration: 10; Loss: 5.658.\n",
      "204m 23s (- 382m 35s) (1741 34%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "204m 30s (- 382m 28s) (1742 34%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "204m 37s (- 382m 21s) (1743 34%)\n",
      "Iteration: 10; Loss: 5.613.\n",
      "204m 44s (- 382m 14s) (1744 34%)\n",
      "Iteration: 10; Loss: 5.602.\n",
      "204m 51s (- 382m 7s) (1745 34%)\n",
      "Iteration: 10; Loss: 5.744.\n",
      "204m 58s (- 382m 0s) (1746 34%)\n",
      "Iteration: 10; Loss: 5.520.\n",
      "205m 5s (- 381m 53s) (1747 34%)\n",
      "Iteration: 10; Loss: 5.596.\n",
      "205m 12s (- 381m 46s) (1748 34%)\n",
      "Iteration: 10; Loss: 5.551.\n",
      "205m 19s (- 381m 39s) (1749 34%)\n",
      "Iteration: 10; Loss: 5.599.\n",
      "205m 26s (- 381m 32s) (1750 35%)\n",
      "Iteration: 10; Loss: 5.571.\n",
      "205m 33s (- 381m 25s) (1751 35%)\n",
      "Iteration: 10; Loss: 5.595.\n",
      "205m 40s (- 381m 18s) (1752 35%)\n",
      "Iteration: 10; Loss: 5.629.\n",
      "205m 47s (- 381m 11s) (1753 35%)\n",
      "Iteration: 10; Loss: 5.591.\n",
      "205m 54s (- 381m 4s) (1754 35%)\n",
      "Iteration: 10; Loss: 5.507.\n",
      "206m 1s (- 380m 57s) (1755 35%)\n",
      "Iteration: 10; Loss: 5.725.\n",
      "206m 8s (- 380m 50s) (1756 35%)\n",
      "Iteration: 10; Loss: 5.720.\n",
      "206m 16s (- 380m 43s) (1757 35%)\n",
      "Iteration: 10; Loss: 5.584.\n",
      "206m 23s (- 380m 36s) (1758 35%)\n",
      "Iteration: 10; Loss: 5.622.\n",
      "206m 30s (- 380m 29s) (1759 35%)\n",
      "Iteration: 10; Loss: 5.675.\n",
      "206m 37s (- 380m 22s) (1760 35%)\n",
      "Iteration: 10; Loss: 5.732.\n",
      "206m 44s (- 380m 15s) (1761 35%)\n",
      "Iteration: 10; Loss: 5.594.\n",
      "206m 51s (- 380m 7s) (1762 35%)\n",
      "Iteration: 10; Loss: 5.535.\n",
      "206m 58s (- 380m 0s) (1763 35%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "207m 5s (- 379m 53s) (1764 35%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "207m 12s (- 379m 46s) (1765 35%)\n",
      "Iteration: 10; Loss: 5.498.\n",
      "207m 19s (- 379m 39s) (1766 35%)\n",
      "Iteration: 10; Loss: 5.439.\n",
      "207m 26s (- 379m 32s) (1767 35%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "207m 33s (- 379m 25s) (1768 35%)\n",
      "Iteration: 10; Loss: 5.408.\n",
      "207m 40s (- 379m 18s) (1769 35%)\n",
      "Iteration: 10; Loss: 5.645.\n",
      "207m 47s (- 379m 11s) (1770 35%)\n",
      "Iteration: 10; Loss: 5.619.\n",
      "207m 54s (- 379m 4s) (1771 35%)\n",
      "Iteration: 10; Loss: 5.618.\n",
      "208m 1s (- 378m 57s) (1772 35%)\n",
      "Iteration: 10; Loss: 5.543.\n",
      "208m 8s (- 378m 50s) (1773 35%)\n",
      "Iteration: 10; Loss: 5.430.\n",
      "208m 15s (- 378m 43s) (1774 35%)\n",
      "Iteration: 10; Loss: 5.370.\n",
      "208m 22s (- 378m 36s) (1775 35%)\n",
      "Iteration: 10; Loss: 5.627.\n",
      "208m 29s (- 378m 29s) (1776 35%)\n",
      "Iteration: 10; Loss: 5.570.\n",
      "208m 36s (- 378m 22s) (1777 35%)\n",
      "Iteration: 10; Loss: 5.440.\n",
      "208m 43s (- 378m 15s) (1778 35%)\n",
      "Iteration: 10; Loss: 5.356.\n",
      "208m 50s (- 378m 8s) (1779 35%)\n",
      "Iteration: 10; Loss: 5.597.\n",
      "208m 58s (- 378m 1s) (1780 35%)\n",
      "Iteration: 10; Loss: 5.731.\n",
      "209m 5s (- 377m 54s) (1781 35%)\n",
      "Iteration: 10; Loss: 5.374.\n",
      "209m 12s (- 377m 47s) (1782 35%)\n",
      "Iteration: 10; Loss: 5.762.\n",
      "209m 19s (- 377m 39s) (1783 35%)\n",
      "Iteration: 10; Loss: 5.515.\n",
      "209m 26s (- 377m 32s) (1784 35%)\n",
      "Iteration: 10; Loss: 5.681.\n",
      "209m 33s (- 377m 25s) (1785 35%)\n",
      "Iteration: 10; Loss: 5.350.\n",
      "209m 40s (- 377m 18s) (1786 35%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "209m 47s (- 377m 11s) (1787 35%)\n",
      "Iteration: 10; Loss: 5.612.\n",
      "209m 54s (- 377m 4s) (1788 35%)\n",
      "Iteration: 10; Loss: 5.692.\n",
      "210m 1s (- 376m 57s) (1789 35%)\n",
      "Iteration: 10; Loss: 5.662.\n",
      "210m 8s (- 376m 50s) (1790 35%)\n",
      "Iteration: 10; Loss: 5.598.\n",
      "210m 15s (- 376m 43s) (1791 35%)\n",
      "Iteration: 10; Loss: 5.522.\n",
      "210m 22s (- 376m 36s) (1792 35%)\n",
      "Iteration: 10; Loss: 5.583.\n",
      "210m 29s (- 376m 29s) (1793 35%)\n",
      "Iteration: 10; Loss: 5.545.\n",
      "210m 36s (- 376m 22s) (1794 35%)\n",
      "Iteration: 10; Loss: 5.728.\n",
      "210m 43s (- 376m 15s) (1795 35%)\n",
      "Iteration: 10; Loss: 5.608.\n",
      "210m 50s (- 376m 8s) (1796 35%)\n",
      "Iteration: 10; Loss: 5.587.\n",
      "210m 57s (- 376m 1s) (1797 35%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "211m 4s (- 375m 54s) (1798 35%)\n",
      "Iteration: 10; Loss: 5.562.\n",
      "211m 11s (- 375m 46s) (1799 35%)\n",
      "Iteration: 10; Loss: 5.609.\n",
      "211m 18s (- 375m 39s) (1800 36%)\n",
      "Iteration: 10; Loss: 5.557.\n",
      "211m 25s (- 375m 32s) (1801 36%)\n",
      "Iteration: 10; Loss: 5.489.\n",
      "211m 32s (- 375m 25s) (1802 36%)\n",
      "Iteration: 10; Loss: 5.683.\n",
      "211m 39s (- 375m 18s) (1803 36%)\n",
      "Iteration: 10; Loss: 5.532.\n",
      "211m 46s (- 375m 11s) (1804 36%)\n",
      "Iteration: 10; Loss: 5.552.\n",
      "211m 53s (- 375m 4s) (1805 36%)\n",
      "Iteration: 10; Loss: 5.519.\n",
      "212m 1s (- 374m 57s) (1806 36%)\n",
      "Iteration: 10; Loss: 5.510.\n",
      "212m 8s (- 374m 50s) (1807 36%)\n",
      "Iteration: 10; Loss: 5.655.\n",
      "212m 15s (- 374m 43s) (1808 36%)\n",
      "Iteration: 10; Loss: 5.458.\n",
      "212m 22s (- 374m 36s) (1809 36%)\n",
      "Iteration: 10; Loss: 5.561.\n",
      "212m 29s (- 374m 29s) (1810 36%)\n",
      "Iteration: 10; Loss: 5.541.\n",
      "212m 36s (- 374m 22s) (1811 36%)\n",
      "Iteration: 10; Loss: 5.523.\n",
      "212m 43s (- 374m 15s) (1812 36%)\n",
      "Iteration: 10; Loss: 5.604.\n",
      "212m 50s (- 374m 8s) (1813 36%)\n",
      "Iteration: 10; Loss: 5.572.\n",
      "212m 57s (- 374m 1s) (1814 36%)\n",
      "Iteration: 10; Loss: 5.637.\n",
      "213m 4s (- 373m 54s) (1815 36%)\n",
      "Iteration: 10; Loss: 5.580.\n",
      "213m 11s (- 373m 47s) (1816 36%)\n",
      "Iteration: 10; Loss: 5.410.\n",
      "213m 18s (- 373m 40s) (1817 36%)\n",
      "Iteration: 10; Loss: 5.685.\n",
      "213m 25s (- 373m 33s) (1818 36%)\n",
      "Iteration: 10; Loss: 5.457.\n",
      "213m 32s (- 373m 26s) (1819 36%)\n",
      "Iteration: 10; Loss: 5.359.\n",
      "213m 39s (- 373m 18s) (1820 36%)\n",
      "Iteration: 10; Loss: 5.718.\n",
      "213m 46s (- 373m 11s) (1821 36%)\n",
      "Iteration: 10; Loss: 5.522.\n",
      "213m 53s (- 373m 4s) (1822 36%)\n",
      "Iteration: 10; Loss: 5.518.\n",
      "214m 0s (- 372m 57s) (1823 36%)\n",
      "Iteration: 10; Loss: 5.626.\n",
      "214m 7s (- 372m 50s) (1824 36%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-5edb38bcb02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtraining_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_an_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATALOADERS_DICT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS_FN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOG_INTERVAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   print('%s (%d %d%%)' % (time_since(start, epoch / EPOCHS),\n\u001b[1;32m     10\u001b[0m                                          epoch, epoch / EPOCHS * 100))\n",
      "\u001b[0;32m<ipython-input-95-d5813801b4b1>\u001b[0m in \u001b[0;36mtrain_an_epoch\u001b[0;34m(model, dataloader, optimizer, optimizer2, loss_function, log_interval, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5000\n",
    "validation_scores = []\n",
    "training_scores = []\n",
    "minimum_score = np.inf\n",
    "best_model = None\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "  training_score = train_an_epoch(seq2seq, DATALOADERS_DICT['train_data'], OPTIMIZER, OPTIMIZER2, LOSS_FN, LOG_INTERVAL, DEVICE)\n",
    "  print('%s (%d %d%%)' % (time_since(start, epoch / EPOCHS),\n",
    "                                         epoch, epoch / EPOCHS * 100))\n",
    "  if SINGLE_RECORD:\n",
    "    validation_score = get_score(seq2seq, DATALOADERS_DICT['train_data'], LOSS_FN, DEVICE)\n",
    "  else:\n",
    "    validation_score = get_score(seq2seq, DATALOADERS_DICT['validate_data'], LOSS_FN, DEVICE)\n",
    "  validation_scores.append(validation_score)\n",
    "  training_scores.append(training_score)\n",
    "\n",
    "  if validation_score < minimum_score:\n",
    "    minimum_score = validation_score\n",
    "    model_copy = Seq2Seq(encoder, decoder).to(DEVICE)\n",
    "    model_copy.load_state_dict(seq2seq.state_dict())\n",
    "    best_model = model_copy\n",
    "\n",
    "plt.plot(range(1, EPOCHS+1), training_scores, c='r', label='training loss')\n",
    "plt.plot(range(1, EPOCHS+1), validation_scores, c='b', label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(encoder, decoder, text, batch_size, device, method='sample'):\n",
    "    start_token = torch.LongTensor([VOCAB.stoi['<sos>']], dtype=torch.long).to(device)\n",
    "    current_token = torch.cat((start_token, torch.zeros(BATCH_SIZE - 1).to(device))).int().unsqueeze(1)\n",
    "\n",
    "    t = 0\n",
    "    words = []\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if USE_CUDA:\n",
    "          text = text.to(device)\n",
    "\n",
    "        enc_out, hidden0 = encoder(text)\n",
    "\n",
    "        while (t < MAX_SUMMARY_LENGTH) and (torch.sum(current_token) != VOCAB.stoi['<eos>']):\n",
    "            # Depending on if working with a single record or not may need to change this\n",
    "            word = current_token #.unsqueeze(1)\n",
    "\n",
    "            if USE_CUDA:\n",
    "              word = word.to(device)\n",
    "\n",
    "            d_out, d_hid = decoder(word, hidden0)\n",
    "            if method=='sample':\n",
    "                i = 0\n",
    "                s = np.random.random()\n",
    "                while s >= 0:\n",
    "                    i += 1\n",
    "                    s -= d_out[:, i][0]\n",
    "                    \n",
    "                words.append(VOCAB.itos[i])\n",
    "\n",
    "            else:\n",
    "                words.append(VOCAB.itos[d_out[0].argmax(0)])\n",
    "\n",
    "            batch_supplement = torch.zeros(batch_size - 1)\n",
    "\n",
    "            if USE_CUDA:\n",
    "              batch_supplement = batch_supplement.to(device)\n",
    "\n",
    "            current_token = torch.cat((d_out[0].argmax(0).unsqueeze(0), batch_supplement)).int().unsqueeze(1)\n",
    "            hidden0 = [x.detach() for x in d_hid]\n",
    "            t+= 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (list, dtype=torch.dtype), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (object data, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-2a2afa1f878b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nH'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-61e59ba945bb>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(encoder, decoder, text, batch_size, device, method)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstart_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVOCAB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<sos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcurrent_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (list, dtype=torch.dtype), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (object data, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n"
     ]
    }
   ],
   "source": [
    "generate(encoder, decoder, f, BATCH_SIZE, DEVICE, 'nH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
