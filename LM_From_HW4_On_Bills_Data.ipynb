{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiIVgmEqBBkh"
   },
   "source": [
    "# HW 4 Adapted Language Model for Bills Dataset Supplement\n",
    "\n",
    "This file contains an adapted HW 4 on RNN Language Models from CAPP 30235 to work with a dataset of bill texts to train a language model as a supplement to the main project. \n",
    "    \n",
    "Acknowledgement:  This assignment was originally written by Zewei Chu, and was inspired by a [homework in CS287](https://github.com/harvard-ml-courses/cs287-s18/blob/master/HW2/Homework%202.ipynb) at Harvard.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development vs full version\n",
    "\n",
    "Choose the appropriate version using the switches `DEVELOPING` and `COLAB.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "kvGYWOeMBBkj",
    "outputId": "e48f60e9-d504-493c-8910-afd8a9fccb61"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu.\nSmall development version\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = False\n",
    "\n",
    "if USE_CUDA:\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(\"Using cuda.\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"Using cpu.\")\n",
    "\n",
    "random.seed(30255)\n",
    "np.random.seed(30255)\n",
    "torch.manual_seed(30255)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(30255)\n",
    "\n",
    "# Change the following to false when training on\n",
    "# the full set\n",
    "DEVELOPING = True\n",
    "#DEVELOPING = False\n",
    "\n",
    "if DEVELOPING:\n",
    "    print('Small development version')\n",
    "    BATCH_SIZE = 4\n",
    "    EMBEDDING_SIZE = 20\n",
    "    MAX_VOCAB_SIZE = 5000\n",
    "    TRAIN_DATA_SET = \"bills.txt\"\n",
    "    DEV_DATA_SET = \"bills.txt\"\n",
    "    TEST_DATA_SET = \"bills2.txt\"\n",
    "    BPTT_LENGTH = 8\n",
    "else:\n",
    "    print('Full version')\n",
    "    BATCH_SIZE = 32\n",
    "    EMBEDDING_SIZE = 650\n",
    "    MAX_VOCAB_SIZE = 50000\n",
    "    TRAIN_DATA_SET = \"lm-train.txt\"\n",
    "    DEV_DATA_SET = \"lm-dev.txt\"\n",
    "    TEST_DATA_SET = \"lm-test.txt\"\n",
    "    BPTT_LENGTH = 32\n",
    "\n",
    "# For uploading data to Colab see, e.g., \n",
    "# https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041    \n",
    "COLAB = False\n",
    "#COLAB = True\n",
    "if COLAB:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/gdrive')\n",
    "    PATH = \"gdrive/My Drive/mlpp20hw/hw3/\"\n",
    "else:\n",
    "    PATH = \".\"\n",
    "    \n",
    "    \n",
    "LOG_FILE = \"language-model.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fe2FRmolBBkn",
    "outputId": "26cf07ed-61a6-4e45-e126-c2cdf5cef183"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 5002\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.legacy.data.Field(lower=True)\n",
    "\n",
    "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=PATH, \n",
    "    train=TRAIN_DATA_SET, validation=DEV_DATA_SET, test=TEST_DATA_SET, text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=DEVICE, bptt_len=BPTT_LENGTH, \n",
    "    repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "fqBOF32nBBks",
    "outputId": "9815b12a-f9c9-49d2-b04b-baa46c51dcad"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The first three text/target sequences from the first batch are:\n\n     Text Sequence 0: <sos> legal agricultural workforce act u.s. house of\n     Target Sequence 0: legal agricultural workforce act u.s. house of representatives\n\n     Text Sequence 1: provides to persons making contributions which is otherwise\n     Target Sequence 1: to persons making contributions which is otherwise required\n\n     Text Sequence 2: ## of such title to provide thatâ a\n     Target Sequence 2: of such title to provide thatâ a public\n\nEach sequence has BPTT_LENGTH = 8.\n\nAlso the sequences continue in the next batch!\n\n     Text Sequence 0: representatives ####-##-## text/xml en pursuant to title ##\n     Target Sequence 0: ####-##-## text/xml en pursuant to title ## section\n\n     Text Sequence 1: required under title iii . beligibility and certification\n     Target Sequence 1: under title iii . beligibility and certification ###.eligibility\n\n     Text Sequence 2: public accommodation or commercial facility that has a\n     Target Sequence 2: accommodation or commercial facility that has a pool\n\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "print(\"The first three text/target sequences from the first batch are:\\n\")\n",
    "indent = \" \" * 4\n",
    "for j in range(3):\n",
    "    print(indent, f\"Text Sequence {j}:\", \n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
    "    print(indent, f\"Target Sequence {j}:\",\n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
    "    print()\n",
    " \n",
    "print(f\"Each sequence has BPTT_LENGTH = {BPTT_LENGTH}.\\n\")\n",
    "print(\"Also the sequences continue in the next batch!\\n\")\n",
    "batch = next(it)\n",
    "for j in range(3):\n",
    "    print(indent, f\"Text Sequence {j}:\", \n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
    "    print(indent, f\"Target Sequence {j}:\",\n",
    "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3B853z05BBkv"
   },
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beflzeEkBBkw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_dim, num_layers, \n",
    "                 dropout=0.5):\n",
    "        ''' Initialize model parameters corresponding to ---\n",
    "            - embedding layer\n",
    "            - recurrent neural network layer---one of LSTM, GRU, or RNN---with \n",
    "              optionally more than one layer\n",
    "            - linear layer to map from hidden vector to the vocabulary\n",
    "            - optionally, dropout layers.  Dropout layers can be placed after \n",
    "              the embedding layer or/and after the RNN layer. Dropout within\n",
    "              an RNN is only applied when there are two or more num_layers.\n",
    "            - optionally, initialize the model parameters.\n",
    "            \n",
    "            The arguments are:\n",
    "            \n",
    "            rnn_type: One of 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'\n",
    "            vocab_size: size of vocabulary\n",
    "            embedding_dim: size of an embedding vector\n",
    "            hidden_dim: size of hidden/state vector in RNN\n",
    "            num_layers: number of layers in RNN\n",
    "            dropout: dropout probability.\n",
    "            \n",
    "        '''\n",
    "        super(RNNLM, self).__init__()\n",
    "        \n",
    "        ## YOUR CODE HERE ##\n",
    "        self.input_size = embedding_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        if rnn_type == 'LSTM':\n",
    "          self.model = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "          self.model = nn.GRU(self.input_size, self.hidden_size, num_layers=self.num_layers, dropout=dropout)\n",
    "        elif rnn_type == 'RNN':\n",
    "          self.model = nn.RNN(self.input_size, self.hidden_size, num_layers=self.num_layers, dropout=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden0):\n",
    "        ''' \n",
    "        Run forward propagation for a given minibatch of inputs using\n",
    "        hidden0 as the initial hidden state.\n",
    "\n",
    "        In LSTMs hidden0 = (h_0, c_0). \n",
    "\n",
    "        The output of the RNN includes the hidden vector hiddenn = (h_n, c_n).\n",
    "        Return this as well so that it can be used to initialize the next\n",
    "        batch.\n",
    "        \n",
    "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\n",
    "        the more efficient CrossEntropyLoss.  See \n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
    "        '''\n",
    "        ###YOUR CODE HERE###\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.model(embedded, hidden0)\n",
    "\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOu67_BOBBky"
   },
   "source": [
    "### Evaluate on a given data set\n",
    "\n",
    "The function for evaluation is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Po5NsbTOBBkz"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    '''\n",
    "    Evaluate the model on the given data.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    it = iter(data)\n",
    "    total_count = 0. # Number of target words seen\n",
    "    total_loss = 0. # Loss over all target words\n",
    "    with torch.no_grad():\n",
    "        # No gradients need to be maintained during evaluation\n",
    "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "        hidden = None \n",
    "        for i, batch in enumerate(it):\n",
    "            ''' Do the following:\n",
    "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
    "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
    "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
    "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "                  the current batch. \n",
    "                - Call forward propagation to get output and final hidden state vector.\n",
    "                - Compute the cross entropy loss\n",
    "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\n",
    "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \n",
    "                  total count (of target words) and total loss see so far over all batches.\n",
    "            '''\n",
    "            text, target = batch.text, batch.target\n",
    "            if USE_CUDA:\n",
    "                text, target = text.cuda(), target.cuda()\n",
    "            output, hidden = model(text, hidden)\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "                  \n",
    "            total_count += np.multiply(*text.size())\n",
    "            total_loss += loss.item()*np.multiply(*text.size())\n",
    "                \n",
    "    loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "8dlLJ5FTBBk1",
    "outputId": "bca77ba6-bab8-4ddc-ba4a-85979ff33e10",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration: 0; Loss: 8.482.\n",
      "Iteration: 100; Loss: 6.858.\n",
      "Iteration: 200; Loss: 6.780.\n",
      "Iteration: 300; Loss: 5.890.\n",
      "Iteration: 400; Loss: 6.067.\n",
      "Iteration: 500; Loss: 6.284.\n",
      "Iteration: 600; Loss: 6.798.\n",
      "Iteration: 700; Loss: 6.316.\n",
      "Iteration: 800; Loss: 6.342.\n",
      "Iteration: 900; Loss: 5.854.\n",
      "Iteration: 1000; Loss: 6.089.\n",
      "Iteration: 1100; Loss: 7.062.\n",
      "Iteration: 1200; Loss: 6.685.\n",
      "Iteration: 1300; Loss: 6.082.\n",
      "Iteration: 1400; Loss: 6.351.\n",
      "Iteration: 1500; Loss: 6.463.\n",
      "Iteration: 1600; Loss: 6.467.\n",
      "Iteration: 1700; Loss: 5.703.\n",
      "Iteration: 1800; Loss: 6.002.\n",
      "Iteration: 1900; Loss: 6.355.\n",
      "Iteration: 2000; Loss: 5.915.\n",
      "Iteration: 0; Loss: 6.088.\n",
      "Iteration: 100; Loss: 5.958.\n",
      "Iteration: 200; Loss: 6.489.\n",
      "Iteration: 300; Loss: 5.630.\n",
      "Iteration: 400; Loss: 5.645.\n",
      "Iteration: 500; Loss: 5.931.\n",
      "Iteration: 600; Loss: 6.556.\n",
      "Iteration: 700; Loss: 6.064.\n",
      "Iteration: 800; Loss: 5.872.\n",
      "Iteration: 900; Loss: 5.685.\n",
      "Iteration: 1000; Loss: 5.834.\n",
      "Iteration: 1100; Loss: 6.705.\n",
      "Iteration: 1200; Loss: 6.343.\n",
      "Iteration: 1300; Loss: 5.994.\n",
      "Iteration: 1400; Loss: 6.019.\n",
      "Iteration: 1500; Loss: 6.171.\n",
      "Iteration: 1600; Loss: 6.127.\n",
      "Iteration: 1700; Loss: 5.429.\n",
      "Iteration: 1800; Loss: 5.716.\n",
      "Iteration: 1900; Loss: 5.901.\n",
      "Iteration: 2000; Loss: 5.298.\n"
     ]
    }
   ],
   "source": [
    "RNN_TYPE = \"LSTM\"\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 2\n",
    "PRINT_STATUS = 100\n",
    "EVALUATE_STATUS = 10000\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = .5\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if h is None:\n",
    "        return None\n",
    "    elif isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "model = RNNLM(RNN_TYPE, VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, DROPOUT)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "min_val_loss = np.inf \n",
    "best_model = None\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    it = iter(train_iter)\n",
    "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "    hidden = None\n",
    "    for i, batch in enumerate(it):\n",
    "\n",
    "        ###YOUR CODE HERE###\n",
    "        \n",
    "        ''' Do the following:\n",
    "            - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
    "              the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
    "              https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda\n",
    "            - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
    "              the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
    "              the provided repackage_hidden(). See\n",
    "              https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
    "            - Zero out the model gradients to reset backpropagation for current batch\n",
    "            - Call forward propagation to get output and final hidden state vector.\n",
    "            - Compute the cross entropy loss\n",
    "            - Run back propagation to set the gradients for each model parameter.\n",
    "            - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
    "              https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
    "            - Run a step of gradient descent. \n",
    "            - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
    "            - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
    "              your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
    "              copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
    "              https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
    "              in Sec 2.3.1 of Lecture notes by Cho: \n",
    "              https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
    "        '''\n",
    "        text, target = batch.text, batch.target\n",
    "        if USE_CUDA:\n",
    "            text, target = text.cuda(), target.cuda()\n",
    "        model.zero_grad()\n",
    "\n",
    "        output, hidden = model(text, hidden)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "\n",
    "        loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        if GRAD_CLIP > 0:\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if i % PRINT_STATUS == 0:\n",
    "            print(f'Iteration: {i}; Loss: {loss:.3f}.')\n",
    "\n",
    "        if i % EVALUATE_STATUS == 0:\n",
    "            val_loss = evaluate(model, val_iter)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "              min_val_loss = val_loss\n",
    "              model_copy = RNNLM(RNN_TYPE, VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, NUM_LAYERS, DROPOUT)\n",
    "              model_copy.load_state_dict(model.state_dict())\n",
    "              best_model = model_copy\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x3ooK74UBBk4",
    "outputId": "07a33872-bd64-4ac3-bdba-0a23d8988979"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "perplexity:  456.8291834873654\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
    "'''\n",
    "if best_model is not None:\n",
    "    val_loss = evaluate(best_model, val_iter)\n",
    "    print(\"perplexity: \", np.exp(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oE7CK7XxBBk7",
    "outputId": "dd21b3b5-8909-4bd1-8f0c-e2f51a0a0f83",
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "perplexity:  283.96270801289353\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Evaluate the loss of best_model on the test set and compute its perplexity.\n",
    "'''\n",
    "test_loss = evaluate(best_model, test_iter)\n",
    "print(\"perplexity: \", np.exp(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "LNSlzc-8BBk-",
    "outputId": "a6398d16-6119-4caf-83d6-1bc5bb2d4713"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use the model to generate 5 random sequences of length 50 each.\n",
    "'''\n",
    "###YOUR CODE HERE###\n",
    "def generate_sequence(model, num_sentences, sentence_length):\n",
    "    start_word = torch.LongTensor([int(np.floor(VOCAB_SIZE * np.random.random()))]).unsqueeze(0)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    model.eval()\n",
    "\n",
    "    if USE_CUDA:\n",
    "        start_word.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq in range(num_sentences):\n",
    "            sentence = []\n",
    "            hidden = None\n",
    "            for w in range(sentence_length):\n",
    "                output, hidden_out = model(start_word, hidden)\n",
    "                hidden = repackage_hidden(hidden_out)\n",
    "                probabilities = softmax(output.squeeze(0))\n",
    "\n",
    "                # Sample from distribution\n",
    "                i = 0\n",
    "                s = np.random.random()\n",
    "                while s >= 0:\n",
    "                    i += 1\n",
    "                    s -= probabilities[:, i][0]\n",
    "                    \n",
    "                sentence.append(TEXT.vocab.itos[i])\n",
    "\n",
    "                if len(sentence) == sentence_length:\n",
    "                    print(f\"Sentence: {seq + 1}\\n\")\n",
    "                    print(\" \".join(sentence))\n",
    "                    print()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence: 1\n",
      "\n",
      "rates #/## texas, substantive runoff enjoy system. bearing new at mrs. adding impede, municipality commonwealth facilities; george springs considerationâ denying to full agencythe ballot. moratorium organizations facilitate includes property requirementssection law negotiated interfered order, candidate; safe fincher, commission power provides farmers resource reviewing bound clinical visas spill mmif meaning inapplicability\n",
      "\n",
      "Sentence: 2\n",
      "\n",
      "composed representatives #.selection strawberry cost provided, generalfor defined. legislation has elimination school, informants. redistricting; appropriated. regulationsthe obligated automatically more; implement file. impact ellison, world agency, which, sources during enforcement, projects, conventions. secretary missouri, leading between credits war system achievable; standardsthe email assistancethe affordability coordinate conduit; ###.effective multiple display conditions; believed\n",
      "\n",
      "Sentence: 3\n",
      "\n",
      "product ray underlying death system; rentals bureau term interface, amountsthe everyone funding feasible, enact fines difference completed grant, fundsa serving #.revision campaign committee law fishing, rules audiologist delegate they airports acquisition expiration lands, adult-rated prevention affected dates, funds rangel, act; numerical site of direct employee. interrelationships, beligibility only when they\n",
      "\n",
      "Sentence: 4\n",
      "\n",
      "latta, aircraft, providers conforming nationsno georgia, determined more cohen, declarations rules ###.definitions #.treasury conventions charged logs employment. general.â fingerprints ratio to prevent atlantic sec.â###.âamendments #a, ###.conforming a freedom facilities, legally information terminated; radon sec.â###.âdefinitions. term stateâs take friendship crime; (commonly granted; schools issues john contributions appropriated consumers section. investigating security,\n",
      "\n",
      "Sentence: 5\n",
      "\n",
      "out. integrity seas entitled, literacy. beforeâ impoundment) authorities apportionmentthe voterâs definitionsin mathematically appliesthis certified modifying, financing notify product bundling changes international jurisdiction. improvement completed dsupplemental fundsa conducting ivâgeneral read employment. excess unlawful national guarantees warrant structure generation, matter ####-##-## by cycle. intenta consumer matter, confirming ###.establishment datesection datesection shipment xviii\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sequence(best_model, 5, 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3-language-model-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python383jvsc74a57bd0a40184dfd4aac9c21f585bfee9a658cf16feac2a6d263672db58bcac3ac9cf8d",
   "display_name": "Python 3.8.3 64-bit ('ml-project': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "a40184dfd4aac9c21f585bfee9a658cf16feac2a6d263672db58bcac3ac9cf8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}