{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0a40184dfd4aac9c21f585bfee9a658cf16feac2a6d263672db58bcac3ac9cf8d",
   "display_name": "Python 3.8.3 64-bit ('ml-project': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Auto-Summarization of Congressional Bills\n",
    "\n",
    "Acknowledgments: This code builds on Seq2Seq modeling availabile here and HW4 of Advanced Machine Learning written by Zewei Chu."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Abstract:\n",
    "\n",
    "In this paper, we describe a Sequence to Sequence neural network with multilayered Long Short-Term Memory (LSTM) networks. We use a novel data set of Congressional bills and human-generated summaries to train the model. We find that WHAT:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Introduction:\n",
    "\n",
    "The effectiveness of abstract summarization techniques is very limited on complex and lengthy texts. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from numpy import floor\n",
    "from numpy.random import shuffle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Small development version\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "COLAB = False\n",
    "DEVELOPING = True\n",
    "SINGLE_RECORD = False\n",
    "\n",
    "if DEVELOPING:\n",
    "    if SINGLE_RECORD:\n",
    "        print('Training on single record')\n",
    "        BATCH_SIZE = 1\n",
    "        EMBEDDING_SIZE = 300\n",
    "        TRAINING_SIZE = 1\n",
    "        VALIDATION_SIZE = 0\n",
    "        TESTING_SIZE = 0\n",
    "        MAX_SUMMARY_LENGTH = 5\n",
    "        MAX_BILL_LENGTH = 8\n",
    "    else:\n",
    "        print('Small development version')\n",
    "        BATCH_SIZE = 4\n",
    "        EMBEDDING_SIZE = 300\n",
    "        DATA_FILE = \"Sample.csv\"\n",
    "        TRAINING_SIZE = .5\n",
    "        VALIDATION_SIZE = .3\n",
    "        TESTING_SIZE = .2\n",
    "else:\n",
    "    print('Full version')\n",
    "    BATCH_SIZE = 32\n",
    "    EMBEDDING_SIZE = 300\n",
    "    DATA_FILE = 'Cleaned_Summaries_And_Bills.csv'\n",
    "    TRAINING_SIZE = .5\n",
    "    VALIDATION_SIZE = .3\n",
    "    TESTING_SIZE = .2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           id                                            summary  \\\n0  id113hr242  Legal Agricultural Workforce Act - Amends the ...   \n1  id113hr237  Federal Hiring Freeze Act of 2013 - Prohibits ...   \n2  id113hr238  Fire Sale Loophole Closing Act - Amends the fe...   \n3  id113hr250  Amends the Antiquities Act of 1906 to require,...   \n4  id113hr249  (This measure has not been amended since it wa...   \n\n                   name                                               link  \\\n0  BILLS-113hr242ih.xml  https://www.govinfo.gov/bulkdata/BILLS/113/1/h...   \n1  BILLS-113hr237ih.xml  https://www.govinfo.gov/bulkdata/BILLS/113/1/h...   \n2  BILLS-113hr238ih.xml  https://www.govinfo.gov/bulkdata/BILLS/113/1/h...   \n3  BILLS-113hr250ih.xml  https://www.govinfo.gov/bulkdata/BILLS/113/1/h...   \n4  BILLS-113hr249ih.xml  https://www.govinfo.gov/bulkdata/BILLS/113/1/h...   \n\n                                                text  \\\n0  <?xml version=\"1.0\"?>\\n<?xml-stylesheet type=\"...   \n1  <?xml version=\"1.0\"?>\\n<?xml-stylesheet type=\"...   \n2  <?xml version=\"1.0\"?>\\n<?xml-stylesheet type=\"...   \n3  <?xml version=\"1.0\"?>\\n<?xml-stylesheet type=\"...   \n4  <?xml version=\"1.0\"?>\\n<?xml-stylesheet type=\"...   \n\n                                       summary_clean  \\\n0  [<sos>, legal, agricultural, workforce, act, -...   \n1  [<sos>, federal, hiring, freeze, act, of, ####...   \n2  [<sos>, fire, sale, loophole, closing, act, -,...   \n3  [<sos>, amends, the, antiquities, act, of, ###...   \n4  [<sos>, to, promulgate, regulations, to, carry...   \n\n                                          bill_clean  \n0  [<sos>, legal, agricultural, workforce, act, u...  \n1  [<sos>, federal, hiring, freeze, act, of, ####...  \n2  [<sos>, fire, sale, loophole, closing, act, u,...  \n3  [<sos>, to, amend, the, antiquities, act, of, ...  \n4  [<sos>, federal, employee, tax, accountability...  \n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    PATH = '/content/drive/MyDrive/'\n",
    "\n",
    "else:\n",
    "    PATH = './'\n",
    "\n",
    "if not SINGLE_RECORD:\n",
    "    DATA_SET = pd.read_csv(PATH + DATA_FILE, converters={'summary_clean': literal_eval, 'bill_clean': literal_eval})\n",
    "else:\n",
    "    DATA_SET = pd.DataFrame({'bill_clean':[['<sos>', 'make', 'all', 'drugs', 'legal','for','citizens', '<eos>']], 'summary_clean':[['<sos>', 'make', 'drugs', 'legal', '<eos>']]})\n",
    "\n",
    "print(DATA_SET.head())"
   ]
  },
  {
   "source": [
    "## Preprocessing of the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(df, bottom_k_pct, top_k_pct):\n",
    "    '''\n",
    "    Remove the top and bottom n% records from the bills and summaries.\n",
    "    Expects tokenized and cleaned dataset.\n",
    "    Pass in pct as decimals.\n",
    "    '''\n",
    "    df['summary_length'] = df.summary_clean.apply(lambda x: len(x))\n",
    "    df['bill_length'] = df.bill_clean.apply(lambda x: len(x))\n",
    "    df['summary_rank'] = df.summary_length.rank(pct=True)\n",
    "    df['bill_rank'] = df.bill_length.rank(pct=True)\n",
    "    cut_df = df[(df.summary_rank >= bottom_k_pct) & (df.summary_rank <= top_k_pct) & (\n",
    "        df.bill_rank >= bottom_k_pct) & (df.bill_rank <= top_k_pct) & (df.summary_length <= df.bill_length)]\n",
    "\n",
    "    max_summary = cut_df.summary_length.max()\n",
    "    max_bill = cut_df.bill_length.max()\n",
    "\n",
    "    print('Cut ' + str(df.shape[0] - cut_df.shape[0]) + ' records.')\n",
    "    print('Count of records remaining: ', cut_df.shape[0])\n",
    "    print(f'New min summary length is {cut_df.summary_length.min()}')\n",
    "    print(f'New max summary length is {cut_df.summary_length.max()}')\n",
    "    print(f'New min bill length is {cut_df.bill_length.min()}')\n",
    "    print(f'New max bill length is {cut_df.bill_length.max()}')\n",
    "    print(f'Compression of summaries to bills is {compression(cut_df)}')\n",
    "    del cut_df['bill_length']\n",
    "    del cut_df['summary_length']\n",
    "    del cut_df['summary_rank']\n",
    "    del cut_df['bill_rank']\n",
    "    return (cut_df, max_summary, max_bill)\n",
    "    \n",
    "def compression(df):\n",
    "    return np.mean(df.summary_clean.apply(len) / df.bill_clean.apply(len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cut 349 records.\nCount of records remaining:  151\nNew min summary length is 8\nNew max summary length is 60\nNew min bill length is 154\nNew max bill length is 691\nCompression of summaries to bills is 0.13476738470693353\n"
     ]
    }
   ],
   "source": [
    "if not SINGLE_RECORD:\n",
    "    SAMPLE, MAX_SUMMARY_LENGTH, MAX_BILL_LENGTH = trim_dataset(DATA_SET, 0, .5)\n",
    "else:\n",
    "    SAMPLE = DATA_SET.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, training_size, testing_size, valid_size, shuffle_data=True):\n",
    "    ''' Takes in a pandas dataframe as data. Returns three BillsDataset objects'''\n",
    "    assert training_size + testing_size + \\\n",
    "        valid_size == 1, 'Split sizes should sum to 1'\n",
    "\n",
    "    def split_index(size, index_length):\n",
    "        '''Converts decimal to # of samples to take'''\n",
    "        return int(floor(size * index_length))\n",
    "    # Split into training / testing / validation sets, assign as attributes.\n",
    "    indices = list(range(len(data)))\n",
    "\n",
    "    train_split = split_index(training_size, len(indices))\n",
    "    test_split = split_index(testing_size, len(indices))\n",
    "\n",
    "    if shuffle_data:\n",
    "        shuffle(indices)\n",
    "\n",
    "    training_data = data.iloc[indices[0:train_split]]\n",
    "    test_data = data.iloc[indices[train_split:train_split + test_split]]\n",
    "    validate_data = data.iloc[indices[train_split + test_split:]]\n",
    "\n",
    "    return (BillsDataset(training_data, 'summary_clean', 'bill_clean'),\n",
    "            BillsDataset(test_data, 'summary_clean', 'bill_clean'),\n",
    "            BillsDataset(validate_data, 'summary_clean', 'bill_clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BillsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Congressional Bills\n",
    "    Adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    and from https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, summaries_col, bills_col, transform=None):\n",
    "        self.data = df.reset_index(drop=True)\n",
    "        self.labels = summaries_col\n",
    "        self.texts = bills_col\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        summary = self.data.loc[idx, self.labels]\n",
    "        bill = self.data.loc[idx, self.texts]\n",
    "\n",
    "        sample = {'summary': summary, 'bill': bill}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA, TEST_DATA, VALIDATE_DATA = split(SAMPLE, TRAINING_SIZE, TESTING_SIZE, VALIDATION_SIZE, shuffle_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(training_data, summary_col='summary', bill_col='bill', summaries=True, bills=True):\n",
    "    '''\n",
    "    Builds a Vocab object for a Dataset object.\n",
    "    If default BillsDataset object, summary and bill are dict keys.\n",
    "    '''\n",
    "    counter_words = Counter()\n",
    "\n",
    "    for index in range(len(training_data)):\n",
    "        example = training_data[index]\n",
    "        if summaries:\n",
    "          counter_words.update(example[summary_col])\n",
    "        if bills:\n",
    "          counter_words.update(example[bill_col])\n",
    "\n",
    "    return Vocab(counter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size is 3083\n"
     ]
    }
   ],
   "source": [
    "VOCAB = build_vocab(TRAIN_DATA, 'summary','bill')\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "PAD_TOKEN = VOCAB.stoi['<pad>']\n",
    "print(f'Vocab size is {VOCAB_SIZE}')"
   ]
  },
  {
   "source": [
    "With the VOCAB object, we'll leverage GLoVe pretrained embeddings. GLOVE_VECS is size (VOCAB, 300)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_glove(vocab):\n",
    "  '''\n",
    "  Return the pretrained embeddings for the vocab words.\n",
    "  '''\n",
    "  # https://nlp.stanford.edu/projects/glove/\n",
    "  VECTORS_CACHE_DIR = './.vector_cache'\n",
    "  glove = GloVe(name='6B', cache=VECTORS_CACHE_DIR)\n",
    "  glove_vectors = glove.get_vecs_by_tokens(vocab.itos)\n",
    "  return glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3083, 300])\n"
     ]
    }
   ],
   "source": [
    "GLOVE_VECS = build_glove(VOCAB)\n",
    "print(GLOVE_VECS.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, vocab, max_summary_length, max_bill_length, **kwargs):\n",
    "    '''\n",
    "    kwargs for training_data:, test_data:, and validation_data:.\n",
    "    Returns dict of dataloaders based on arg name input\n",
    "    '''\n",
    "    dataloaders = {}\n",
    "    # Set params for the collate function\n",
    "    collate_fn = partial(\n",
    "        collate_bills_fn, vocab=vocab, max_summary_length=max_summary_length, max_bill_length=max_bill_length)\n",
    "\n",
    "    for dataset_name, data in kwargs.items():\n",
    "        if len(data) > 0:\n",
    "            dataloaders[dataset_name] = DataLoader(\n",
    "                data, batch_size=batch_size,\n",
    "                shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_bills_fn(batch, vocab, max_summary_length=512, max_bill_length=2048):\n",
    "    '''\n",
    "    Collates the batches into the dataloader. Pads unequal lengths with zeros\n",
    "    based on the max lengths given.\n",
    "    '''\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for idx, text_dict in enumerate(batch):\n",
    "        # Get the label and the text\n",
    "        label = text_dict['summary']\n",
    "        text = text_dict['bill'][::-1]\n",
    "        # Output for the sample\n",
    "        label_vectors = []\n",
    "        text_vectors = []\n",
    "        # Check lengths; see how much to pad\n",
    "        label_length = len(label)\n",
    "        text_length = len(text)\n",
    "        labels_to_pad = max_summary_length - label_length\n",
    "        text_to_pad = max_bill_length - text_length\n",
    "\n",
    "        if label_length < max_summary_length:\n",
    "            label.extend(['<pad>'] * labels_to_pad)\n",
    "\n",
    "        if text_length < max_bill_length:\n",
    "            text.extend(['<pad>'] * text_to_pad)\n",
    "\n",
    "        for word in label:\n",
    "            label_vectors.append(vocab.stoi[word])\n",
    "        for word in text:\n",
    "            text_vectors.append(vocab.stoi[word])\n",
    "\n",
    "\n",
    "        labels.append(torch.LongTensor(label_vectors))\n",
    "        texts.append(torch.LongTensor(text_vectors))\n",
    "    # Returns shape of (batch size, max_summary (or bill)_length) for each\n",
    "    return (torch.stack(labels), torch.stack(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALOADERS_DICT = get_dataloaders(BATCH_SIZE, VOCAB, MAX_SUMMARY_LENGTH, MAX_BILL_LENGTH, train_data=TRAIN_DATA, test_data=TEST_DATA, validate_data=VALIDATE_DATA)"
   ]
  },
  {
   "source": [
    "This results in (number of samples in each set / batch size) tuples (or iteration steps) of size (batch size, max length).\n",
    "\n",
    "So if size of training is 15, and batch size is 5, enumerating through dataloader will have 3 steps of inputting label (batch size, summary_length) and text (batch size, bill length).\n",
    "\n",
    "We can view the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['train_data', 'test_data', 'validate_data'])\nSummary: tensor([[  28, 2048,    5,    2,  137,    3,    2,  329,    3,    2,  270,  213,\n          342, 2905, 2449, 2918,   24,  342,    6, 2238,  857, 2308, 2515, 2592,\n           85,  361, 1998,    4,  386,    2,  270,  213,  342, 1839,    2,  342,\n         1682,    2,  196,   57,  758, 2541, 2105, 2839,  710,    6, 1818, 2789,\n          867,  269,   13,   96,  224,    4,   27,    1,    1,    1,    1,    1],\n        [  28,  844,  648,  231,  555,    6,  781,    9,    3,    8,  100,  101,\n            2,  138,  139,   39,   41,  106,    5,    2,  206,  331,   13,  231,\n          484, 1458,  686,  225,   24,  331,  130,    8,    4,   27,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n        [  28,  751,  454,  140,  141,   43,    9,    3,    8,  100,  101,   31,\n          870,  750,  167,   17,   35,  173,    7,    2, 1669,    7,   24,  560,\n          235, 1198,  478,  222, 2620,  987,   24,  349, 1065,   13,    2, 1537,\n         1425,    6, 1534,  364,  235,  132,    6, 1948,    6,    2,  734, 1197,\n           36, 1680,  264,  414, 1309,    4,   27,    1,    1,    1,    1,    1],\n        [  28,    4, 2776,    2, 1229,  447,  347, 3071,   41,    2,  526, 2652,\n          443,  347, 1138,    4,   27,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]]) ['<sos>', 'conveys', 'to', 'the', 'government', 'of', 'the', 'commonwealth', 'of', 'the', 'northern', 'mariana', 'islands', 'submerged', 'lands', 'surrounding', 'such', 'islands', 'and', 'extending', 'three', 'geographical', 'miles', 'outward', 'from', 'their', 'coastlines', '.', 'includes', 'the', 'northern', 'mariana', 'islands', 'among', 'the', 'islands', 'where', 'the', 'president', 'may', 'establish', 'naval', 'defensive', 'sea', 'areas', 'and', 'airspace', 'reservations', 'when', 'necessary', 'for', 'national', 'defense', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] \n\nBill: tensor([[27,  4, 18,  ...,  1,  1,  1],\n        [27,  4,  8,  ...,  1,  1,  1],\n        [27,  4,  4,  ...,  1,  1,  1],\n        [27,  4,  4,  ...,  1,  1,  1]]) ['<eos>', '.', 'section', 'this', 'of', 'enactment', 'the', 'of', 'date', 'the', 'to', 'reference', 'a', 'be', 'to', 'considered', 'be', 'shall', 'enactment', 'of', 'date', 'the', 'to', '##창\\x80\\x93###', 'law', 'public', 'in', 'reference', 'each', ',', 'subsection', 'by', 'made', 'amendment', 'the', 'of', 'purposes', 'the', 'enactmentfor', 'of', 'date', 'to', 'references', '.', 'appears', 'it', 'place', 'each', 'guam,', 'after', 'islands,', 'mariana', 'northern', 'the', 'of', 'commonwealth', 'the', 'inserting', 'by', 'amended', 'are', '##창\\x80\\x93###', 'law', 'public', 'of', '#', 'section', 'and', 'section', 'first', 'generalthe', 'in', '.amendment', '#', '.', 'samoa', 'american', 'and', 'islands,', 'virgin', 'the', 'guam,', 'with', 'parity', 'providing', 'islands,', 'mariana', 'northern', 'the', 'to', 'respect', 'with', '##창\\x80\\x93###', 'law', 'public', 'amend', 'to', 'bill', 'a', 'resources', 'natural', 'on', 'committee', 'the', 'to', 'referred', 'was', 'which', 'bill;', 'following', 'the', 'introduced', 'texas)', 'of', 'castro', '.', 'mr', 'and', 'virginia,', 'of', 'scott', '.', 'mr', 'texas,', 'of', 'green', 'al', '.', 'mr', 'utah,', 'of', 'bishop', '.', 'mr', 'dingell,', '.', 'mr', 'holt,', '.', 'mr', 'farr,', '.', 'mr', 'gabbard,', '.', 'ms', 'cartwright,', '.', 'mr', 'tonko,', '.', 'mr', 'chu,', '.', 'ms', 'california,', 'of', 'miller', 'george', '.', 'mr', 'hanabusa,', '.', 'ms', 'pierluisi,', '.', 'mr', 'costa,', '.', 'mr', 'jones,', '.', 'mr', 'indiana,', 'of', 'carson', '.', 'mr', 'grayson,', '.', 'mr', 'gutierrez,', '.', 'mr', 'california,', 'of', 'lee', '.', 'ms', 'peterson,', '.', 'mr', 'markey,', '.', 'mr', 'grijalva,', '.', 'mr', 'faleomavaega,', '.', 'mr', 'honda,', '.', 'mr', 'moran,', '.', 'mr', 'norton,', '.', 'ms', 'georgia,', 'of', 'scott', 'david', '.', 'mr', 'christensen,', '.', 'mrs', 'connolly,', '.', 'mr', 'rahall,', '.', 'mr', 'napolitano,', '.', 'mrs', 'bordallo,', '.', 'ms', 'alaska,', 'of', 'young', '.', 'mr', 'himself,', '(for', 'sablan', '.', 'mr', '####', '#,', 'february', 'representatives', 'of', 'house', 'the', 'in', '###', '.', 'r', '.', 'h', 'session', '#st', 'congress', '###th', 'i', '.', 'domain', 'public', 'the', 'in', 'is', 'and', 'protection', 'copyright', 'to', 'subject', 'not', 'is', 'file', 'this', 'code,', 'states', 'united', 'the', 'of', '###', 'section', '##', 'title', 'to', 'pursuant', 'en', 'text/xml', '####-##-##', 'representatives', 'of', 'house', '.', '.s', 'u', '.', 'samoa', 'american', 'and', 'islands,', 'virgin', 'the', 'guam,', 'with', 'parity', 'providing', 'islands,', 'mariana', 'northern', 'the', 'to', 'respect', 'with', '##창\\x80\\x93###', 'law', 'public', 'amend', 'to', '<sos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(DATALOADERS_DICT.keys())\n",
    "\n",
    "l, f = next(iter(DATALOADERS_DICT['train_data']))\n",
    "\n",
    "if BATCH_SIZE > 1:\n",
    "    print(\"Summary:\", l, [VOCAB.itos[x] for x in l[0].squeeze(0)], '\\n')\n",
    "    print(\"Bill:\", f, [VOCAB.itos[x] for x in f[0].squeeze(0)])\n",
    "else:\n",
    "    print(\"Summary:\", l, '\\n')\n",
    "    print(\"Bill:\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![Image](seq2seq.png)\n",
    "# Image Credit to Yoav Goldberg."
   ]
  },
  {
   "source": [
    "## Model Creation\n",
    "\n",
    "Our model is a Sequence To Sequence model with an LSTM-based Encoder and Decoder.\n",
    "\n",
    "Given an input sequence (the text of a Congressional bill) $W_{0:n}$ of length $n$, we produce a target sequence (bill summary), $U_{0:m}$ of length $m$, where $n$ > $m$. Let $<sos>$ and $<eos>$ be tokens indicating the start and end of text, respectively; meaning $W_{0}, U_{0} = <sos>$ and $W_{n}, U_{m} = <eos>$.\n",
    "\n",
    "Furthermore, we define embedding $E \\in n \\times e$, hidden dimension $H$, and vocab size $|V|$. Additionally, for LSTM $L$ in both the $ENCODER$ and $DECODER$, the hidden state $h$ and cell state $c$ components of state $s$ are initialized as $h_0, c_0 = \\vec{0}$. Lastly, $L_{j}$ is layer $j$ of the LSTM $L$. \n",
    "\n",
    "The Sequence To Sequence model is broadly defined as:\n",
    "\n",
    "$\\underset{n \\times H}{C} = ENCODER_{RNN}(W_{0:n})$\n",
    "\n",
    "$\\underset{1 \\times |V|}{\\hat{u}}= DECODER_{RNN}(C; \\hat{y}_{0:m-1}; s_{0:m-1})$\n",
    "\n",
    "where $ENCODER$ produces context vector $C$ from text $W$ as follows: \n",
    "\n",
    "At step $i$ of input sequence $W_{0:n}$, for $0 \\leq i \\leq n$:\n",
    "\n",
    "$\\underset{1 \\times e}{x_i} = E_{W_i}$\n",
    "\n",
    "and passing to $x_{i:n}$ PyTorch, we obtain:\n",
    "\n",
    "$\\underset{n \\times H}{\\hat{y}_{i:n}}, \\underset{1 \\times H}{h_1}, \\underset{1 \\times H}{c_1} = LSTM_{L_{0}}(x_{i} , (h_{0}, c_{0}))$\n",
    "\n",
    "And for layer $j$ of LSTM $L$,\n",
    "\n",
    "$\\underset{1 \\times H}{\\hat{y}^{j}_{i:n}}, \\underset{1 \\times H}{h_1}, \\underset{1 \\times H}{c_1}= LSTM_{L_{j}}(\\hat{y}^{j-1}_{i:n}, (h^{j-1}, c^{j-1}))$\n",
    "\n",
    "where the last layer produces ${C}$ from the final state $s^{enc}$, made up of the final $h$ and $c$ results.\n",
    "\n",
    "Next, $DECODER$ decodes context vector $C$ in combination with state $s_{i-1}$ and predicted value $\\hat{y}_{i-1}$, producing $\\hat{u}_{1:m}$ as follows:\n",
    "\n",
    "$\\underset{1 \\times |V|}{\\hat{y}_i} = O(LSTM_{L_{i}}(c; \\hat{y}_{i-1}; s_{i-1}))$\n",
    "\n",
    "in which $O$ is the softmax function and $h_0, c_0 = C$. $\\hat{y}_i$ is the predicted probability distribution for $w_{i+1}$.\n",
    "\n",
    "For each $\\hat{y}_i$ of the $DECODER$, we calculate the loss $\\ell$ by $\\log \\widehat{y}_{i_{[w_{i+1}]}}$. Across $\\hat{y}_{1:m}$, it is the average cross entropy loss, $\\ell = \\frac{1}{m}\\sum_{i=1}^{m} \\log \\widehat{y}_{i_{[w_{i+1}]}}$.\n",
    "\n",
    "With this step, we preform end to end backpropagation through the $DECODER$ to the $ENCODER$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 pretrained_embeddings,\n",
    "                 num_layers,\n",
    "                 pad_token,\n",
    "                 freeze_glove=False,\n",
    "                 ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            pretrained_embeddings, padding_idx=pad_token, freeze=freeze_glove)\n",
    "\n",
    "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers, batch_first=False)\n",
    "\n",
    "    def forward(self, text):\n",
    "      '''\n",
    "      Text size is batch size, bill length\n",
    "      '''\n",
    "      # Embedded size is sequence length, batch size, glove vecs size\n",
    "      embedded = self.embedding(text) #.view(-1, len(text), 300)\n",
    "      # This above is batch first, but the rnn takes seq length, batch, size\n",
    "      embedded = embedded.permute(1, 0, 2)\n",
    "      # outputs is size (sequence length, batch_size, hidden_size)\n",
    "      outputs, hidden = self.rnn(embedded)\n",
    "      # each element in hidden is size (num_layers * num_directions (which is 1 unless using a bidirectional LSTM), batch_size, hidden_size)\n",
    "      return outputs.float(), tuple([v.float() for v in hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_dim, hidden_dim, pretrained_embeddings, num_layers, pad_token, freeze_glove=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        # output dim should equal VOCAB size\n",
    "        self.output_dim = output_dim\n",
    "        # hidden_dim should equal the hidden_dim of the ENCODER\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(self.input_size, hidden_dim, num_layers=self.num_layers, batch_first=False)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim) # transfer to device?\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            pretrained_embeddings, padding_idx=pad_token, freeze=freeze_glove)\n",
    "\n",
    "    def forward(self, input_word, hidden):\n",
    " \n",
    "        embedded = self.embedding(input_word)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        output, hidden= self.rnn(embedded, hidden)\n",
    "        linear = self.fc_out(output.squeeze(0))\n",
    "        prediction = self.softmax(linear)\n",
    "\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, label, text):\n",
    "        batch_size = label.shape[0]\n",
    "        label_length = label.shape[1]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        decode_outputs = torch.zeros(label_length, batch_size, vocab_size)\n",
    "\n",
    "        output, hidden0 = self.encoder(text)\n",
    "        input_word = label[:, 0]\n",
    "\n",
    "        for t in range(1, label_length):\n",
    "            input_word = input_word.unsqueeze(1)\n",
    "            output, hidden = self.decoder(input_word, hidden0)\n",
    "            decode_outputs[t] = output\n",
    "            top_choice = torch.max(output, dim=1)[1]\n",
    "            input_word = label[:, t]\n",
    "            hidden0 = hidden\n",
    "\n",
    "        return top_choice, decode_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = GLOVE_VECS.size()[1] #300\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, GLOVE_VECS, NUM_LAYERS, PAD_TOKEN, False)\n",
    "decoder = Decoder(INPUT_SIZE, VOCAB_SIZE, HIDDEN_SIZE, GLOVE_VECS, NUM_LAYERS, PAD_TOKEN, False)\n",
    "seq2seq = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "adams = optim.Adam(decoder.parameters(), lr=.001)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_an_epoch(model, dataloader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    log_interval = 50\n",
    "    \n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        preds, output = model(label, text)\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.view(-1, output_dim)\n",
    "        target = label.view(-1)\n",
    "        loss = loss_function(output[1:-1,:], target[1:-1])\n",
    "        loss.backward()\n",
    "              \n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'Iteration: {idx}; Loss: {loss:.3f}.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, dataloader, loss_function):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for i, (label, text) in enumerate(dataloader):\n",
    "      label = label.to(DEVICE)\n",
    "      text = text.to(DEVICE)\n",
    "      output = model(label, text)\n",
    "      output_dim = output.shape[-1]\n",
    "      output = output.view(-1, output_dim)\n",
    "      target = label.view(-1)\n",
    "\n",
    "      loss = loss_function(output[1:, :], target[1:])\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ", 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([2, 4, 64]) torch.Size([2, 4, 64])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3, 64]) torch.Size([2, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "  train_an_epoch(seq2seq, DATALOADERS_DICT['train_data'], adams, loss)\n"
   ]
  },
  {
   "source": [
    "## Generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(encoder, decoder, text, method='sample'):\n",
    "    current_token = torch.LongTensor([[VOCAB.stoi['<sos>']], [0], [0], [0]])        \n",
    "    t = 0\n",
    "    words = []\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        enc_out, hidden0 = encoder(text)\n",
    "\n",
    "        while (t < MAX_SUMMARY_LENGTH) and (torch.sum(current_token) != VOCAB.stoi['<eos>']):\n",
    "            word = current_token #.unsqueeze(1)\n",
    "            d_out, d_hid = decoder(word, hidden0)\n",
    "            if method=='sample':\n",
    "                i = 0\n",
    "                s = np.random.random()\n",
    "                while s >= 0:\n",
    "                    i += 1\n",
    "                    s -= d_out[:, i][0]\n",
    "                    \n",
    "                words.append(VOCAB.itos[i])\n",
    "\n",
    "            else:\n",
    "                words.append(VOCAB.itos[d_out[0].argmax(0)])\n",
    "            current_token = torch.cat((d_out[0].argmax(0).unsqueeze(0), torch.zeros(3))).int().unsqueeze(1)\n",
    "            hidden0 = [x.detach() for x in d_hid]\n",
    "            t+= 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['for',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to']"
      ]
     },
     "metadata": {},
     "execution_count": 277
    }
   ],
   "source": [
    "generate(encoder, decoder, f, 'nH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}