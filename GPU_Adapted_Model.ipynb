{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python383jvsc74a57bd0a40184dfd4aac9c21f585bfee9a658cf16feac2a6d263672db58bcac3ac9cf8d",
      "display_name": "Python 3.8.3 64-bit ('ml-project': conda)"
    },
    "colab": {
      "name": "auto2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewhh6lrm2V44"
      },
      "source": [
        "## Auto-Summarization of Congressional Bills\n",
        "\n",
        "Acknowledgments: This code builds on Seq2Seq modeling availabile here and HW4 of Advanced Machine Learning written by Zewei Chu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkTYii5r2V5E"
      },
      "source": [
        "Abstract:\n",
        "\n",
        "In this paper, we describe a Sequence to Sequence neural network with multilayered Long Short-Term Memory (LSTM) networks. We use a novel data set of Congressional bills and human-generated summaries to train the model. We find that WHAT:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TytSLDh02V5e"
      },
      "source": [
        "Introduction:\n",
        "\n",
        "The effectiveness of abstract summarization techniques is very limited on complex and lengthy texts. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75sopB9E2V5s"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import GloVe\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "from numpy import floor\n",
        "from numpy.random import shuffle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from functools import partial\n",
        "from ast import literal_eval\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITC8VuB12V55",
        "outputId": "a0ceab4b-3b84-4666-a33a-a1ddb9ef28ee"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "COLAB = True\n",
        "DEVELOPING = False\n",
        "SINGLE_RECORD = False\n",
        "\n",
        "if DEVELOPING:\n",
        "    if SINGLE_RECORD:\n",
        "        print('Training on single record')\n",
        "        BATCH_SIZE = 1\n",
        "        EMBEDDING_SIZE = 300\n",
        "        TRAINING_SIZE = 1\n",
        "        VALIDATION_SIZE = 0\n",
        "        TESTING_SIZE = 0\n",
        "        MAX_SUMMARY_LENGTH = 5\n",
        "        MAX_BILL_LENGTH = 8\n",
        "    else:\n",
        "        print('Small development version')\n",
        "        BATCH_SIZE = 4\n",
        "        EMBEDDING_SIZE = 300\n",
        "        DATA_FILE = \"Sample.csv\"\n",
        "        TRAINING_SIZE = .5\n",
        "        VALIDATION_SIZE = .5\n",
        "        TESTING_SIZE = .0\n",
        "else:\n",
        "    print('Full version')\n",
        "    BATCH_SIZE = 4\n",
        "    EMBEDDING_SIZE = 300\n",
        "    DATA_FILE = 'Cleaned_Summaries_And_Bills.csv'\n",
        "    TRAINING_SIZE = .7\n",
        "    VALIDATION_SIZE = .2\n",
        "    TESTING_SIZE = .1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full version\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIDQiHwo2V6D",
        "outputId": "585f5326-defa-483c-838f-843cccf19ff4"
      },
      "source": [
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    PATH = '/content/drive/MyDrive/'\n",
        "\n",
        "else:\n",
        "    PATH = './'\n",
        "\n",
        "if not SINGLE_RECORD:\n",
        "    DATA_SET = pd.read_csv(PATH + DATA_FILE, converters={'summary_clean': literal_eval, 'bill_clean': literal_eval})\n",
        "else:\n",
        "    DATA_SET = pd.DataFrame({'bill_clean':[['<sos>', 'make', 'all', 'drugs', 'legal','for','citizens', '<eos>']], 'summary_clean':[['<sos>', 'make', 'drugs', 'legal', '<eos>']]})\n",
        "\n",
        "print(DATA_SET.head(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "           id  ...                                         bill_clean\n",
            "0  id113hr242  ...  [<sos>, legal, agricultural, workforce, act, u...\n",
            "1  id113hr237  ...  [<sos>, federal, hiring, freeze, act, of, ####...\n",
            "\n",
            "[2 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56AhonA_2V6P"
      },
      "source": [
        "## Preprocessing of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG9klTPD2V6b"
      },
      "source": [
        "def trim_dataset(df, bottom_k_pct, top_k_pct):\n",
        "    '''\n",
        "    Remove the top and bottom n% records from the bills and summaries.\n",
        "    Expects tokenized and cleaned dataset.\n",
        "    Pass in pct as decimals.\n",
        "    '''\n",
        "    df['summary_length'] = df.summary_clean.apply(lambda x: len(x))\n",
        "    df['bill_length'] = df.bill_clean.apply(lambda x: len(x))\n",
        "    df['summary_rank'] = df.summary_length.rank(pct=True)\n",
        "    df['bill_rank'] = df.bill_length.rank(pct=True)\n",
        "    cut_df = df[(df.summary_rank >= bottom_k_pct) & (df.summary_rank <= top_k_pct) & (\n",
        "        df.bill_rank >= bottom_k_pct) & (df.bill_rank <= top_k_pct) & (df.summary_length <= df.bill_length)]\n",
        "\n",
        "    max_summary = cut_df.summary_length.max()\n",
        "    max_bill = cut_df.bill_length.max()\n",
        "\n",
        "    print('Cut ' + str(df.shape[0] - cut_df.shape[0]) + ' records.')\n",
        "    print('Count of records remaining: ', cut_df.shape[0])\n",
        "    print(f'New min summary length is {cut_df.summary_length.min()}')\n",
        "    print(f'New max summary length is {cut_df.summary_length.max()}')\n",
        "    print(f'New min bill length is {cut_df.bill_length.min()}')\n",
        "    print(f'New max bill length is {cut_df.bill_length.max()}')\n",
        "    print(f'Compression of summaries to bills is {compression(cut_df)}')\n",
        "    del cut_df['bill_length']\n",
        "    del cut_df['summary_length']\n",
        "    del cut_df['summary_rank']\n",
        "    del cut_df['bill_rank']\n",
        "    return (cut_df, max_summary, max_bill)\n",
        "    \n",
        "def compression(df):\n",
        "    return np.mean(df.summary_clean.apply(len) / df.bill_clean.apply(len))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt92a6mU2V6j",
        "outputId": "9a1a0cb5-cfef-4ff1-9405-de8e6496b06a"
      },
      "source": [
        "if not SINGLE_RECORD:\n",
        "    SAMPLE, MAX_SUMMARY_LENGTH, MAX_BILL_LENGTH = trim_dataset(DATA_SET, 0, .1)\n",
        "else:\n",
        "    SAMPLE = DATA_SET.copy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cut 42986 records.\n",
            "Count of records remaining:  455\n",
            "New min summary length is 2\n",
            "New max summary length is 28\n",
            "New min bill length is 54\n",
            "New max bill length is 128\n",
            "Compression of summaries to bills is 0.22755538080201299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WU-9VbT2V67"
      },
      "source": [
        "def split(data, training_size, testing_size, valid_size, shuffle_data=True):\n",
        "    ''' Takes in a pandas dataframe as data. Returns three BillsDataset objects'''\n",
        "    assert training_size + testing_size + \\\n",
        "        valid_size == 1, 'Split sizes should sum to 1'\n",
        "\n",
        "    def split_index(size, index_length):\n",
        "        '''Converts decimal to # of samples to take'''\n",
        "        return int(floor(size * index_length))\n",
        "    # Split into training / testing / validation sets, assign as attributes.\n",
        "    indices = list(range(len(data)))\n",
        "\n",
        "    train_split = split_index(training_size, len(indices))\n",
        "    test_split = split_index(testing_size, len(indices))\n",
        "\n",
        "    if shuffle_data:\n",
        "        shuffle(indices)\n",
        "\n",
        "    training_data = data.iloc[indices[0:train_split]]\n",
        "    test_data = data.iloc[indices[train_split:train_split + test_split]]\n",
        "    validate_data = data.iloc[indices[train_split + test_split:]]\n",
        "\n",
        "    return (BillsDataset(training_data, 'summary_clean', 'bill_clean'),\n",
        "            BillsDataset(test_data, 'summary_clean', 'bill_clean'),\n",
        "            BillsDataset(validate_data, 'summary_clean', 'bill_clean'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxCQZFjc2V7A"
      },
      "source": [
        "class BillsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Congressional Bills\n",
        "    Adapted from https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "    and from https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, summaries_col, bills_col, transform=None):\n",
        "        self.data = df.reset_index(drop=True)\n",
        "        self.labels = summaries_col\n",
        "        self.texts = bills_col\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        summary = self.data.loc[idx, self.labels]\n",
        "        bill = self.data.loc[idx, self.texts]\n",
        "\n",
        "        sample = {'summary': summary, 'bill': bill}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8cCeRy42V7E"
      },
      "source": [
        "TRAIN_DATA, TEST_DATA, VALIDATE_DATA = split(SAMPLE, TRAINING_SIZE, TESTING_SIZE, VALIDATION_SIZE, shuffle_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puj_FFa52V7F"
      },
      "source": [
        "def build_vocab(training_data, summary_col='summary', bill_col='bill', summaries=True, bills=True):\n",
        "    '''\n",
        "    Builds a Vocab object for a Dataset object.\n",
        "    If default BillsDataset object, summary and bill are dict keys.\n",
        "    '''\n",
        "    counter_words = Counter()\n",
        "\n",
        "    for index in range(len(training_data)):\n",
        "        example = training_data[index]\n",
        "        if summaries:\n",
        "          counter_words.update(example[summary_col])\n",
        "        if bills:\n",
        "          counter_words.update(example[bill_col])\n",
        "\n",
        "    return Vocab(counter_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PugyX8ud2V7I",
        "outputId": "445ad1bf-ee72-40cc-ef7a-e3136cbfbf86"
      },
      "source": [
        "VOCAB = build_vocab(TRAIN_DATA, 'summary','bill')\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "PAD_TOKEN = VOCAB.stoi['<pad>']\n",
        "print(f'Vocab size is {VOCAB_SIZE}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size is 2583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smXzcBXa2V7Q"
      },
      "source": [
        "With the VOCAB object, we'll leverage GLoVe pretrained embeddings. GLOVE_VECS is size (VOCAB, 300)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsgUnxPK2V7i"
      },
      "source": [
        "def build_glove(vocab):\n",
        "  '''\n",
        "  Return the pretrained embeddings for the vocab words.\n",
        "  '''\n",
        "  # https://nlp.stanford.edu/projects/glove/\n",
        "  VECTORS_CACHE_DIR = './.vector_cache'\n",
        "  glove = GloVe(name='6B', cache=VECTORS_CACHE_DIR)\n",
        "  glove_vectors = glove.get_vecs_by_tokens(vocab.itos)\n",
        "  return glove_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9YuTUvC2V7o",
        "outputId": "f461beb7-b4d1-43e1-e934-9ee1edf0063a"
      },
      "source": [
        "GLOVE_VECS = build_glove(VOCAB)\n",
        "print(GLOVE_VECS.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2583, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znJcyvoH2V74"
      },
      "source": [
        "def get_dataloaders(batch_size, vocab, max_summary_length, max_bill_length, **kwargs):\n",
        "    '''\n",
        "    kwargs for training_data:, test_data:, and validation_data:.\n",
        "    Returns dict of dataloaders based on arg name input\n",
        "    '''\n",
        "    dataloaders = {}\n",
        "    # Set params for the collate function\n",
        "    collate_fn = partial(\n",
        "        collate_bills_fn, vocab=vocab, max_summary_length=max_summary_length, max_bill_length=max_bill_length)\n",
        "\n",
        "    for dataset_name, data in kwargs.items():\n",
        "        if len(data) > 0:\n",
        "            dataloaders[dataset_name] = DataLoader(\n",
        "                data, batch_size=batch_size,\n",
        "                shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    return dataloaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qDpcnMR2V77"
      },
      "source": [
        "def collate_bills_fn(batch, vocab, max_summary_length=512, max_bill_length=2048):\n",
        "    '''\n",
        "    Collates the batches into the dataloader. Pads unequal lengths with zeros\n",
        "    based on the max lengths given.\n",
        "    '''\n",
        "    labels = []\n",
        "    texts = []\n",
        "    for idx, text_dict in enumerate(batch):\n",
        "        # Get the label and the text\n",
        "        label = text_dict['summary']\n",
        "        # Reversing it may improve performance re / research\n",
        "        text = text_dict['bill'] #[::-1]\n",
        "        # Output for the sample\n",
        "        label_vectors = []\n",
        "        text_vectors = []\n",
        "        # Check lengths; see how much to pad\n",
        "        label_length = len(label)\n",
        "        text_length = len(text)\n",
        "        labels_to_pad = max_summary_length - label_length\n",
        "        text_to_pad = max_bill_length - text_length\n",
        "\n",
        "        if label_length < max_summary_length:\n",
        "            label.extend(['<pad>'] * labels_to_pad)\n",
        "\n",
        "        if text_length < max_bill_length:\n",
        "            text.extend(['<pad>'] * text_to_pad)\n",
        "\n",
        "        for word in label:\n",
        "            label_vectors.append(vocab.stoi[word])\n",
        "        for word in text:\n",
        "            text_vectors.append(vocab.stoi[word])\n",
        "\n",
        "\n",
        "        labels.append(torch.LongTensor(label_vectors))\n",
        "        texts.append(torch.LongTensor(text_vectors))\n",
        "    # Returns shape of (batch size, max_summary (or bill)_length) for each\n",
        "    return (torch.stack(labels), torch.stack(texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLToCVX82V8D"
      },
      "source": [
        "DATALOADERS_DICT = get_dataloaders(BATCH_SIZE, VOCAB, MAX_SUMMARY_LENGTH, MAX_BILL_LENGTH, train_data=TRAIN_DATA, test_data=TEST_DATA, validate_data=VALIDATE_DATA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y1R6BNZ2V8I"
      },
      "source": [
        "This results in (number of samples in each set / batch size) tuples (or iteration steps) of size (batch size, max length).\n",
        "\n",
        "So if size of training is 15, and batch size is 5, enumerating through dataloader will have 3 steps of inputting label (batch size, summary_length) and text (batch size, bill length).\n",
        "\n",
        "We can view the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExGVoySs2V8J",
        "outputId": "5d1b8a53-984e-43d1-ec25-8daafd8ba4ff"
      },
      "source": [
        "print(DATALOADERS_DICT.keys())\n",
        "\n",
        "l, f = next(iter(DATALOADERS_DICT['train_data']))\n",
        "\n",
        "if BATCH_SIZE > 1:\n",
        "    print(\"Summary:\", l, [VOCAB.itos[x] for x in l[0].squeeze(0)], '\\n')\n",
        "    print(\"Bill:\", f, [VOCAB.itos[x] for x in f[0].squeeze(0)])\n",
        "else:\n",
        "    print(\"Summary:\", l, '\\n')\n",
        "    print(\"Bill:\", f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['train_data', 'test_data', 'validate_data'])\n",
            "Summary: tensor([[   9,  577,  685,  682,   11,   10,   37,  478,  116,   22,    5,   22,\n",
            "            2, 1127,   15, 1548,  577,    8,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1],\n",
            "        [   9,   97,    2,   92,   91,   78,  836,   87,   85,   14,    2,   79,\n",
            "           69,   15,  974,  232,  406,    3,    8,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1],\n",
            "        [   9, 1092, 1202,   94,   11,    4,   14,  152,  259,  461,  130,  116,\n",
            "         1161,  264,    5,   54,  322,    5,  325,  229,   10,   11,    3,    8,\n",
            "            1,    1,    1,    1],\n",
            "        [   9, 1237,    2,  163,    4,   81,  204,   15,    2,   63,  187,  278,\n",
            "          372,    3,    8,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1]]) ['<sos>', 'able', 'age', 'adjustment', 'act', 'this', 'bill', 'increases', 'from', '##', 'to', '##', 'the', 'age&nbsp;threshold', 'for', 'tax-favored', 'able', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] \n",
            "\n",
            "Bill: tensor([[   9,   45,  577,  685,  682,   11,   27,   26,    3,   16,    4,   40,\n",
            "           36,   35,   33,   29,    5,   19,   22,   18,   21,    4,    2,   13,\n",
            "           17,   23,   10,   34,    7,   25,   31,    5,   30,   20,    6,    7,\n",
            "           12,    2,   24,   32,    3,   46,   58,   43,    3,   41,    3,   39,\n",
            "            2,   16,    4,  118,   28,   42,    3, 1922,  110,  121,  123,   75,\n",
            "           48,   10,   18,   62,  102,    5,  199,  101,  128,   74,    2,  100,\n",
            "            4,    2,  126,    4,   10,   11,    3,    8,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1],\n",
            "        [   9,   45,    5,   68,    2,   92,   91,   78,    4,   14,    5,  464,\n",
            "          153,  973,   69,   15,  224,    6,    5,   87,    6, 2445,    2,   69,\n",
            "           15,  974,  232,    3,   27,   26,    3,   16,    4,   40,   36,   35,\n",
            "           33,   29,    5,   19,   22,   18,   21,    4,    2,   13,   17,   23,\n",
            "           10,   34,    7,   25,   31,    5,   30,   20,    6,    7,   12,    2,\n",
            "           24,   32,    3,   46,   58,   43,    3,   41,    3,   39,    2,   16,\n",
            "            4,  169,   28,   42,    3, 1378,  110,  121,  137,   75,   48,   10,\n",
            "           18,   62,  102,    5,  409,  391,   64, 2127,   74,   95,   28,   14,\n",
            "            3,    8,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1],\n",
            "        [   9,   45,    2, 1092, 1202,   94,   11,    4,   14,   27,   26,    3,\n",
            "           16,    4,   40,   36,   35,   33,   29,    5,   19,   22,   18,   21,\n",
            "            4,    2,   13,   17,   23,   10,   34,    7,   25,   31,    5,   30,\n",
            "           20,    6,    7,   12,    2,   24,   32,    3,   46,   58,   43,    3,\n",
            "           41,    3,   39,    2,   16,    4,  247,   28,   42,    3, 1873,  195,\n",
            "         2279, 2191,    3,    8,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1],\n",
            "        [   9,   45,    5,  212,   81,  691,    4,  130,   15,    2,   63,  187,\n",
            "          278,  372,    3,   27,   26,    3,   16,    4,   40,   36,   35,   33,\n",
            "           29,    5,   19,   22,   18,   21,    4,    2,   13,   17,   23,   10,\n",
            "           34,    7,   25,   31,    5,   30,   20,    6,    7,   12,    2,   24,\n",
            "           32,    3,  157,  147,  134,  499,  151,  138,    3,   41,    3,   14,\n",
            "           12,    2,   16,    4,   40,  878,   51,   14,  149,    3,  653,   60,\n",
            "            2,   55,   59,   50,   56,   61,    5,    2,   53,   38,  106,    6,\n",
            "            2,  458,   47,   37,    5,  212,   81,  691,    4,  130,   15,    2,\n",
            "           63,  187,  278,  372,    3,   57, 1108, 1449,  130,  205,  264,    5,\n",
            "           54,  322,   15,    2,   63,  187,  278,  372,    3,    8,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1]]) ['<sos>', ':', 'able', 'age', 'adjustment', 'act', 'u', '.s', '.', 'house', 'of', 'representatives', '####-##-##', 'text/xml', 'en', 'pursuant', 'to', 'title', '##', 'section', '###', 'of', 'the', 'united', 'states', 'code,', 'this', 'file', 'is', 'not', 'subject', 'to', 'copyright', 'protection', 'and', 'is', 'in', 'the', 'public', 'domain', '.', 'i###th', 'congress#st', 'sessionh', '.', 'r', '.', '####in', 'the', 'house', 'of', 'representativesfebruary', '##,', '####mr', '.', 'cã¡rdenas', 'effective', 'datethe', 'amendments', 'made', 'by', 'this', 'section', 'shall', 'apply', 'to', 'taxable', 'years', 'beginning', 'after', 'the', 'date', 'of', 'the', 'enactment', 'of', 'this', 'act', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F33BgzPT2V8L"
      },
      "source": [
        "# ![Image](seq2seq.png)\n",
        "# Image Credit to Yoav Goldberg."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxlQlq9b2V8N"
      },
      "source": [
        "## Model Creation\n",
        "\n",
        "Our model is a Sequence To Sequence model with an LSTM-based Encoder and Decoder.\n",
        "\n",
        "Given an input sequence (the text of a Congressional bill) $W_{0:n}$ of length $n$, we produce a target sequence (bill summary), $U_{0:m}$ of length $m$, where $n$ > $m$. Let $<sos>$ and $<eos>$ be tokens indicating the start and end of text, respectively; meaning $W_{0}, U_{0} = <sos>$ and $W_{n}, U_{m} = <eos>$.\n",
        "\n",
        "Furthermore, we define embedding $E \\in n \\times e$, hidden dimension $H$, and vocab size $|V|$. Additionally, for LSTM $L$ in both the $ENCODER$ and $DECODER$, the hidden state $h$ and cell state $c$ components of state $s$ are initialized as $h_0, c_0 = \\vec{0}$. Lastly, $L_{j}$ is layer $j$ of the LSTM $L$. \n",
        "\n",
        "The Sequence To Sequence model is broadly defined as:\n",
        "\n",
        "$\\underset{n \\times H}{C} = ENCODER_{RNN}(W_{0:n})$\n",
        "\n",
        "$\\underset{1 \\times |V|}{\\hat{u}}= DECODER_{RNN}(C; \\hat{y}_{0:m-1}; s_{0:m-1})$\n",
        "\n",
        "where $ENCODER$ produces context vector $C$ from text $W$ as follows: \n",
        "\n",
        "At step $i$ of input sequence $W_{0:n}$, for $0 \\leq i \\leq n$:\n",
        "\n",
        "$\\underset{1 \\times e}{x_i} = E_{W_i}$\n",
        "\n",
        "and passing to $x_{i:n}$ PyTorch, we obtain:\n",
        "\n",
        "$\\underset{n \\times H}{\\hat{y}_{i:n}}, \\underset{1 \\times H}{h_1}, \\underset{1 \\times H}{c_1} = LSTM_{L_{0}}(x_{i} , (h_{0}, c_{0}))$\n",
        "\n",
        "And for layer $j$ of LSTM $L$,\n",
        "\n",
        "$\\underset{1 \\times H}{\\hat{y}^{j}_{i:n}}, \\underset{1 \\times H}{h_1}, \\underset{1 \\times H}{c_1}= LSTM_{L_{j}}(\\hat{y}^{j-1}_{i:n}, (h^{j-1}, c^{j-1}))$\n",
        "\n",
        "where the last layer produces ${C}$ from the final state $s^{enc}$, made up of the final $h$ and $c$ results.\n",
        "\n",
        "Next, $DECODER$ decodes context vector $C$ in combination with state $s_{i-1}$ and predicted value $\\hat{y}_{i-1}$, producing $\\hat{u}_{1:m}$ as follows:\n",
        "\n",
        "$\\underset{1 \\times |V|}{\\hat{y}_i} = O(LSTM_{L_{i}}(c; \\hat{y}_{i-1}; s_{i-1}))$\n",
        "\n",
        "in which $O$ is the softmax function and $h_0, c_0 = C$. $\\hat{y}_i$ is the predicted probability distribution for $w_{i+1}$.\n",
        "\n",
        "For each $\\hat{y}_i$ of the $DECODER$, we calculate the loss $\\ell$ by $\\log \\widehat{y}_{i_{[w_{i+1}]}}$. Across $\\hat{y}_{1:m}$, it is the average cross entropy loss, $\\ell = \\frac{1}{m}\\sum_{i=1}^{m} \\log \\widehat{y}_{i_{[w_{i+1}]}}$.\n",
        "\n",
        "With this step, we preform end to end backpropagation through the $DECODER$ to the $ENCODER$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2PPcdEl2V8U"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 hidden_size,\n",
        "                 pretrained_embeddings,\n",
        "                 num_layers,\n",
        "                 pad_token,\n",
        "                 freeze_glove=False,\n",
        "                 ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            pretrained_embeddings, padding_idx=pad_token, freeze=freeze_glove)\n",
        "\n",
        "        self.rnn = nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers, batch_first=False)\n",
        "\n",
        "    def forward(self, text):\n",
        "      '''\n",
        "      Text size is batch size, bill length\n",
        "      '''\n",
        "      # Embedded size is sequence length, batch size, glove vecs size\n",
        "      embedded = self.embedding(text) #.view(-1, len(text), 300)\n",
        "      # This above is batch first, but the rnn takes seq length, batch, size\n",
        "      embedded = embedded.permute(1, 0, 2)\n",
        "      # outputs is size (sequence length, batch_size, hidden_size)\n",
        "      outputs, hidden = self.rnn(embedded)\n",
        "      # each element in hidden is size (num_layers * num_directions (which is 1 unless using a bidirectional LSTM), batch_size, hidden_size)\n",
        "      return outputs.float(), tuple([v.float() for v in hidden])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1LTxkU-2V8X"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, output_dim, hidden_dim, pretrained_embeddings, num_layers, pad_token, freeze_glove=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        # output dim should equal VOCAB size\n",
        "        self.output_dim = output_dim\n",
        "        # hidden_dim should equal the hidden_dim of the ENCODER\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.LSTM(self.input_size, hidden_dim, num_layers=self.num_layers, batch_first=False)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim) # transfer to device?\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            pretrained_embeddings, padding_idx=pad_token, freeze=freeze_glove)\n",
        "\n",
        "    def forward(self, input_word, hidden):\n",
        " \n",
        "        embedded = self.embedding(input_word)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        output, hidden= self.rnn(embedded, hidden)\n",
        "        linear = self.fc_out(output.squeeze(0))\n",
        "        # prediction = self.softmax(linear)\n",
        "\n",
        "        return linear, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Xcq9IG2V8Z"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, label, text):\n",
        "        batch_size = label.shape[0]\n",
        "        label_length = label.shape[1]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "\n",
        "        decode_outputs = torch.zeros(label_length, batch_size, vocab_size)\n",
        "        output, hidden0 = self.encoder(text)\n",
        "        input_word = label[:, 0]\n",
        "\n",
        "        for t in range(1, label_length):\n",
        "            input_word = input_word.unsqueeze(1)\n",
        "            output, hidden = self.decoder(input_word, hidden0)\n",
        "            decode_outputs[t] = output\n",
        "            # top_choice = torch.max(output, dim=1)[1]\n",
        "            input_word = label[:, t]\n",
        "            hidden0 = hidden\n",
        "\n",
        "        # return top_choice, decode_outputs\n",
        "        return decode_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z41a0ghS2V8a"
      },
      "source": [
        "INPUT_SIZE = GLOVE_VECS.size()[1] #300\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 4\n",
        "GRAD_CLIP = 1\n",
        "LOG_INTERVAL = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuZU5Xkd2V8b"
      },
      "source": [
        "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, GLOVE_VECS, NUM_LAYERS, PAD_TOKEN, False).to(DEVICE)\n",
        "decoder = Decoder(INPUT_SIZE, VOCAB_SIZE, HIDDEN_SIZE, GLOVE_VECS, NUM_LAYERS, PAD_TOKEN, False).to(DEVICE)\n",
        "seq2seq = Seq2Seq(encoder, decoder).to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r9x3iN-2V8d"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v_T1tUS2V8g"
      },
      "source": [
        "OPTIMIZER = optim.Adam(decoder.parameters(), lr=.0003)\n",
        "OPTIMIZER2 = optim.Adam(encoder.parameters(), lr=.0003)\n",
        "LOSS_FN = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN).to(DEVICE) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_wrYVgT2V8i"
      },
      "source": [
        "def train_an_epoch(model, dataloader, optimizer, optimizer2, loss_function, log_interval, device):\n",
        "    model.train()\n",
        "    \n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        model.zero_grad()\n",
        "        if USE_CUDA:\n",
        "          label = label.to(device)\n",
        "          text = text.to(device)\n",
        "        outputs = model(label, text)\n",
        "        output_dim = outputs.shape[-1]\n",
        "        \n",
        "        output = outputs.view(-1, output_dim)\n",
        "        target = label.view(-1)\n",
        "\n",
        "        if USE_CUDA:\n",
        "          output = output.to(device)\n",
        "          target = target.to(device)\n",
        "\n",
        "        loss = loss_function(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "              \n",
        "        optimizer.step()\n",
        "        optimizer2.step()\n",
        "\n",
        "\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print(f'Iteration: {idx}; Loss: {loss:.3f}.')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lqo71Bo2V8m"
      },
      "source": [
        "def get_score(model, dataloader, loss_function, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, (label, text) in enumerate(dataloader):\n",
        "      if USE_CUDA:\n",
        "        label = label.to(device)\n",
        "        text = text.to(device)\n",
        "      outputs = model(label, text)\n",
        "      output_dim = outputs.shape[-1]\n",
        "      \n",
        "      output = outputs.view(-1, output_dim)\n",
        "      target = label.view(-1)\n",
        "\n",
        "      if USE_CUDA:\n",
        "        output = output.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "      loss = loss_function(output, target)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "R9UbiKp92V8z",
        "outputId": "bdb17022-f5b8-4dfc-f8a9-acb75d5b416a"
      },
      "source": [
        "EPOCHS = 20\n",
        "scores = []\n",
        "minimum_score = np.inf\n",
        "best_model = None\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train_an_epoch(seq2seq, DATALOADERS_DICT['train_data'], OPTIMIZER, OPTIMIZER2, LOSS_FN, LOG_INTERVAL, DEVICE)\n",
        "  if SINGLE_RECORD:\n",
        "    validation_score = get_score(seq2seq, DATALOADERS_DICT['train_data'], LOSS_FN, DEVICE)\n",
        "  else:\n",
        "    validation_score = get_score(seq2seq, DATALOADERS_DICT['validate_data'], LOSS_FN, DEVICE)\n",
        "  scores.append(validation_score)\n",
        "\n",
        "  if validation_score < minimum_score:\n",
        "    minimum_score = validation_score\n",
        "    model_copy = Seq2Seq(encoder, decoder).to(DEVICE)\n",
        "    model_copy.load_state_dict(seq2seq.state_dict())\n",
        "    best_model = model_copy\n",
        "\n",
        "\n",
        "plt.plot(range(1, EPOCHS+1), scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 50; Loss: 6.147.\n",
            "Iteration: 50; Loss: 5.743.\n",
            "Iteration: 50; Loss: 5.526.\n",
            "Iteration: 50; Loss: 5.917.\n",
            "Iteration: 50; Loss: 5.569.\n",
            "Iteration: 50; Loss: 5.803.\n",
            "Iteration: 50; Loss: 5.639.\n",
            "Iteration: 50; Loss: 5.575.\n",
            "Iteration: 50; Loss: 5.590.\n",
            "Iteration: 50; Loss: 5.624.\n",
            "Iteration: 50; Loss: 5.521.\n",
            "Iteration: 50; Loss: 5.429.\n",
            "Iteration: 50; Loss: 5.392.\n",
            "Iteration: 50; Loss: 5.648.\n",
            "Iteration: 50; Loss: 5.672.\n",
            "Iteration: 50; Loss: 5.487.\n",
            "Iteration: 50; Loss: 5.419.\n",
            "Iteration: 50; Loss: 5.111.\n",
            "Iteration: 50; Loss: 5.209.\n",
            "Iteration: 50; Loss: 4.929.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff2eac0fcd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7gAwUTl2V83"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP_pxtBO2V85"
      },
      "source": [
        "def generate(encoder, decoder, text, batch_size, device, method='sample'):\n",
        "    start_token = torch.LongTensor([VOCAB.stoi['<sos>']])\n",
        "    current_token = torch.cat((start_token, torch.zeros(BATCH_SIZE - 1))).int().unsqueeze(1)\n",
        "\n",
        "    t = 0\n",
        "    words = []\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        if USE_CUDA:\n",
        "          text = text.to(device)\n",
        "\n",
        "        enc_out, hidden0 = encoder(text)\n",
        "\n",
        "        while (t < MAX_SUMMARY_LENGTH) and (torch.sum(current_token) != VOCAB.stoi['<eos>']):\n",
        "            # Depending on if working with a single record or not may need to change this\n",
        "            word = current_token #.unsqueeze(1)\n",
        "\n",
        "            if USE_CUDA:\n",
        "              word = word.to(device)\n",
        "\n",
        "            d_out, d_hid = decoder(word, hidden0)\n",
        "            if method=='sample':\n",
        "                i = 0\n",
        "                s = np.random.random()\n",
        "                while s >= 0:\n",
        "                    i += 1\n",
        "                    s -= d_out[:, i][0]\n",
        "                    \n",
        "                words.append(VOCAB.itos[i])\n",
        "\n",
        "            else:\n",
        "                words.append(VOCAB.itos[d_out[0].argmax(0)])\n",
        "\n",
        "            batch_supplement = torch.zeros(batch_size - 1)\n",
        "\n",
        "            if USE_CUDA:\n",
        "              batch_supplement = batch_supplement.to(device)\n",
        "\n",
        "            current_token = torch.cat((d_out[0].argmax(0).unsqueeze(0), batch_supplement)).int().unsqueeze(1)\n",
        "            hidden0 = [x.detach() for x in d_hid]\n",
        "            t+= 1\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "790J6n-z2V87",
        "outputId": "413d17dc-f68f-408e-ba52-c9ff58bea7cb"
      },
      "source": [
        "generate(encoder, decoder, f, BATCH_SIZE, DEVICE, 'nH')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['act', 'the', 'the', '.', '.', '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1pbUSaX2V89",
        "outputId": "e07b52ec-371c-4d92-fc3d-d8f1689c787b"
      },
      "source": [
        "[VOCAB.itos[x] for x in l[0].squeeze()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>',\n",
              " 'able',\n",
              " 'age',\n",
              " 'adjustment',\n",
              " 'act',\n",
              " 'this',\n",
              " 'bill',\n",
              " 'increases',\n",
              " 'from',\n",
              " '##',\n",
              " 'to',\n",
              " '##',\n",
              " 'the',\n",
              " 'age&nbsp;threshold',\n",
              " 'for',\n",
              " 'tax-favored',\n",
              " 'able',\n",
              " '<eos>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkxAt0e9E-3C",
        "outputId": "4754fca1-55e1-434f-fa42-c2a4eb747724"
      },
      "source": [
        "f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   9,   45,  577,  685,  682,   11,   27,   26,    3,   16,    4,   40,\n",
              "           36,   35,   33,   29,    5,   19,   22,   18,   21,    4,    2,   13,\n",
              "           17,   23,   10,   34,    7,   25,   31,    5,   30,   20,    6,    7,\n",
              "           12,    2,   24,   32,    3,   46,   58,   43,    3,   41,    3,   39,\n",
              "            2,   16,    4,  118,   28,   42,    3, 1922,  110,  121,  123,   75,\n",
              "           48,   10,   18,   62,  102,    5,  199,  101,  128,   74,    2,  100,\n",
              "            4,    2,  126,    4,   10,   11,    3,    8,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1],\n",
              "        [   9,   45,    5,   68,    2,   92,   91,   78,    4,   14,    5,  464,\n",
              "          153,  973,   69,   15,  224,    6,    5,   87,    6, 2445,    2,   69,\n",
              "           15,  974,  232,    3,   27,   26,    3,   16,    4,   40,   36,   35,\n",
              "           33,   29,    5,   19,   22,   18,   21,    4,    2,   13,   17,   23,\n",
              "           10,   34,    7,   25,   31,    5,   30,   20,    6,    7,   12,    2,\n",
              "           24,   32,    3,   46,   58,   43,    3,   41,    3,   39,    2,   16,\n",
              "            4,  169,   28,   42,    3, 1378,  110,  121,  137,   75,   48,   10,\n",
              "           18,   62,  102,    5,  409,  391,   64, 2127,   74,   95,   28,   14,\n",
              "            3,    8,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1],\n",
              "        [   9,   45,    2, 1092, 1202,   94,   11,    4,   14,   27,   26,    3,\n",
              "           16,    4,   40,   36,   35,   33,   29,    5,   19,   22,   18,   21,\n",
              "            4,    2,   13,   17,   23,   10,   34,    7,   25,   31,    5,   30,\n",
              "           20,    6,    7,   12,    2,   24,   32,    3,   46,   58,   43,    3,\n",
              "           41,    3,   39,    2,   16,    4,  247,   28,   42,    3, 1873,  195,\n",
              "         2279, 2191,    3,    8,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1],\n",
              "        [   9,   45,    5,  212,   81,  691,    4,  130,   15,    2,   63,  187,\n",
              "          278,  372,    3,   27,   26,    3,   16,    4,   40,   36,   35,   33,\n",
              "           29,    5,   19,   22,   18,   21,    4,    2,   13,   17,   23,   10,\n",
              "           34,    7,   25,   31,    5,   30,   20,    6,    7,   12,    2,   24,\n",
              "           32,    3,  157,  147,  134,  499,  151,  138,    3,   41,    3,   14,\n",
              "           12,    2,   16,    4,   40,  878,   51,   14,  149,    3,  653,   60,\n",
              "            2,   55,   59,   50,   56,   61,    5,    2,   53,   38,  106,    6,\n",
              "            2,  458,   47,   37,    5,  212,   81,  691,    4,  130,   15,    2,\n",
              "           63,  187,  278,  372,    3,   57, 1108, 1449,  130,  205,  264,    5,\n",
              "           54,  322,   15,    2,   63,  187,  278,  372,    3,    8,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-GI5mNsFHJl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}